{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.5"
    },
    "colab": {
      "name": "MIL.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pastal24/rep/blob/master/MIL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UhzkunO0G0UQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "449e6f65-8edd-4172-d439-4d134a36e923"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3yFYPHdVF01U"
      },
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import tensorflow "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fgM00m6hF01a"
      },
      "source": [
        "from tensorflow.keras.layers import GlobalAveragePooling1D,GlobalMaxPooling1D,GlobalAveragePooling2D,GlobalMaxPooling2D,ConvLSTM2D,Bidirectional,Input,Activation,UpSampling1D,Conv1D,MaxPooling1D,AveragePooling1D,Attention,AdditiveAttention,Conv2D,AveragePooling2D, Dense, TimeDistributed, MaxPooling2D, UpSampling2D , Flatten ,Reshape ,Dropout,LSTM,RepeatVector,ActivityRegularization,BatchNormalization,Softmax\n",
        "from tensorflow.keras.models import Model,Sequential\n",
        "from tensorflow.keras.regularizers import l1,l2,l1_l2\n",
        "from tensorflow.keras import backend as K"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kvKpego2Ktkm"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K0IqFrmvIU-q",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "outputId": "70de833a-ff51-430f-c80f-1f41eb9084c1"
      },
      "source": [
        "\n",
        "!pip install attention-sampling"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting attention-sampling\n",
            "  Downloading https://files.pythonhosted.org/packages/ff/2d/4474d1f516865eb83419c67045d885adc03266f97858fa8eeb14117c709d/attention-sampling-0.2.tar.gz\n",
            "Requirement already satisfied: keras>=2 in /usr/local/lib/python3.6/dist-packages (from attention-sampling) (2.4.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from attention-sampling) (1.18.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras>=2->attention-sampling) (3.13)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras>=2->attention-sampling) (1.4.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras>=2->attention-sampling) (2.10.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from h5py->keras>=2->attention-sampling) (1.15.0)\n",
            "Building wheels for collected packages: attention-sampling\n",
            "  Building wheel for attention-sampling (setup.py) ... \u001b[?25l\u001b[?25hcanceled\n",
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yneIwMN7F01d"
      },
      "source": [
        "import pickle\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "objects={}\n",
        "objects2={}\n",
        "infile = open(\"/content/gdrive/My Drive/GdataML100_40.pickle\",'rb')\n",
        "objects = pickle.load(infile, encoding='latin1')\n",
        "# infile2 = open(\"/content/gdrive/My Drive/GdataML100_25_50.pickle\",'rb')\n",
        "# objects2 = pickle.load(infile2, encoding='latin1')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eA4KspGGF01g"
      },
      "source": [
        "\n",
        "data_dem=pd.read_excel(\"/content/gdrive/My Drive/thesis/Demographics.xlsx\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D7Kh38aFF01j",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        },
        "outputId": "761d5813-8f76-403e-f896-70b058f607b6"
      },
      "source": [
        "data = pd.read_csv(\"/content/gdrive/My Drive/thesis/SDATA_SEPTEMBER_new.csv\") \n",
        "data['Secondary ID'].value_counts()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8b0c496cc31f42fa    1\n",
              "11a9e65564567d48    1\n",
              "2be1fe7ec81e1b5e    1\n",
              "c6ffffbbce0a8fe9    1\n",
              "5b21e87a204e9879    1\n",
              "                   ..\n",
              "fa1ca92b6d69c880    1\n",
              "a83230cbb772f842    1\n",
              "edfe952987be6876    1\n",
              "8e9de02b429f50ef    1\n",
              "393ac67b9a76136f    1\n",
              "Name: Secondary ID, Length: 128, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q0wxeg6oF01o",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        },
        "outputId": "82bb1a06-a11b-466e-d316-903d1eb93c79"
      },
      "source": [
        "df=pd.DataFrame.from_dict(objects)\n",
        "\n",
        "df.isna().sum()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "y           0\n",
              "X_flight    0\n",
              "ids         0\n",
              "X_hold      0\n",
              "time        0\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JVXkNMPNF01r"
      },
      "source": [
        "df.drop([ 'time'], axis=1,inplace=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EFNe4wQxF01v",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6e9f5743-234d-4994-9f7d-2e4c76c02e27"
      },
      "source": [
        "dff=pd.DataFrame.from_dict(objects2)\n",
        "\n",
        "dff.isna().sum()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Series([], dtype: float64)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f12Gq1sFF01y"
      },
      "source": [
        "data.drop(data.iloc[:, 11:], inplace = True, axis = 1) \n",
        "data.rename(columns={\"Secondary ID\": \"ids\"},inplace=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "d92fWba6F011"
      },
      "source": [
        "\n",
        "data = data[data.ids != 'e75ecd6e5c41a2cc']\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UWiQ-yZBF014",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        },
        "outputId": "310edbf6-5d54-4752-e733-3533e11a6ef9"
      },
      "source": [
        "data['UPDRS_22'] = data[' UPDRS_22_UEXTR_LEFT_E1_1_C1 '] + data[' UPDRS_22_UEXTR_RIGHT_E1_1_C1 ']  \n",
        "data['UPDRS_23'] = data [' UPDRS_23_RIGHT_E1_1_C1 '] + data[' UPDRS_23_LEFT_E1_1_C1 ']\n",
        "data['UPDRS_22'].value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    57\n",
              "2    28\n",
              "1    24\n",
              "3    19\n",
              "4     4\n",
              "7     1\n",
              "5     1\n",
              "Name: UPDRS_22, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WwAWE1-6F016",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 319
        },
        "outputId": "562a04e7-11af-4df1-f654-56513b42e578"
      },
      "source": [
        "\n",
        "data['Study Subject ID'].unique()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['SUK01', 'SUK02', 'SUK21', 'SUK03', 'SUK04', 'SUK05', 'SUK06',\n",
              "       'SUK07', 'SUK08', 'SUK09', 'SUK10', 'SUK11', 'SUK12', 'SUK13',\n",
              "       'SUK14', 'SUK15', 'SUK16', 'SUK17', 'SUK18', 'SUK19', 'SUK20',\n",
              "       'SUK22', 'SUK23', 'SUK24', 'SUK27', 'SUK28', 'SUK29', 'SUK30',\n",
              "       'SUK31', 'SUK36', 'SUK37', 'SUK38', 'SUK39', 'SGR01', 'SGR02',\n",
              "       'SGR03', 'SGR04', 'SGR05', 'SGR06', 'SGR07', 'SGR08', 'SGR09',\n",
              "       'SGR10', 'SGR11', 'SGR12', 'SGR13', 'SGR14', 'SGR15', 'SGR16',\n",
              "       'SGR17', 'SGR18', 'SGR19', 'SGR20', 'SGR21', 'SGR22', 'SGR23',\n",
              "       'SGR24', 'SGR25', 'SGR26', 'SGR27', 'SGR28', 'SGR29', 'SGR30',\n",
              "       'SGR31', 'SGR32', 'SGR33', 'SGR34', 'SGR35', 'SDE02', 'SDE07',\n",
              "       'SDE01', 'SDE03', 'SDE04', 'SDE05', 'SDE06', 'SDE08', 'SDE09',\n",
              "       'SDE10', 'SDE11', 'SDE12', 'SDE13', 'SDE14', 'SDE15', 'SDE16',\n",
              "       'SDE17', 'SDE18', 'SDE19', 'SDE20', 'SDE21', 'SDE22', 'SDE23',\n",
              "       'SDE24', 'SDE25', 'SDE26', 'SDE27', 'SDE28', 'SDE29', 'SDE30',\n",
              "       'SDE31', 'SDE32', 'SDE33', 'SDE34', 'SDE35', 'SDE36', 'SDE37',\n",
              "       'SDE38', 'SDE39', 'SDE40', 'SDE41', 'SDE42', 'SDE 43', 'SDE44',\n",
              "       'SDE45', 'SDE46', 'SDE47', 'SDE 48', 'SDE49', 'SDE50', 'SDE51',\n",
              "       'Leontios', 'Sergiadis', 'CNT_1', 'CNT_2'], dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qgEt2j4S5YTc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "outputId": "462da5e2-64e0-4eb2-cb3d-00e24e1a1770"
      },
      "source": [
        "dff[['y']]=dff[['y']].replace([0.0,1.0,2.0],[1,0,0])\n",
        "dff['y'].value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-985dacf1e41f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdff\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'y'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdff\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'y'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdff\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'y'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'dff' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "arAwTdOYF019"
      },
      "source": [
        "\n",
        "\n",
        "dff['X_flight'].isna().sum()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jn28J2yZ5SE_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a5a928ba-f3eb-453e-b04c-604264d06f42"
      },
      "source": [
        "df2=pd.merge(df, data,how='outer')\n",
        "df2\n",
        "df2.rename(columns={\" UPDRS_31_E1_1_C1 \": \"UPDRS_31_E1_1_C1\"},inplace=True)\n",
        "df2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>y</th>\n",
              "      <th>X_flight</th>\n",
              "      <th>ids</th>\n",
              "      <th>X_hold</th>\n",
              "      <th>Study Subject ID</th>\n",
              "      <th>Protocol ID</th>\n",
              "      <th>Subject Status</th>\n",
              "      <th>Sex</th>\n",
              "      <th>UPDRS_22_UEXTR_RIGHT_E1_1_C1</th>\n",
              "      <th>UPDRS_22_UEXTR_LEFT_E1_1_C1</th>\n",
              "      <th>UPDRS_23_RIGHT_E1_1_C1</th>\n",
              "      <th>UPDRS_23_LEFT_E1_1_C1</th>\n",
              "      <th>UPDRS_31_E1_1_C1</th>\n",
              "      <th>SUM_PART_3_E1_1_C1</th>\n",
              "      <th>UPDRS_22</th>\n",
              "      <th>UPDRS_23</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2.0</td>\n",
              "      <td>[0.304, 0.206, 0.287, 0.28, 0.575, 0.148, 0.39...</td>\n",
              "      <td>0168568f68e1b02b</td>\n",
              "      <td>[0.10599999999976717, 0.11599999999452848, 0.1...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2.0</td>\n",
              "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
              "      <td>0168568f68e1b02b</td>\n",
              "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2.0</td>\n",
              "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
              "      <td>0168568f68e1b02b</td>\n",
              "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2.0</td>\n",
              "      <td>[0.312, 0.386, 0.608, 0.37, 0.295, 0.781, 0.26...</td>\n",
              "      <td>0168568f68e1b02b</td>\n",
              "      <td>[0.09000000002561137, 0.0650000000023283, 0.06...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2.0</td>\n",
              "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.247, 0.1...</td>\n",
              "      <td>0168568f68e1b02b</td>\n",
              "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1070000000181...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23666</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>58dab6321800bfe0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>SDE49</td>\n",
              "      <td>iprognosis-sdata-study - EK 451112017</td>\n",
              "      <td>available</td>\n",
              "      <td>m</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23667</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>8e9de02b429f50ef</td>\n",
              "      <td>NaN</td>\n",
              "      <td>SDE50</td>\n",
              "      <td>iprognosis-sdata-study - EK 451112017</td>\n",
              "      <td>available</td>\n",
              "      <td>f</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23668</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4365496b0b6b8342</td>\n",
              "      <td>NaN</td>\n",
              "      <td>SDE51</td>\n",
              "      <td>iprognosis-sdata-study - EK 451112017</td>\n",
              "      <td>available</td>\n",
              "      <td>f</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23669</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>c6ffffbbce0a8fe9</td>\n",
              "      <td>NaN</td>\n",
              "      <td>CNT_1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>m</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23670</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>ac2d2ebf8041e560</td>\n",
              "      <td>NaN</td>\n",
              "      <td>CNT_2</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>f</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>23671 rows × 16 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         y  ... UPDRS_23\n",
              "0      2.0  ...      NaN\n",
              "1      2.0  ...      NaN\n",
              "2      2.0  ...      NaN\n",
              "3      2.0  ...      NaN\n",
              "4      2.0  ...      NaN\n",
              "...    ...  ...      ...\n",
              "23666  NaN  ...      0.0\n",
              "23667  NaN  ...      2.0\n",
              "23668  NaN  ...      0.0\n",
              "23669  NaN  ...      0.0\n",
              "23670  NaN  ...      0.0\n",
              "\n",
              "[23671 rows x 16 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2CG0DsCgF02A"
      },
      "source": [
        "df2.UPDRS_22=df2.sort_values(['ids','UPDRS_22']).groupby(['ids','UPDRS_22']).UPDRS_22.ffill()\n",
        "df2['UPDRS_22'].isna().sum()\n",
        "# df.COL=df.sort_values(['ID','COL']).COL.ffill()\n",
        "df2.UPDRS_23=df2.sort_values(['ids','UPDRS_23']).groupby(['ids','UPDRS_23']).UPDRS_23.ffill()\n",
        "df2['UPDRS_23'].isna().sum()\n",
        "df2.UPDRS_31_E1_1_C1=df2.sort_values(['ids','UPDRS_31_E1_1_C1']).groupby(['ids','UPDRS_31_E1_1_C1']).UPDRS_31_E1_1_C1.fillna(df2.UPDRS_31_E1_1_C1)\n",
        "df2['UPDRS_23'].isna().sum()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hGU4ue9f7dCJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a52d569e-e08a-4aee-9b32-54898b478e6c"
      },
      "source": [
        " df2.groupby('ids').ffill().fillna('')\n",
        " df2.groupby('ids').bfill().fillna('')\n",
        " df2['UPDRS_23'].isna().sum()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "18938"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8cXhxnZZbTlE"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_KEP_nRNGzEP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "cdac71f0-9f02-42cc-fd42-185e4d8735e6"
      },
      "source": [
        "s = df2['ids'].ffill()\n",
        "m = s == df2['ids'].bfill()\n",
        "df2.loc[m, 'ids'] = s\n",
        "df2['UPDRS_23'].isna().sum()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "18938"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tpBbOpG95fW3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3af6f370-894f-4e2c-91b0-ba97af4ae711"
      },
      "source": [
        "for each in df_tot.groupby([\"Subject_id\"], sort=True): print (each)  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('Leontios',      index    y  ... UPDRS_22 UPDRS_23\n",
            "0      739  2.0  ...      0.0      0.0\n",
            "1      740  2.0  ...      0.0      0.0\n",
            "2      741  2.0  ...      0.0      0.0\n",
            "3      742  2.0  ...      0.0      0.0\n",
            "4      743  2.0  ...      0.0      0.0\n",
            "..     ...  ...  ...      ...      ...\n",
            "391   1130  2.0  ...      0.0      0.0\n",
            "392   1131  2.0  ...      0.0      0.0\n",
            "393   1132  2.0  ...      0.0      0.0\n",
            "394   1133  2.0  ...      0.0      0.0\n",
            "395   1134  2.0  ...      0.0      0.0\n",
            "\n",
            "[396 rows x 17 columns])\n",
            "('SDE 43',      index    y  ... UPDRS_22 UPDRS_23\n",
            "396   1154  1.0  ...      0.0      0.0\n",
            "397   1155  1.0  ...      0.0      0.0\n",
            "398   1156  1.0  ...      0.0      0.0\n",
            "399   1157  1.0  ...      0.0      0.0\n",
            "400   1158  1.0  ...      0.0      0.0\n",
            "..     ...  ...  ...      ...      ...\n",
            "558   1316  1.0  ...      0.0      0.0\n",
            "559   1317  1.0  ...      0.0      0.0\n",
            "560   1318  1.0  ...      0.0      0.0\n",
            "561   1319  1.0  ...      0.0      0.0\n",
            "562   1320  1.0  ...      0.0      0.0\n",
            "\n",
            "[167 rows x 17 columns])\n",
            "('SDE08',       index    y  ... UPDRS_22 UPDRS_23\n",
            "1824  14921  0.0  ...      0.0      3.0\n",
            "1825  14922  0.0  ...      0.0      3.0\n",
            "1826  14923  0.0  ...      0.0      3.0\n",
            "1827  14924  0.0  ...      0.0      3.0\n",
            "1828  14925  0.0  ...      0.0      3.0\n",
            "1829  14926  0.0  ...      0.0      3.0\n",
            "1830  14927  0.0  ...      0.0      3.0\n",
            "1831  14928  0.0  ...      0.0      3.0\n",
            "1832  14929  0.0  ...      0.0      3.0\n",
            "1833  14930  0.0  ...      0.0      3.0\n",
            "1834  14931  0.0  ...      0.0      3.0\n",
            "1835  14932  0.0  ...      0.0      3.0\n",
            "1836  14933  0.0  ...      0.0      3.0\n",
            "1837  14934  0.0  ...      0.0      3.0\n",
            "1838  14935  0.0  ...      0.0      3.0\n",
            "1839  14936  0.0  ...      0.0      3.0\n",
            "1840  14937  0.0  ...      0.0      3.0\n",
            "1841  14938  0.0  ...      0.0      3.0\n",
            "1842  14939  0.0  ...      0.0      3.0\n",
            "1843  14940  0.0  ...      0.0      3.0\n",
            "1844  14941  0.0  ...      0.0      3.0\n",
            "1845  14942  0.0  ...      0.0      3.0\n",
            "1846  14943  0.0  ...      0.0      3.0\n",
            "1847  14944  0.0  ...      0.0      3.0\n",
            "1848  14945  0.0  ...      0.0      3.0\n",
            "1849  14946  0.0  ...      0.0      3.0\n",
            "\n",
            "[26 rows x 17 columns])\n",
            "('SDE09',       index    y  ... UPDRS_22 UPDRS_23\n",
            "1920  15588  0.0  ...      0.0      0.0\n",
            "1921  15589  0.0  ...      0.0      0.0\n",
            "1922  15590  0.0  ...      0.0      0.0\n",
            "1923  15591  0.0  ...      0.0      0.0\n",
            "1924  15592  0.0  ...      0.0      0.0\n",
            "1925  15593  0.0  ...      0.0      0.0\n",
            "1926  15594  0.0  ...      0.0      0.0\n",
            "1927  15595  0.0  ...      0.0      0.0\n",
            "1928  15596  0.0  ...      0.0      0.0\n",
            "1929  15597  0.0  ...      0.0      0.0\n",
            "1930  15598  0.0  ...      0.0      0.0\n",
            "1931  15599  0.0  ...      0.0      0.0\n",
            "\n",
            "[12 rows x 17 columns])\n",
            "('SDE14',       index    y  ... UPDRS_22 UPDRS_23\n",
            "4165  23507  0.0  ...      0.0      0.0\n",
            "4166  23508  0.0  ...      0.0      0.0\n",
            "\n",
            "[2 rows x 17 columns])\n",
            "('SDE39',       index    y  ... UPDRS_22 UPDRS_23\n",
            "3540  21944  2.0  ...      0.0      0.0\n",
            "3541  21945  2.0  ...      0.0      0.0\n",
            "3542  21946  2.0  ...      0.0      0.0\n",
            "3543  21947  2.0  ...      0.0      0.0\n",
            "3544  21948  2.0  ...      0.0      0.0\n",
            "3545  21949  2.0  ...      0.0      0.0\n",
            "3546  21950  2.0  ...      0.0      0.0\n",
            "\n",
            "[7 rows x 17 columns])\n",
            "('SGR04',       index    y  ... UPDRS_22 UPDRS_23\n",
            "2024  17695  2.0  ...      0.0      0.0\n",
            "2025  17696  2.0  ...      0.0      0.0\n",
            "2026  17697  2.0  ...      0.0      0.0\n",
            "2027  17698  2.0  ...      0.0      0.0\n",
            "2028  17699  2.0  ...      0.0      0.0\n",
            "...     ...  ...  ...      ...      ...\n",
            "2703  18374  2.0  ...      0.0      0.0\n",
            "2704  18375  2.0  ...      0.0      0.0\n",
            "2705  18376  2.0  ...      0.0      0.0\n",
            "2706  18377  2.0  ...      0.0      0.0\n",
            "2707  18378  2.0  ...      0.0      0.0\n",
            "\n",
            "[684 rows x 17 columns])\n",
            "('SGR05',       index    y  ... UPDRS_22 UPDRS_23\n",
            "3511  20765  2.0  ...      0.0      0.0\n",
            "3512  20766  2.0  ...      0.0      0.0\n",
            "3513  20767  2.0  ...      0.0      0.0\n",
            "3514  20768  2.0  ...      0.0      0.0\n",
            "3515  20769  2.0  ...      0.0      0.0\n",
            "3516  20770  2.0  ...      0.0      0.0\n",
            "3517  20771  2.0  ...      0.0      0.0\n",
            "3518  20772  2.0  ...      0.0      0.0\n",
            "3519  20773  2.0  ...      0.0      0.0\n",
            "3520  20774  2.0  ...      0.0      0.0\n",
            "3521  20775  2.0  ...      0.0      0.0\n",
            "3522  20776  2.0  ...      0.0      0.0\n",
            "3523  20777  2.0  ...      0.0      0.0\n",
            "3524  20778  2.0  ...      0.0      0.0\n",
            "3525  20779  2.0  ...      0.0      0.0\n",
            "3526  20780  2.0  ...      0.0      0.0\n",
            "3527  20781  2.0  ...      0.0      0.0\n",
            "3528  20782  2.0  ...      0.0      0.0\n",
            "3529  20783  2.0  ...      0.0      0.0\n",
            "3530  20784  2.0  ...      0.0      0.0\n",
            "3531  20785  2.0  ...      0.0      0.0\n",
            "3532  20786  2.0  ...      0.0      0.0\n",
            "3533  20787  2.0  ...      0.0      0.0\n",
            "3534  20788  2.0  ...      0.0      0.0\n",
            "3535  20789  2.0  ...      0.0      0.0\n",
            "3536  20790  2.0  ...      0.0      0.0\n",
            "3537  20791  2.0  ...      0.0      0.0\n",
            "\n",
            "[27 rows x 17 columns])\n",
            "('SGR06',       index    y  ... UPDRS_22 UPDRS_23\n",
            "1412  14507  0.0  ...      2.0      3.0\n",
            "1413  14508  0.0  ...      2.0      3.0\n",
            "1414  14509  0.0  ...      2.0      3.0\n",
            "1415  14510  0.0  ...      2.0      3.0\n",
            "1416  14511  0.0  ...      2.0      3.0\n",
            "...     ...  ...  ...      ...      ...\n",
            "1819  14914  0.0  ...      2.0      3.0\n",
            "1820  14915  0.0  ...      2.0      3.0\n",
            "1821  14916  0.0  ...      2.0      3.0\n",
            "1822  14917  0.0  ...      2.0      3.0\n",
            "1823  14918  0.0  ...      2.0      3.0\n",
            "\n",
            "[412 rows x 17 columns])\n",
            "('SGR07',      index    y  ... UPDRS_22 UPDRS_23\n",
            "563   1321  0.0  ...      3.0      3.0\n",
            "564   1322  0.0  ...      3.0      3.0\n",
            "565   1323  0.0  ...      3.0      3.0\n",
            "566   1324  0.0  ...      3.0      3.0\n",
            "567   1325  0.0  ...      3.0      3.0\n",
            "..     ...  ...  ...      ...      ...\n",
            "659   1417  0.0  ...      3.0      3.0\n",
            "660   1418  0.0  ...      3.0      3.0\n",
            "661   1419  0.0  ...      3.0      3.0\n",
            "662   1420  0.0  ...      3.0      3.0\n",
            "663   1421  0.0  ...      3.0      3.0\n",
            "\n",
            "[101 rows x 17 columns])\n",
            "('SGR10',       index    y  ... UPDRS_22 UPDRS_23\n",
            "1932  16205  0.0  ...      4.0      4.0\n",
            "1933  16206  0.0  ...      4.0      4.0\n",
            "1934  16207  0.0  ...      4.0      4.0\n",
            "1935  16208  0.0  ...      4.0      4.0\n",
            "1936  16209  0.0  ...      4.0      4.0\n",
            "...     ...  ...  ...      ...      ...\n",
            "2019  16292  0.0  ...      4.0      4.0\n",
            "2020  16293  0.0  ...      4.0      4.0\n",
            "2021  16294  0.0  ...      4.0      4.0\n",
            "2022  16295  0.0  ...      4.0      4.0\n",
            "2023  16296  0.0  ...      4.0      4.0\n",
            "\n",
            "[92 rows x 17 columns])\n",
            "('SGR16',      index    y  ... UPDRS_22 UPDRS_23\n",
            "664   2510  0.0  ...      1.0      1.0\n",
            "665   2511  0.0  ...      1.0      1.0\n",
            "666   2512  0.0  ...      1.0      1.0\n",
            "667   2513  0.0  ...      1.0      1.0\n",
            "668   2514  0.0  ...      1.0      1.0\n",
            "669   2515  0.0  ...      1.0      1.0\n",
            "670   2516  0.0  ...      1.0      1.0\n",
            "671   2517  0.0  ...      1.0      1.0\n",
            "672   2518  0.0  ...      1.0      1.0\n",
            "673   2519  0.0  ...      1.0      1.0\n",
            "674   2520  0.0  ...      1.0      1.0\n",
            "675   2521  0.0  ...      1.0      1.0\n",
            "676   2522  0.0  ...      1.0      1.0\n",
            "677   2523  0.0  ...      1.0      1.0\n",
            "678   2524  0.0  ...      1.0      1.0\n",
            "679   2525  0.0  ...      1.0      1.0\n",
            "680   2526  0.0  ...      1.0      1.0\n",
            "681   2527  0.0  ...      1.0      1.0\n",
            "\n",
            "[18 rows x 17 columns])\n",
            "('SGR19',      index    y  ... UPDRS_22 UPDRS_23\n",
            "691   6689  1.0  ...      0.0      0.0\n",
            "692   6690  1.0  ...      0.0      0.0\n",
            "693   6691  1.0  ...      0.0      0.0\n",
            "694   6692  1.0  ...      0.0      0.0\n",
            "695   6693  1.0  ...      0.0      0.0\n",
            "696   6694  1.0  ...      0.0      0.0\n",
            "697   6695  1.0  ...      0.0      0.0\n",
            "698   6696  1.0  ...      0.0      0.0\n",
            "699   6697  1.0  ...      0.0      0.0\n",
            "700   6698  1.0  ...      0.0      0.0\n",
            "701   6699  1.0  ...      0.0      0.0\n",
            "702   6700  1.0  ...      0.0      0.0\n",
            "703   6701  1.0  ...      0.0      0.0\n",
            "704   6702  1.0  ...      0.0      0.0\n",
            "705   6703  1.0  ...      0.0      0.0\n",
            "706   6704  1.0  ...      0.0      0.0\n",
            "707   6705  1.0  ...      0.0      0.0\n",
            "708   6706  1.0  ...      0.0      0.0\n",
            "709   6707  1.0  ...      0.0      0.0\n",
            "710   6708  1.0  ...      0.0      0.0\n",
            "711   6709  1.0  ...      0.0      0.0\n",
            "712   6710  1.0  ...      0.0      0.0\n",
            "713   6711  1.0  ...      0.0      0.0\n",
            "714   6712  1.0  ...      0.0      0.0\n",
            "715   6713  1.0  ...      0.0      0.0\n",
            "716   6714  1.0  ...      0.0      0.0\n",
            "717   6715  1.0  ...      0.0      0.0\n",
            "718   6716  1.0  ...      0.0      0.0\n",
            "719   6717  1.0  ...      0.0      0.0\n",
            "720   6718  1.0  ...      0.0      0.0\n",
            "\n",
            "[30 rows x 17 columns])\n",
            "('SGR26',      index    y  ... UPDRS_22 UPDRS_23\n",
            "682   5665  0.0  ...      4.0      4.0\n",
            "683   5666  0.0  ...      4.0      4.0\n",
            "684   5667  0.0  ...      4.0      4.0\n",
            "685   5668  0.0  ...      4.0      4.0\n",
            "686   5669  0.0  ...      4.0      4.0\n",
            "687   5670  0.0  ...      4.0      4.0\n",
            "688   5671  0.0  ...      4.0      4.0\n",
            "689   5672  0.0  ...      4.0      4.0\n",
            "690   5673  0.0  ...      4.0      4.0\n",
            "\n",
            "[9 rows x 17 columns])\n",
            "('SGR27',       index    y  ... UPDRS_22 UPDRS_23\n",
            "1855  15523  0.0  ...      2.0      2.0\n",
            "1856  15524  0.0  ...      2.0      2.0\n",
            "1857  15525  0.0  ...      2.0      2.0\n",
            "1858  15526  0.0  ...      2.0      2.0\n",
            "1859  15527  0.0  ...      2.0      2.0\n",
            "...     ...  ...  ...      ...      ...\n",
            "1915  15583  0.0  ...      2.0      2.0\n",
            "1916  15584  0.0  ...      2.0      2.0\n",
            "1917  15585  0.0  ...      2.0      2.0\n",
            "1918  15586  0.0  ...      2.0      2.0\n",
            "1919  15587  0.0  ...      2.0      2.0\n",
            "\n",
            "[65 rows x 17 columns])\n",
            "('SGR28',       index    y  ... UPDRS_22 UPDRS_23\n",
            "1850  15517  0.0  ...      2.0      1.0\n",
            "1851  15518  0.0  ...      2.0      1.0\n",
            "1852  15519  0.0  ...      2.0      1.0\n",
            "1853  15520  0.0  ...      2.0      1.0\n",
            "1854  15521  0.0  ...      2.0      1.0\n",
            "\n",
            "[5 rows x 17 columns])\n",
            "('SGR30',      index    y  ... UPDRS_22 UPDRS_23\n",
            "792  10959  0.0  ...      2.0      1.0\n",
            "793  10960  0.0  ...      2.0      1.0\n",
            "794  10961  0.0  ...      2.0      1.0\n",
            "795  10962  0.0  ...      2.0      1.0\n",
            "796  10963  0.0  ...      2.0      1.0\n",
            "..     ...  ...  ...      ...      ...\n",
            "937  11104  0.0  ...      2.0      1.0\n",
            "938  11105  0.0  ...      2.0      1.0\n",
            "939  11106  0.0  ...      2.0      1.0\n",
            "940  11107  0.0  ...      2.0      1.0\n",
            "941  11108  0.0  ...      2.0      1.0\n",
            "\n",
            "[150 rows x 17 columns])\n",
            "('SGR32',       index    y  ... UPDRS_22 UPDRS_23\n",
            "3538  21922  0.0  ...      5.0      5.0\n",
            "3539  21923  0.0  ...      5.0      5.0\n",
            "\n",
            "[2 rows x 17 columns])\n",
            "('SGR33',      index    y  ... UPDRS_22 UPDRS_23\n",
            "721  10538  2.0  ...      0.0      0.0\n",
            "722  10539  2.0  ...      0.0      0.0\n",
            "723  10540  2.0  ...      0.0      0.0\n",
            "724  10541  2.0  ...      0.0      0.0\n",
            "725  10542  2.0  ...      0.0      0.0\n",
            "..     ...  ...  ...      ...      ...\n",
            "787  10604  2.0  ...      0.0      0.0\n",
            "788  10605  2.0  ...      0.0      0.0\n",
            "789  10606  2.0  ...      0.0      0.0\n",
            "790  10607  2.0  ...      0.0      0.0\n",
            "791  10608  2.0  ...      0.0      0.0\n",
            "\n",
            "[71 rows x 17 columns])\n",
            "('SGR34',       index    y  ... UPDRS_22 UPDRS_23\n",
            "3547  22258  1.0  ...      0.0      0.0\n",
            "3548  22259  1.0  ...      0.0      0.0\n",
            "3549  22260  1.0  ...      0.0      0.0\n",
            "3550  22261  1.0  ...      0.0      0.0\n",
            "3551  22262  1.0  ...      0.0      0.0\n",
            "...     ...  ...  ...      ...      ...\n",
            "3657  22368  1.0  ...      0.0      0.0\n",
            "3658  22369  1.0  ...      0.0      0.0\n",
            "3659  22370  1.0  ...      0.0      0.0\n",
            "3660  22371  1.0  ...      0.0      0.0\n",
            "3661  22372  1.0  ...      0.0      0.0\n",
            "\n",
            "[115 rows x 17 columns])\n",
            "('SUK01',       index    y  ... UPDRS_22 UPDRS_23\n",
            "942   13213  0.0  ...      0.0      1.0\n",
            "943   13214  0.0  ...      0.0      1.0\n",
            "944   13215  0.0  ...      0.0      1.0\n",
            "945   13216  0.0  ...      0.0      1.0\n",
            "946   13217  0.0  ...      0.0      1.0\n",
            "...     ...  ...  ...      ...      ...\n",
            "3481  19152  0.0  ...      0.0      1.0\n",
            "3482  19153  0.0  ...      0.0      1.0\n",
            "3483  19154  0.0  ...      0.0      1.0\n",
            "3484  19155  0.0  ...      0.0      1.0\n",
            "3485  19156  0.0  ...      0.0      1.0\n",
            "\n",
            "[1248 rows x 17 columns])\n",
            "('SUK03',       index    y  ... UPDRS_22 UPDRS_23\n",
            "3699  22622  0.0  ...      3.0      1.0\n",
            "3700  22623  0.0  ...      3.0      1.0\n",
            "3701  22624  0.0  ...      3.0      1.0\n",
            "3702  22625  0.0  ...      3.0      1.0\n",
            "3703  22626  0.0  ...      3.0      1.0\n",
            "...     ...  ...  ...      ...      ...\n",
            "4113  23036  0.0  ...      3.0      1.0\n",
            "4114  23037  0.0  ...      3.0      1.0\n",
            "4115  23038  0.0  ...      3.0      1.0\n",
            "4116  23039  0.0  ...      3.0      1.0\n",
            "4117  23040  0.0  ...      3.0      1.0\n",
            "\n",
            "[419 rows x 17 columns])\n",
            "('SUK13',       index    y  ... UPDRS_22 UPDRS_23\n",
            "4118  23136  1.0  ...      1.0      0.0\n",
            "4119  23137  1.0  ...      1.0      0.0\n",
            "4120  23138  1.0  ...      1.0      0.0\n",
            "4121  23139  1.0  ...      1.0      0.0\n",
            "4122  23140  1.0  ...      1.0      0.0\n",
            "4123  23141  1.0  ...      1.0      0.0\n",
            "4124  23142  1.0  ...      1.0      0.0\n",
            "4125  23143  1.0  ...      1.0      0.0\n",
            "4126  23144  1.0  ...      1.0      0.0\n",
            "4127  23145  1.0  ...      1.0      0.0\n",
            "4128  23146  1.0  ...      1.0      0.0\n",
            "4129  23147  1.0  ...      1.0      0.0\n",
            "4130  23148  1.0  ...      1.0      0.0\n",
            "4131  23149  1.0  ...      1.0      0.0\n",
            "4132  23150  1.0  ...      1.0      0.0\n",
            "4133  23151  1.0  ...      1.0      0.0\n",
            "4134  23152  1.0  ...      1.0      0.0\n",
            "4135  23153  1.0  ...      1.0      0.0\n",
            "4136  23154  1.0  ...      1.0      0.0\n",
            "4137  23155  1.0  ...      1.0      0.0\n",
            "4138  23156  1.0  ...      1.0      0.0\n",
            "4139  23157  1.0  ...      1.0      0.0\n",
            "4140  23158  1.0  ...      1.0      0.0\n",
            "4141  23159  1.0  ...      1.0      0.0\n",
            "4142  23160  1.0  ...      1.0      0.0\n",
            "4143  23161  1.0  ...      1.0      0.0\n",
            "4144  23162  1.0  ...      1.0      0.0\n",
            "4145  23163  1.0  ...      1.0      0.0\n",
            "4146  23164  1.0  ...      1.0      0.0\n",
            "4147  23165  1.0  ...      1.0      0.0\n",
            "4148  23166  1.0  ...      1.0      0.0\n",
            "4149  23167  1.0  ...      1.0      0.0\n",
            "4150  23168  1.0  ...      1.0      0.0\n",
            "4151  23169  1.0  ...      1.0      0.0\n",
            "4152  23170  1.0  ...      1.0      0.0\n",
            "4153  23171  1.0  ...      1.0      0.0\n",
            "4154  23172  1.0  ...      1.0      0.0\n",
            "4155  23173  1.0  ...      1.0      0.0\n",
            "4156  23174  1.0  ...      1.0      0.0\n",
            "4157  23175  1.0  ...      1.0      0.0\n",
            "4158  23176  1.0  ...      1.0      0.0\n",
            "4159  23177  1.0  ...      1.0      0.0\n",
            "4160  23178  1.0  ...      1.0      0.0\n",
            "4161  23179  1.0  ...      1.0      0.0\n",
            "4162  23180  1.0  ...      1.0      0.0\n",
            "4163  23181  1.0  ...      1.0      0.0\n",
            "4164  23182  1.0  ...      1.0      0.0\n",
            "\n",
            "[47 rows x 17 columns])\n",
            "('SUK17',       index    y  ... UPDRS_22 UPDRS_23\n",
            "3486  19599  0.0  ...      2.0      1.0\n",
            "3487  19600  0.0  ...      2.0      1.0\n",
            "\n",
            "[2 rows x 17 columns])\n",
            "('SUK28',       index    y  ... UPDRS_22 UPDRS_23\n",
            "3488  19941  1.0  ...      0.0      0.0\n",
            "3489  19942  1.0  ...      0.0      0.0\n",
            "\n",
            "[2 rows x 17 columns])\n",
            "('SUK36',       index    y  ... UPDRS_22 UPDRS_23\n",
            "3490  20580  2.0  ...      0.0      0.0\n",
            "3491  20581  2.0  ...      0.0      0.0\n",
            "3492  20582  2.0  ...      0.0      0.0\n",
            "3493  20583  2.0  ...      0.0      0.0\n",
            "3494  20584  2.0  ...      0.0      0.0\n",
            "3495  20585  2.0  ...      0.0      0.0\n",
            "3496  20586  2.0  ...      0.0      0.0\n",
            "3497  20587  2.0  ...      0.0      0.0\n",
            "3498  20588  2.0  ...      0.0      0.0\n",
            "3499  20589  2.0  ...      0.0      0.0\n",
            "3500  20590  2.0  ...      0.0      0.0\n",
            "3501  20591  2.0  ...      0.0      0.0\n",
            "3502  20592  2.0  ...      0.0      0.0\n",
            "3503  20593  2.0  ...      0.0      0.0\n",
            "3504  20594  2.0  ...      0.0      0.0\n",
            "3505  20595  2.0  ...      0.0      0.0\n",
            "3506  20596  2.0  ...      0.0      0.0\n",
            "3507  20597  2.0  ...      0.0      0.0\n",
            "3508  20598  2.0  ...      0.0      0.0\n",
            "3509  20599  2.0  ...      0.0      0.0\n",
            "3510  20600  2.0  ...      0.0      0.0\n",
            "\n",
            "[21 rows x 17 columns])\n",
            "('SUK39',       index    y  ... UPDRS_22 UPDRS_23\n",
            "3662  22413  0.0  ...      1.0      1.0\n",
            "3663  22414  0.0  ...      1.0      1.0\n",
            "3664  22415  0.0  ...      1.0      1.0\n",
            "3665  22416  0.0  ...      1.0      1.0\n",
            "3666  22417  0.0  ...      1.0      1.0\n",
            "3667  22418  0.0  ...      1.0      1.0\n",
            "3668  22419  0.0  ...      1.0      1.0\n",
            "3669  22420  0.0  ...      1.0      1.0\n",
            "3670  22421  0.0  ...      1.0      1.0\n",
            "3671  22422  0.0  ...      1.0      1.0\n",
            "3672  22423  0.0  ...      1.0      1.0\n",
            "3673  22424  0.0  ...      1.0      1.0\n",
            "3674  22425  0.0  ...      1.0      1.0\n",
            "3675  22426  0.0  ...      1.0      1.0\n",
            "3676  22427  0.0  ...      1.0      1.0\n",
            "3677  22428  0.0  ...      1.0      1.0\n",
            "3678  22429  0.0  ...      1.0      1.0\n",
            "3679  22430  0.0  ...      1.0      1.0\n",
            "3680  22431  0.0  ...      1.0      1.0\n",
            "3681  22432  0.0  ...      1.0      1.0\n",
            "3682  22433  0.0  ...      1.0      1.0\n",
            "3683  22434  0.0  ...      1.0      1.0\n",
            "3684  22435  0.0  ...      1.0      1.0\n",
            "3685  22436  0.0  ...      1.0      1.0\n",
            "3686  22437  0.0  ...      1.0      1.0\n",
            "3687  22438  0.0  ...      1.0      1.0\n",
            "3688  22439  0.0  ...      1.0      1.0\n",
            "3689  22440  0.0  ...      1.0      1.0\n",
            "3690  22441  0.0  ...      1.0      1.0\n",
            "3691  22442  0.0  ...      1.0      1.0\n",
            "3692  22443  0.0  ...      1.0      1.0\n",
            "3693  22444  0.0  ...      1.0      1.0\n",
            "3694  22445  0.0  ...      1.0      1.0\n",
            "3695  22446  0.0  ...      1.0      1.0\n",
            "3696  22447  0.0  ...      1.0      1.0\n",
            "3697  22448  0.0  ...      1.0      1.0\n",
            "3698  22449  0.0  ...      1.0      1.0\n",
            "\n",
            "[37 rows x 17 columns])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NA_b9ESTF02D",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b4e3cc34-73f7-4403-de8d-3c21455acf57"
      },
      "source": [
        "\n",
        "\n",
        "df2.rename(columns={\"Study Subject ID\": \"Subject_id\"},inplace=True)\n",
        "# df2.Subject_id=df2.groupby(['ids','Subject_id']).Subject_id.ffill()\n",
        "df2['Subject_id'].isna().sum()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "18938"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gW6uzpNAafDS"
      },
      "source": [
        "df2[['y']]=df2[['y']].replace([0.0,1.0,2.0],[1,0,0])\n",
        "df2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MFGjbSNNrxN0"
      },
      "source": [
        "df2['X_flight'].isna().sum()\n",
        "df_new=df2.dropna(subset=['X_flight'])\n",
        "df2=df2.dropna(subset=['UPDRS_22', 'UPDRS_23','X_flight'])\n",
        "# df_train=df2[0:1120]\n",
        "# df_train['y'].value_counts()\n",
        "# df_train[['y']]=df_train[['y']].replace([0.0,1.0,2.0],[1,0,0])\n",
        "# df_train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qhvDBJKrF02G"
      },
      "source": [
        "# dff=df2[:995]\n",
        "df2=df2[31752:40513-94].reset_index() \n",
        "df2.drop(\"index\",axis=1,inplace=True)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "84ydWiqaF02J"
      },
      "source": [
        "\n",
        "df2['UPDRS_22'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lkHphbuHF02M",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1a7fcc50-23fb-4dea-f755-021ea5bb1d3c"
      },
      "source": [
        "# dff=dff.append(df2[df2.Subject_id == 'SUK38'])\n",
        "df2=df2[df2.Subject_id != 'SUK38']\n",
        "df2\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>y</th>\n",
              "      <th>X_flight</th>\n",
              "      <th>ids</th>\n",
              "      <th>X_hold</th>\n",
              "      <th>Subject_id</th>\n",
              "      <th>Protocol ID</th>\n",
              "      <th>Subject Status</th>\n",
              "      <th>Sex</th>\n",
              "      <th>UPDRS_22_UEXTR_RIGHT_E1_1_C1</th>\n",
              "      <th>UPDRS_22_UEXTR_LEFT_E1_1_C1</th>\n",
              "      <th>UPDRS_23_RIGHT_E1_1_C1</th>\n",
              "      <th>UPDRS_23_LEFT_E1_1_C1</th>\n",
              "      <th>UPDRS_31_E1_1_C1</th>\n",
              "      <th>SUM_PART_3_E1_1_C1</th>\n",
              "      <th>UPDRS_22</th>\n",
              "      <th>UPDRS_23</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2.0</td>\n",
              "      <td>[0.304, 0.206, 0.287, 0.28, 0.575, 0.148, 0.39...</td>\n",
              "      <td>0168568f68e1b02b</td>\n",
              "      <td>[0.10599999999976717, 0.11599999999452848, 0.1...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2.0</td>\n",
              "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
              "      <td>0168568f68e1b02b</td>\n",
              "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2.0</td>\n",
              "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
              "      <td>0168568f68e1b02b</td>\n",
              "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2.0</td>\n",
              "      <td>[0.312, 0.386, 0.608, 0.37, 0.295, 0.781, 0.26...</td>\n",
              "      <td>0168568f68e1b02b</td>\n",
              "      <td>[0.09000000002561137, 0.0650000000023283, 0.06...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2.0</td>\n",
              "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.247, 0.1...</td>\n",
              "      <td>0168568f68e1b02b</td>\n",
              "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1070000000181...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23666</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>58dab6321800bfe0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>SDE49</td>\n",
              "      <td>iprognosis-sdata-study - EK 451112017</td>\n",
              "      <td>available</td>\n",
              "      <td>m</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23667</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>8e9de02b429f50ef</td>\n",
              "      <td>NaN</td>\n",
              "      <td>SDE50</td>\n",
              "      <td>iprognosis-sdata-study - EK 451112017</td>\n",
              "      <td>available</td>\n",
              "      <td>f</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23668</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4365496b0b6b8342</td>\n",
              "      <td>NaN</td>\n",
              "      <td>SDE51</td>\n",
              "      <td>iprognosis-sdata-study - EK 451112017</td>\n",
              "      <td>available</td>\n",
              "      <td>f</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23669</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>c6ffffbbce0a8fe9</td>\n",
              "      <td>NaN</td>\n",
              "      <td>CNT_1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>m</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23670</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>ac2d2ebf8041e560</td>\n",
              "      <td>NaN</td>\n",
              "      <td>CNT_2</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>f</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>23666 rows × 16 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         y  ... UPDRS_23\n",
              "0      2.0  ...      NaN\n",
              "1      2.0  ...      NaN\n",
              "2      2.0  ...      NaN\n",
              "3      2.0  ...      NaN\n",
              "4      2.0  ...      NaN\n",
              "...    ...  ...      ...\n",
              "23666  NaN  ...      0.0\n",
              "23667  NaN  ...      2.0\n",
              "23668  NaN  ...      0.0\n",
              "23669  NaN  ...      0.0\n",
              "23670  NaN  ...      0.0\n",
              "\n",
              "[23666 rows x 16 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "chUBpOr_F02Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a363ea8d-248b-4323-c77e-d490e3832d20"
      },
      "source": [
        "\n",
        "!pip install plot-model"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting plot-model\n",
            "  Downloading https://files.pythonhosted.org/packages/62/b8/0967e30391a7c07002c5e7bca868763dcfd26808dcb13aba052a737aa01d/plot_model-0.20-py3-none-any.whl\n",
            "Installing collected packages: plot-model\n",
            "Successfully installed plot-model-0.20\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LnUmW_TwF02T"
      },
      "source": [
        "# df2 = df2[df2.groupby('Subject_id').ids.transform(len) < 3000]\n",
        "# dff=dff.append(df2[df2.groupby('Subject_id').Subject_id.transform(len) < 10])\n",
        "df2 = df2[df2.groupby('Subject_id').Subject_id.transform(len) >= 2]\n",
        "# dff"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bu32r8dWF02W"
      },
      "source": [
        "# df2['UPDRS_22'] = df2['UPDRS_22'].apply(np.int64)\n",
        "# df2['UPDRS_23'] = df2['UPDRS_23'].apply(np.int64)\n",
        "# df2['UPDRS_31_E1_1_C1'] = df2['UPDRS_31_E1_1_C1'].apply(np.int64)\n",
        "# df2['ids']\n",
        "\n",
        "df2=df2.reset_index()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zhZAvUCX6Os-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "51f7fe52-c0bc-4000-b1f1-b1487fa13dc7"
      },
      "source": [
        "df2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>y</th>\n",
              "      <th>X_flight</th>\n",
              "      <th>ids</th>\n",
              "      <th>X_hold</th>\n",
              "      <th>Subject_id</th>\n",
              "      <th>Protocol ID</th>\n",
              "      <th>Subject Status</th>\n",
              "      <th>Sex</th>\n",
              "      <th>UPDRS_22_UEXTR_RIGHT_E1_1_C1</th>\n",
              "      <th>UPDRS_22_UEXTR_LEFT_E1_1_C1</th>\n",
              "      <th>UPDRS_23_RIGHT_E1_1_C1</th>\n",
              "      <th>UPDRS_23_LEFT_E1_1_C1</th>\n",
              "      <th>UPDRS_31_E1_1_C1</th>\n",
              "      <th>SUM_PART_3_E1_1_C1</th>\n",
              "      <th>UPDRS_22</th>\n",
              "      <th>UPDRS_23</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>739</th>\n",
              "      <td>2.0</td>\n",
              "      <td>[0.911, 0.494, 0.635, 0.343, 0.326, 0.16, 0.43...</td>\n",
              "      <td>11a9e65564567d48</td>\n",
              "      <td>[0.04999999998835847, 0.07600000000093132, 0.0...</td>\n",
              "      <td>Leontios</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>m</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>740</th>\n",
              "      <td>2.0</td>\n",
              "      <td>[0.827, 0.604, 0.519, 0.142, 0.442, 0.557, 0.4...</td>\n",
              "      <td>11a9e65564567d48</td>\n",
              "      <td>[0.06700000003911555, 0.08300000004237518, 0.0...</td>\n",
              "      <td>Leontios</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>m</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>741</th>\n",
              "      <td>2.0</td>\n",
              "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
              "      <td>11a9e65564567d48</td>\n",
              "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
              "      <td>Leontios</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>m</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>742</th>\n",
              "      <td>2.0</td>\n",
              "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
              "      <td>11a9e65564567d48</td>\n",
              "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
              "      <td>Leontios</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>m</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>743</th>\n",
              "      <td>2.0</td>\n",
              "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
              "      <td>11a9e65564567d48</td>\n",
              "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
              "      <td>Leontios</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>m</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23180</th>\n",
              "      <td>1.0</td>\n",
              "      <td>[0.185, 0.242, 1.973, 0.477, 1.066, 0.134, 0.2...</td>\n",
              "      <td>f96144638dcc1018</td>\n",
              "      <td>[0.10199999995529652, 0.07500000018626451, 0.1...</td>\n",
              "      <td>SUK13</td>\n",
              "      <td>iprognosis-sdata-study - IRAS 237157</td>\n",
              "      <td>available</td>\n",
              "      <td>m</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23181</th>\n",
              "      <td>1.0</td>\n",
              "      <td>[0.513, 0.376, 0.541, 2.85, 0.234, 0.284, 0.25...</td>\n",
              "      <td>f96144638dcc1018</td>\n",
              "      <td>[0.08100000000558794, 0.08500000019557774, 0.0...</td>\n",
              "      <td>SUK13</td>\n",
              "      <td>iprognosis-sdata-study - IRAS 237157</td>\n",
              "      <td>available</td>\n",
              "      <td>m</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23182</th>\n",
              "      <td>1.0</td>\n",
              "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
              "      <td>f96144638dcc1018</td>\n",
              "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
              "      <td>SUK13</td>\n",
              "      <td>iprognosis-sdata-study - IRAS 237157</td>\n",
              "      <td>available</td>\n",
              "      <td>m</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23507</th>\n",
              "      <td>0.0</td>\n",
              "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
              "      <td>fa1ca92b6d69c880</td>\n",
              "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
              "      <td>SDE14</td>\n",
              "      <td>iprognosis-sdata-study - EK 451112017</td>\n",
              "      <td>available</td>\n",
              "      <td>m</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23508</th>\n",
              "      <td>0.0</td>\n",
              "      <td>[1.591, 0.83, 0.633, 2.93, 1.414, 2.256, 0.746...</td>\n",
              "      <td>fa1ca92b6d69c880</td>\n",
              "      <td>[0.14400000000023283, 0.16999999999825377, 0.0...</td>\n",
              "      <td>SDE14</td>\n",
              "      <td>iprognosis-sdata-study - EK 451112017</td>\n",
              "      <td>available</td>\n",
              "      <td>m</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>4624 rows × 16 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         y  ... UPDRS_23\n",
              "739    2.0  ...      0.0\n",
              "740    2.0  ...      0.0\n",
              "741    2.0  ...      0.0\n",
              "742    2.0  ...      0.0\n",
              "743    2.0  ...      0.0\n",
              "...    ...  ...      ...\n",
              "23180  1.0  ...      0.0\n",
              "23181  1.0  ...      0.0\n",
              "23182  1.0  ...      0.0\n",
              "23507  0.0  ...      0.0\n",
              "23508  0.0  ...      0.0\n",
              "\n",
              "[4624 rows x 16 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OxXkAxd0YkKu"
      },
      "source": [
        "a=np.zeros(30)\n",
        "df_tot=pd.DataFrame()\n",
        "for i in range (45801):\n",
        "    \n",
        "    j=35\n",
        "    \n",
        "    if (df2['X_flight'][i][0:j] != 0.0).any() :\n",
        "        print('True')\n",
        "        df_new=df2.loc[i]\n",
        "        df_tot=df_tot.append(df_new,ignore_index=True)\n",
        "\n",
        "    else :\n",
        "       print('False')\n",
        "       df_kek=df2.loc[i]\n",
        "       df_train=df_train.append(df_kek,ignore_index=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yM74sHbhZsry"
      },
      "source": [
        "df_tot"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FppyhFuAvDsk"
      },
      "source": [
        "# df_tot=df_tot.drop(columns=['level_0','index'])\n",
        "!pip install keras-self-attention"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E7hgrHmlY-73"
      },
      "source": [
        "# import tensorflow as tf\n",
        "# import keras\n",
        "\n",
        "# from keras.layers import Layer\n",
        "# from keras.layers import Conv2D, SeparableConv2D\n",
        "# from keras.layers import concatenate\n",
        "\n",
        "# from keras import initializers\n",
        "# from keras import backend as K\n",
        "# from keras.layers import Input\n",
        "# from keras.models import Model\n",
        "\n",
        "\n",
        "# class AttentionAugmentation2D(keras.models.Model):\n",
        "#     def __init__(self,Fout, k, dk, dv, Nh, relative=False):\n",
        "#         super(AttentionAugmentation2D,self).__init__()\n",
        "#         self.output_filters = Fout\n",
        "#         self.kernel_size = k\n",
        "#         self.depth_k = dk\n",
        "#         self.depth_v = dv\n",
        "#         self.num_heads = Nh\n",
        "#         self.relative = relative\n",
        "\n",
        "#         self.conv_out = SeparableConv2D(filters = Fout - dv,kernel_size = k,padding=\"same\")\n",
        "#         self.qkv = Conv2D(filters = 2*dk + dv,kernel_size= 1)\n",
        "#         self.attn_out_conv = Conv2D(filters = dv,kernel_size = 1)\n",
        "\n",
        "#     def call(self,inputs):\n",
        "#         out = self.conv_out(inputs)\n",
        "#         shape = K.int_shape(inputs)\n",
        "#         if None in shape:\n",
        "#             shape = [-1 if type(v)==type(None) else v for v in shape]\n",
        "#         Batch,H,W,_ = shape\n",
        "\n",
        "#         flat_q, flat_k, flat_v = self.compute_flat_qkv(inputs)\n",
        "#         dkh = self.depth_k//self.num_heads\n",
        "#         logits = tf.matmul(flat_q, flat_k, transpose_b=True)\n",
        "#         if self.relative:\n",
        "#             h_rel_logits, w_rel_logits = self.relative_logits(self.q, H, W, self.num_heads,dkh)\n",
        "#             logits += h_rel_logits\n",
        "#             logits += w_rel_logits\n",
        "        \n",
        "#         weights = K.softmax(logits,axis=-1)\n",
        "#         attn_out = tf.matmul(weights,flat_v)\n",
        "#         attn_out = K.reshape(attn_out, [Batch, self.num_heads, H, W, self.depth_v // self.num_heads])\n",
        "\n",
        "#         attn_out = self.combine_heads_2d(attn_out)\n",
        "#         attn_out = self.attn_out_conv(attn_out)\n",
        "#         output =  concatenate([out,attn_out],axis=3)\n",
        "#         return output\n",
        "\n",
        "#     def combine_heads_2d(self,x):\n",
        "#         # [batch, num_heads, height, width, depth_v // num_heads]\n",
        "#         transposed = K.permute_dimensions(x, [0, 2, 3, 1, 4])\n",
        "#         # [batch, height, width, num_heads, depth_v // num_heads]\n",
        "#         shape = K.int_shape(transposed)\n",
        "#         if None in shape:\n",
        "#             shape = [-1 if type(v)==type(None) else v for v in shape]\n",
        "#         batch, h , w, a , b = shape \n",
        "#         ret_shape = [batch, h ,w, a*b]\n",
        "#         # [batch, height, width, depth_v]\n",
        "#         return K.reshape(transposed, ret_shape)\n",
        "\n",
        "#     def rel_to_abs(self,x):\n",
        "#         shape = K.shape(x)\n",
        "#         shape = [shape[i] for i in range(3)]\n",
        "#         B, Nh, L = shape\n",
        "#         col_pad = K.zeros((B, Nh, L, 1))\n",
        "#         x = K.concatenate([x, col_pad], axis=3)\n",
        "#         flat_x = K.reshape(x, [B, Nh, L * 2 * L])\n",
        "#         flat_pad = K.zeros((B, Nh, L-1))\n",
        "#         flat_x_padded = K.concatenate([flat_x, flat_pad], axis=2)\n",
        "#         final_x = K.reshape(flat_x_padded, [B, Nh, L+1, 2*L-1])\n",
        "#         final_x = final_x[:, :, :L, L-1:]\n",
        "#         return final_x\n",
        "\n",
        "#     def relative_logits_1d(self, q, rel_k, H, W, Nh, transpose_mask):\n",
        "#         rel_logits = tf.einsum('bhxyd,md->bhxym', q, rel_k)\n",
        "#         rel_logits = K.reshape(rel_logits, [-1, Nh*H, W, 2*W-1])\n",
        "#         rel_logits = self.rel_to_abs(rel_logits)\n",
        "#         rel_logits = K.reshape(rel_logits, [-1, Nh, H, W, W])\n",
        "#         rel_logits = K.expand_dims(rel_logits, axis=3)\n",
        "#         rel_logits = K.tile(rel_logits, [1, 1, 1, H, 1, 1])\n",
        "#         rel_logits = K.permute_dimensions(rel_logits, transpose_mask)\n",
        "#         rel_logits = K.reshape(rel_logits, [-1, Nh, H*W, H*W])\n",
        "#         return rel_logits\n",
        "\n",
        "\n",
        "#     def relative_logits(self,q,H, W, Nh,dkh):\n",
        "#         key_rel_w  = K.random_normal(shape = (int(2 * W - 1),dkh))\n",
        "#         key_rel_h  = K.random_normal(shape = (int(2 * H - 1),dkh))\n",
        "\n",
        "#         rel_logits_w = self.relative_logits_1d(q,key_rel_w, H, W, Nh, [0, 1, 2, 4, 3, 5])\n",
        "\n",
        "#         rel_logits_h = self.relative_logits_1d(K.permute_dimensions(q, [0, 1, 3, 2, 4]),key_rel_h, W, H, Nh, [0, 1, 4, 2, 5, 3])\n",
        "\n",
        "#         return rel_logits_h , rel_logits_w\n",
        "\n",
        "#     def split_heads_2d(self,q,Nh):\n",
        "#         batch, height,width,channels = K.int_shape(q)\n",
        "#         ret_shape = [-1,height,width,Nh,channels//Nh]\n",
        "#         split = K.reshape(q,ret_shape)\n",
        "#         transpose_axes = (0, 3, 1, 2, 4)\n",
        "#         split = K.permute_dimensions(split, transpose_axes)\n",
        "#         return split\n",
        "\n",
        "\n",
        "#     def compute_flat_qkv(self,inputs):\n",
        "\n",
        "#         qkv = self.qkv(inputs)\n",
        "#         B,H,W,_ = K.int_shape(inputs)\n",
        "#         q,k,v = tf.split(qkv,[self.depth_k,self.depth_k,self.depth_v],axis=3)\n",
        "\n",
        "#         dkh = self.depth_k // self.num_heads\n",
        "#         dvh = self.depth_v // self.num_heads\n",
        "#         q *= dkh ** -0.5\n",
        "\n",
        "#         self.q = self.split_heads_2d(q, self.num_heads)\n",
        "#         self.k = self.split_heads_2d(k, self.num_heads)\n",
        "#         self.v = self.split_heads_2d(v, self.num_heads)\n",
        "\n",
        "#         flat_q = K.reshape(q, [-1, self.num_heads, H * W, dkh])\n",
        "#         flat_k = K.reshape(k, [-1, self.num_heads, H * W, dkh])\n",
        "#         flat_v = K.reshape(v, [-1, self.num_heads, H * W, dvh])\n",
        "\n",
        "#         return flat_q, flat_k, flat_v\n",
        "    \n",
        "#     def compute_output_shape(self, input_shape):\n",
        "#         output_shape = list(input_shape)\n",
        "#         output_shape[-1] = self.output_filters\n",
        "#         return tuple(output_shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8TSbDBSGHeB-"
      },
      "source": [
        "\n",
        "temp1 = df2['X_flight'].apply(pd.Series)\n",
        "temp2 = df2['X_hold'].apply(pd.Series)\n",
        "npa=np.dstack((temp1,temp2))\n",
        "npa = npa.swapaxes(1,2)\n",
        "npa=np.nan_to_num(npa)\n",
        "npa=npa.reshape(-1 , 2, 100 , 1)\n",
        "X=npa\n",
        "X = np.asarray(X).astype(np.float32)\n",
        "temp4= np.asarray(df2['y'])\n",
        "\n",
        "\n",
        "\n",
        "#         temp7=temp7.reshape(-1,3,1,1)\n",
        "y=temp4\n",
        "y = np.asarray(y).astype(np.float32)\n",
        "\n",
        "\n",
        "        \n",
        "\n",
        "  # Encoder Layers\n",
        "\n",
        "       \n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "   X, y, test_size=0.2, random_state=42)\n",
        "        \n",
        "          \n",
        "          # autoencoder.add(MaxPooling2D((1, 2), padding='same'))\n",
        "Visible=Input(X.shape[1:])\n",
        "                                       \n",
        "\n",
        "\n",
        "x=((Conv2D(8, (2,3) ,activation='elu', padding='same')))(Visible)\n",
        "x=((Conv2D(8, (1,1) ,activation='elu', padding='same')))(x)\n",
        "x=((Conv2D(8, (2,3) ,activation='elu', padding='same')))(x)\n",
        "x=((MaxPooling2D( (1,2), padding='same')))(x)  \n",
        "x=((Conv2D(8, (2,3) ,activation='elu', padding='same')))(x)\n",
        "\n",
        "x=((Conv2D(8, (1,1) ,activation='elu', padding='same')))(x)\n",
        "x=((Conv2D(8, (2,3) ,activation='elu', padding='same')))(x)\n",
        "\n",
        "x=((MaxPooling2D( (1,2), padding='same')))(x)  \n",
        "x=((Conv2D(8, (2,3) ,activation='elu', padding='same')))(x)\n",
        "\n",
        "x=((Conv2D(8, (1,1) ,activation='elu', padding='same')))(x)\n",
        "x=((Conv2D(8, (2,3) ,activation='elu', padding='same')))(x)\n",
        "x=((MaxPooling2D( (1,2), padding='same')))(x)  \n",
        "x=((Conv2D(16, (2,3) ,activation='elu', padding='same')))(x)\n",
        "\n",
        "x=((Conv2D(16, (1,1) ,activation='elu', padding='same')))(x)\n",
        "x=((Conv2D(16, (2,3) ,activation='elu', padding='same')))(x)\n",
        "\n",
        "x=((MaxPooling2D( (1,2), padding='same')))(x)  \n",
        "x=((Conv2D(32, (2,3) ,activation='elu', padding='same')))(x)\n",
        "\n",
        "x=((Conv2D(32, (1,1) ,activation='elu', padding='same')))(x)\n",
        "x=((Conv2D(32, (2,3) ,activation='elu' ,padding='same')))(x)\n",
        "\n",
        "x=((MaxPooling2D( (1,2), padding='same')))(x)  \n",
        "x=((Conv2D(32, (2,3) ,activation='elu', padding='same')))(x)\n",
        "\n",
        "x=((Conv2D(32, (1,1) ,activation='elu', padding='same')))(x)\n",
        "x=((Conv2D(32, (2,3) , padding='same')))(x)\n",
        "# x=(BatchNormalization())(x)                \n",
        "# x=(Activation('relu'))(x)\n",
        "# \n",
        "# x=((MaxPooling2D( (1,2), padding='same')))(x)  \n",
        "# x=((Conv2D(256, (2,2) , padding='same')))(x)\n",
        "\n",
        "\n",
        "\n",
        "H=(Activation('elu'))(x) \n",
        "\n",
        "# x=((MaxPooling2D( (1,2), padding='same')))(x)  \n",
        "# x=((Conv2D(128, (2,3) , padding='same')))(x)\n",
        "# x=(BatchNormalization())(x)                \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# x=(Activation('relu'))(x) \n",
        "\n",
        "\n",
        "x=(Flatten())(H)  \n",
        "\n",
        "\n",
        "\n",
        "  \n",
        "   \n",
        "\n",
        "# x=(Dense(64))(x)\n",
        "        # model.add(GaussianNoise(0.05))\n",
        "        # x=(BatchNormalization())(x)\n",
        "# x=(Activation('relu'))(x)\n",
        "# x=(Dropout(0.3))(x)\n",
        "\n",
        "x=(Dense(128))(x)\n",
        "# x=(BatchNormalization())(x)\n",
        "x=(Activation('elu'))(x)\n",
        "# x=(Dropout(0.25))(x)\n",
        "\n",
        "attention=(Flatten())(H)        \n",
        "attention=(Dense(128))(attention)\n",
        "# attention=BatchNormalization()(attention)\n",
        "attention=(Activation('tanh'))(attention)\n",
        "attention=Softmax()(attention)\n",
        "# attention=(Dropout(0.25))(attention)\n",
        "# attention=(Dropout(0.3))(attention)\n",
        "# attention=(Dense(1))(attention)\n",
        "\n",
        "\n",
        "\n",
        "        \n",
        "output=tensorflow.keras.layers.Concatenate()([attention,x])\n",
        "\n",
        "output=(Dense(1))(output)\n",
        "output=(Activation('sigmoid'))(output)\n",
        "\n",
        "        \n",
        "\n",
        "model = Model(Visible,outputs=[output])\n",
        "opt3=tensorflow.keras.optimizers.Adam(learning_rate=8*1e-4,decay=1e-6,amsgrad=True)\n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer=opt3, metrics=['accuracy'])\n",
        "model.summary()\n",
        "history=model.fit(X_train,y_train,batch_size=64,steps_per_epoch=np.floor(X_train.shape[0]/64).astype('int'),\n",
        "                epochs=40,verbose=2,validation_data=(X_test,y_test))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AwFP4AhSl4O9"
      },
      "source": [
        "from sklearn.metrics import roc_curve\n",
        "y_pred_keras = model.predict(X_test).ravel()\n",
        "fpr_keras, tpr_keras, thresholds_keras = roc_curve(y_test, y_pred)\n",
        "\n",
        "from sklearn.metrics import auc\n",
        "auc_keras = auc(fpr_keras, tpr_keras)\n",
        "\n",
        "plt.figure(1)\n",
        "plt.plot([0, 1], [0, 1], 'k--')\n",
        "plt.plot(fpr_keras, tpr_keras, label='Keras (area = {:.3f})'.format(auc_keras))\n",
        "\n",
        "plt.xlabel('False positive rate')\n",
        "plt.ylabel('True positive rate')\n",
        "plt.title('ROC curve')\n",
        "plt.legend(loc='best')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mdRH-SZDQX-D"
      },
      "source": [
        "y_test=np.asarray(scores1)\n",
        "# y_test=y_test >0.5 \n",
        "# y_test=y_test.astype(int)\n",
        "y_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CsMzh3kqRHKB"
      },
      "source": [
        "y_pred=np.asarray(scores2)\n",
        "y_pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EZJ_3wmavz7T"
      },
      "source": [
        "import json\n",
        "from tensorflow.keras.layers import multiply,Permute,Dot\n",
        "from tensorflow.keras.layers import GaussianNoise   \n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from tensorflow.keras.models import model_from_json,load_model\n",
        "import numpy as np\n",
        "import sklearn\n",
        "from sklearn.model_selection import GroupKFold\n",
        "group_kfold = GroupKFold(n_splits=220)\n",
        "groups=dff['ids']\n",
        "GroupKFold(n_splits=220)\n",
        "scores=[]\n",
        "scores1=[]\n",
        "scores2=[]\n",
        "scores3=[]\n",
        "scores4=[]\n",
        "i=1\n",
        "for train_index, test_index in group_kfold.split(dff,dff, groups):\n",
        "\n",
        "        temp1 = dff['X_flight'].apply(pd.Series)\n",
        "        temp2 = dff['X_hold'].apply(pd.Series)\n",
        "\n",
        "        npa=np.dstack((temp1,temp2))\n",
        "        npa = npa.swapaxes(1,2)\n",
        "        npa=np.nan_to_num(npa)\n",
        "        npa=npa.reshape(-1 ,2, 100, 1)\n",
        "        X=npa\n",
        "        X = np.asarray(X).astype(np.float32)\n",
        "        temp4= np.asarray(dff['y'])\n",
        "\n",
        "        temp4=temp4.reshape(-1,1)\n",
        "\n",
        "        # PD=np.asarray(PD).astype(np.int32)\n",
        "#         temp7=temp7.reshape(-1,3,1,1)\n",
        "        y=temp4\n",
        "        y = np.asarray(y).astype(np.int32)\n",
        "        print(y.shape)\n",
        "\n",
        "\n",
        "        X_train, X_test = X[train_index], X[test_index]\n",
        "        y_train, y_test = y[train_index], y[test_index]\n",
        "\n",
        "        # X_train, X_eval, y_train, y_eval = train_test_split(\n",
        "        #   X_train, y_train, test_size=0.15, random_state=42) \n",
        "        # loaded_model.load_weights(\"/content/CNN100_deep4.h5\")\n",
        "        initializer = tensorflow.keras.initializers.Constant(0.01)\n",
        "        \n",
        "        Visible=Input(X.shape[1:])\n",
        "                                              \n",
        "\n",
        "\n",
        "        x=((Conv2D(8, (2,2) ,activation='relu', padding='same')))(Visible)\n",
        "        # x=BatchNormalization()(x)\n",
        "        x=((MaxPooling2D( (1,2), padding='same')))(x)  \n",
        "\n",
        "\n",
        "        x=((Conv2D(12, (2,2) , padding='same')))(x)\n",
        "\n",
        "        # x=BatchNormalization()(x)\n",
        "\n",
        "\n",
        "        # x=(BatchNormalization())(x)                \n",
        "        x=(Activation('relu'))(x)\n",
        "        # \n",
        "        x=((MaxPooling2D( (1,2), padding='same')))(x)  \n",
        "        x=((Conv2D(18, (2,2) , padding='same')))(x)\n",
        "        # x=BatchNormalization()(x)\n",
        "\n",
        "        x=(Activation('relu'))(x)\n",
        "        # \n",
        "        x=((MaxPooling2D( (1,2), padding='same')))(x)  \n",
        "        x=((Conv2D(24, (2,2) , padding='same')))(x)\n",
        "        # x=BatchNormalization()(x)\n",
        "\n",
        "        x=(Activation('relu'))(x)\n",
        "        # \n",
        "        x=((MaxPooling2D( (1,2), padding='same')))(x)  \n",
        "        x=((Conv2D(36, (2,2) , padding='same')))(x)\n",
        "        # x=BatchNormalization()(x)\n",
        "\n",
        "\n",
        "\n",
        "        H=(Activation('relu'))(x) \n",
        "\n",
        "        # x=((MaxPooling2D( (1,2), padding='same')))(x)  \n",
        "        # x=((Conv2D(128, (2,3) , padding='same')))(x)\n",
        "        # x=(BatchNormalization())(x)                \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        # x=(Activation('relu'))(x) \n",
        "\n",
        "\n",
        "        x=(GlobalAveragePooling2D())(H)  \n",
        "\n",
        "\n",
        "\n",
        "          \n",
        "          \n",
        "\n",
        "        # x=(Dense(64))(x)\n",
        "                # model.add(GaussianNoise(0.05))\n",
        "                # x=(BatchNormalization())(x)\n",
        "        # x=(Activation('relu'))(x)\n",
        "        # x=(Dropout(0.3))(x)\n",
        "\n",
        "        x=(Dense(64))(x)\n",
        "        # x=(BatchNormalization())(x)\n",
        "        x=(Activation('relu'))(x)\n",
        "        x=(Dropout(0.25))(x)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "                \n",
        "        \n",
        "\n",
        "        output=(Dense(1))(x)\n",
        "        output=(Activation('sigmoid'))(output)\n",
        "\n",
        "                \n",
        "\n",
        "        model = Model(Visible,outputs=[output])\n",
        "        opt3=tensorflow.keras.optimizers.Adam(learning_rate=5*1e-4)\n",
        "\n",
        "        model.compile(loss='binary_crossentropy', optimizer=opt3, metrics=['accuracy'])\n",
        "        model.summary()\n",
        "        history=model.fit(X_train,y_train,batch_size=128,shuffle=True,\n",
        "                        epochs=40,verbose=2,validation_data=(X_test,y_test))\n",
        "                                       \n",
        "\n",
        "\n",
        "       \n",
        "\n",
        "\n",
        "\n",
        " \n",
        "\n",
        "        # saved_model = load_model('/content/gdrive/My Drive/models/best_model_15_%d.h5'%(i))\n",
        "\n",
        "        y_hat = model.predict(X_test)\n",
        "        print (np.median(y_hat))\n",
        "        if np.median(y_hat) >= 0.5 :\n",
        "          y_hat1=1\n",
        "        else :\n",
        "          y_hat1=0\n",
        "        \n",
        "\n",
        "        \n",
        "        y_test1=np.mean(y_test)\n",
        "\n",
        "\n",
        "        scores1.append(y_test1)\n",
        "        print(y_test1)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        print(y_hat1)\n",
        "        scores2.append(y_hat1)\n",
        "\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "brZMAMKGZsl-"
      },
      "source": [
        "\n",
        "for i in range (6651) :\n",
        "    \n",
        "    for j in range (1,98) :\n",
        "    \n",
        "      if df_tot['X_flight'][i][j] > 3.0 :\n",
        "          print('True')\n",
        "          df_tot['X_flight'][i][j] = np.median(df_tot['X_flight'][i][30:70])\n",
        "      # if (df_tot['X_hold'][i][j])  >0.3 : \n",
        "      #     df_tot['X_hold'][i][j]= np.median(df_tot['X_hold'][i][30:70])\n",
        "\n",
        "          "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MjSY3qXE2HBz"
      },
      "source": [
        "df_tot=df2\n",
        "# df_tot=df_tot[(df_tot['UPDRS_23'] < 3) ]\n",
        "# df_tot=df_tot[(df_tot['UPDRS_22'] < 6 ) ]\n",
        "df_tot=df_tot.reset_index()\n",
        "\n",
        "# df_new=df2\n",
        "# df_new=df_new[(df_new['UPDRS_23'] >= 3) ]\n",
        "# df_new1=df_new[(df_new['UPDRS_22'] >= 3) ]\n",
        "# df_new=df_new.append(df_new1)\n",
        "# df_tot=df_tot.reset_index()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hRnfaPLIOP1V",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 555
        },
        "outputId": "7aa19715-78ff-4555-ce46-d195574af139"
      },
      "source": [
        "df_tot[(df_tot['Subject_id','y'])]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2890\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2891\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2892\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: ('Subject_id', 'y')",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-654e8d05322c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_tot\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_tot\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Subject_id'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'y'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2900\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2901\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2902\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2903\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2904\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2891\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2892\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2893\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2894\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2895\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtolerance\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: ('Subject_id', 'y')"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M2g-QSa1pzVL"
      },
      "source": [
        "df_tot2=df2\n",
        "df_tot2=df_tot2[(df_tot2['UPDRS_23'] < 3) ]\n",
        "df_tot2=df_tot2[(df_tot2['UPDRS_22'] < 3 ) ]\n",
        "\n",
        "df_new2=df2\n",
        "df_new2=df_new[(df_new['UPDRS_23'] >= 3) ]\n",
        "df_new1=df_new[(df_new['UPDRS_22'] >= 3) ]\n",
        "df_new2=df_new.append(df_new1)\n",
        "df_to2t=df_tot2.reset_index()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GXATujiTtfJ3"
      },
      "source": [
        "df_tot2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "05EXFkYhF02Z"
      },
      "source": [
        "import json\n",
        "from tensorflow.keras.models import model_from_json\n",
        "json_file = open('/content/gdrive/My Drive/thesis/autoencoders/CNN100_deep.json', 'r')\n",
        "loaded_model_json = json_file.read()\n",
        "json_file.close()\n",
        "loaded_model = model_from_json(loaded_model_json)\n",
        "# load weights into new model\n",
        "loaded_model.load_weights(\"/content/gdrive/My Drive/CNN100_deep.h5\")\n",
        "print(\"Loaded model from disk\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OHoB_t8NF02b"
      },
      "source": [
        "loaded_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KI0CECD0kIke"
      },
      "source": [
        "df2['X_flight'].all()==0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SOeBIUKnz5R9"
      },
      "source": [
        "from tensorflow.keras.utils import plot_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K5BjGkjJF02e"
      },
      "source": [
        "data_dem"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CFJeov4_F02i"
      },
      "source": [
        "data_dem['UPDRS 22'] = data_dem['UPDRS_III Item 22 Rigidity-Right hand'] + data_dem['UPDRS_III Item 22 Rigidity-Left hand']\n",
        "data_dem['UPDRS 23']= data_dem['UPDRS_III Item 23 Finger Taps-Right hand'] + data_dem['UPDRS_III Item 23 Finger Taps-Left hand']\n",
        "data_dem.rename(columns={'UPDRS_III Item 31 Body Bradykinesia/Hypokinesia' : 'UPDRS 31','Subject ID':\"ID\"},inplace=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MnYP4il5YdIi"
      },
      "source": [
        "!pip install keras-self-attention"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WvO__k6IzAu5"
      },
      "source": [
        "df_tot['UPDRS_23'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Kh2-Bby4-x0"
      },
      "source": [
        "df_tot['UPDRS_22'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S-aJapgM0RCJ"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lhxRKhRmap8H"
      },
      "source": [
        "class_weight={0:1.2,1:1,2:3,3:3,4:20,5:50,6:50}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RfKu-VYtF02k"
      },
      "source": [
        "data_dem"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r2yX9UW8F02o"
      },
      "source": [
        "df2['ids'].unique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1KfV6dOPF02q"
      },
      "source": [
        "\n",
        "df2f2 =df2.sample( frac=0.5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_P1T1h3VF02t"
      },
      "source": [
        "        temp4= np.asarray(df2['UPDRS_22'])\n",
        "        temp5= np.asarray(df2['UPDRS_23'])\n",
        "        temp6= np.asarray(df2['UPDRS_31_E1_1_C1'])\n",
        "        temp7=np.dstack((temp4,temp5,temp6))\n",
        "        temp7=temp7.swapaxes(0,1).swapaxes(1,2)\n",
        "        temp7.shape\n",
        "        temp7=temp7.reshape(-1,3)\n",
        "        temp7.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HXzD4DyWF02v"
      },
      "source": [
        "df2['ids'].value_counts()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Hmvk8tlF02y"
      },
      "source": [
        "\n",
        "        \n",
        "temp1 = df_tot['X_flight'].apply(pd.Series)\n",
        "temp2 = df_tot['X_hold'].apply(pd.Series)\n",
        "npa=np.dstack((temp1,temp2))\n",
        "npa = npa.swapaxes(1,2)\n",
        "npa=np.nan_to_num(npa)\n",
        "npa=npa.reshape(-1 , 2, 100 , 1)\n",
        "X=npa\n",
        "temp4= np.asarray(df_tot['UPDRS_22'])\n",
        "temp5= np.asarray(df_tot['UPDRS_23'])\n",
        "temp6= np.asarray(df_tot['UPDRS_31_E1_1_C1'])\n",
        "\n",
        "temp7=np.dstack((temp4,temp5,temp6))\n",
        "temp7=np.nan_to_num(temp7)\n",
        "temp7=temp7.swapaxes(0,1).swapaxes(1,2)\n",
        "temp7=temp7.reshape(-1,3)\n",
        "#         temp7=temp7.reshape(-1,3,1,1)\n",
        "y=temp7\n",
        "\n",
        "       "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e7v672lAF021"
      },
      "source": [
        "temp7=temp7.reshape(-1,3,1,1)\n",
        "temp7.shape[1:]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i9J-Jx7pF023"
      },
      "source": [
        "\n",
        "np.isnan().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FvsnvczAF026"
      },
      "source": [
        "import tensorflow as tensorflow\n",
        "callback = tensorflow.keras.callbacks.EarlyStopping(monitor='val_loss', patience=70)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MjtQKweVF028"
      },
      "source": [
        "import sys,os,os.path\n",
        "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XF_fBzKVxQ1w"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "znBYtkO6ntYl"
      },
      "source": [
        " def scheduler(epoch, lr):\n",
        "   if epoch < 50 :\n",
        "    return lr\n",
        "   elif epoch >50 and epoch <100 :\n",
        "     lr=0.0005\n",
        "     return lr\n",
        "   else :\n",
        "     lr=0.00001\n",
        "     return lr\n",
        "callback2 = tensorflow.keras.callbacks.LearningRateScheduler(scheduler)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-YAr00_DgOWe"
      },
      "source": [
        "from tensorflow.keras.regularizers import l1,l2\n",
        "\n",
        "from sklearn.preprocessing import Normalizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ansBrnT16h79"
      },
      "source": [
        "!pip install tf-nightly"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nGsid8C3HufJ"
      },
      "source": [
        "# !sudo pip install imbalanced-learn\n",
        "!pip install -U scikit-learn\n",
        "!pip install -U imbalanced-learn\n",
        "import imblearn\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.keras import balanced_batch_generator\n",
        "from imblearn.under_sampling import NearMiss\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9LTufJCjcv01"
      },
      "source": [
        "import random\n",
        "import numpy as np\n",
        "\n",
        "def balanced_batch_generator(x, y, batch_size, categorical=True):\n",
        "    \"\"\"A generator for creating balanced batched.\n",
        "    This generator loops over its data indefinitely and yields balanced,\n",
        "    shuffled batches.\n",
        "    Args:\n",
        "    x (numpy.ndarray): Samples (inputs). Must have the same length as `y`.\n",
        "    y (numpy.ndarray): Labels (targets). Must be a binary class matrix (i.e.,\n",
        "        shape `(num_samples, num_classes)`). You can use `keras.utils.to_categorical`\n",
        "        to convert a class vector to a binary class matrix.\n",
        "    batch_size (int): Batch size.\n",
        "    categorical (bool, optional): If true, generates binary class matrices\n",
        "        (i.e., shape `(num_samples, num_classes)`) for batch labels (targets).\n",
        "        Otherwise, generates class vectors (i.e., shape `(num_samples, )`).\n",
        "        Defaults to `True`.\n",
        "    Returns a generator yielding batches as tuples `(inputs, targets)` that can\n",
        "        be directly used with Keras.\n",
        "    \"\"\"\n",
        "    if x.shape[0] != y.shape[0]:\n",
        "        raise ValueError('Args `x` and `y` must have the same length.')\n",
        "    if len(y.shape) != 2:\n",
        "        raise ValueError(\n",
        "            'Arg `y` must have a shape of (num_samples, num_classes). ' +\n",
        "            'You can use `keras.utils.to_categorical` to convert a class vector ' +\n",
        "            'to a binary class matrix.'\n",
        "        )\n",
        "    if batch_size < 1:\n",
        "        raise ValueError('Arg `batch_size` must be a positive integer.')\n",
        "    num_samples = y.shape[0]\n",
        "    num_classes = y.shape[2]\n",
        "    batch_x_shape = (batch_size, *x.shape[1:])\n",
        "    batch_y_shape = (batch_size, num_classes) if categorical else (batch_size, )\n",
        "    indexes = [0 for _ in range(num_classes)]\n",
        "    samples = [[] for _ in range(num_classes)]\n",
        "    for i in range(num_samples):\n",
        "        samples[np.argmax(y[i])].append(x[i])\n",
        "    while True:\n",
        "        batch_x = np.ndarray(shape=batch_x_shape, dtype=x.dtype)\n",
        "        batch_y = np.zeros(shape=batch_y_shape, dtype=y.dtype)\n",
        "        for i in range(batch_size):\n",
        "            random_class = random.randrange(num_classes)\n",
        "            current_index = indexes[random_class]\n",
        "            indexes[random_class] = (current_index + 1) % len(samples[random_class])\n",
        "            if current_index == 0:\n",
        "                random.shuffle(samples[random_class])\n",
        "            batch_x[i] = samples[random_class][current_index]\n",
        "            if categorical:\n",
        "                batch_y[i][random_class] = 1\n",
        "            else:\n",
        "                batch_y[i] = random_class\n",
        "        return (batch_x, batch_y)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PP4-gNMQYjCb"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "temp1 = df2['X_flight'].apply(pd.Series)\n",
        "temp2 = df2['X_hold'].apply(pd.Series)\n",
        "npa=np.dstack((temp1,temp2))\n",
        "npa = npa.swapaxes(1,2)\n",
        "npa=np.nan_to_num(npa)\n",
        "npa=npa.reshape(-1 , 2, 100 , 1)\n",
        "X=npa\n",
        "X = np.asarray(X).astype(np.float32)\n",
        "temp4= np.asarray(df2['UPDRS_22'])\n",
        "temp5= np.asarray(df2['UPDRS_23'])\n",
        "temp6= np.asarray(df2['UPDRS_31_E1_1_C1'])\n",
        "temp7=np.dstack((temp4,temp5,temp6))\n",
        "temp7=np.nan_to_num(temp7)\n",
        "print(temp7.shape)\n",
        "temp7=temp7.reshape(-1,3)\n",
        "\n",
        "#         temp7=temp7.reshape(-1,3,1,1)\n",
        "y=temp7\n",
        "y = np.asarray(y).astype(np.float32)\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        " X, y, test_size=0.2, random_state=42)       \n",
        "\n",
        "  # Encoder Layers\n",
        "\n",
        "       \n",
        "        \n",
        "print(X.shape)\n",
        "        \n",
        "          \n",
        "          # autoencoder.add(MaxPooling2D((1, 2), padding='same'))\n",
        "Visible=Input(X.shape[1:])\n",
        "                                       \n",
        "\n",
        "\n",
        "x=((Conv2D(8, (2,3) ,activation='elu', padding='same')))(Visible)\n",
        "x=((Conv2D(8, (1,1) ,activation='elu', padding='same')))(x)\n",
        "x=((Conv2D(8, (2,3) ,activation='elu', padding='same')))(x)\n",
        "x=((MaxPooling2D( (1,2), padding='same')))(x)  \n",
        "x=((Conv2D(8, (2,3) ,activation='elu', padding='same')))(x)\n",
        "\n",
        "x=((Conv2D(8, (1,1) ,activation='elu', padding='same')))(x)\n",
        "x=((Conv2D(8, (2,3) ,activation='elu', padding='same')))(x)\n",
        "\n",
        "x=((MaxPooling2D( (1,2), padding='same')))(x)  \n",
        "x=((Conv2D(8, (2,3) ,activation='elu', padding='same')))(x)\n",
        "\n",
        "x=((Conv2D(8, (1,1) ,activation='elu', padding='same')))(x)\n",
        "x=((Conv2D(8, (2,3) ,activation='elu', padding='same')))(x)\n",
        "x=((MaxPooling2D( (1,2), padding='same')))(x)  \n",
        "x=((Conv2D(16, (2,3) ,activation='elu', padding='same')))(x)\n",
        "\n",
        "x=((Conv2D(16, (1,1) ,activation='elu', padding='same')))(x)\n",
        "x=((Conv2D(16, (2,3) ,activation='elu', padding='same')))(x)\n",
        "\n",
        "x=((MaxPooling2D( (1,2), padding='same')))(x)  \n",
        "x=((Conv2D(32, (2,3) ,activation='elu', padding='same')))(x)\n",
        "\n",
        "x=((Conv2D(32, (1,1) ,activation='elu', padding='same')))(x)\n",
        "x=((Conv2D(32, (2,3) ,activation='elu' ,padding='same')))(x)\n",
        "\n",
        "x=((MaxPooling2D( (1,2), padding='same')))(x)  \n",
        "x=((Conv2D(32, (2,3) ,activation='elu', padding='same')))(x)\n",
        "\n",
        "x=((Conv2D(32, (1,1) ,activation='elu', padding='same')))(x)\n",
        "x=((Conv2D(32, (2,3) , padding='same')))(x)\n",
        "# x=(BatchNormalization())(x)                \n",
        "# x=(Activation('relu'))(x)\n",
        "# \n",
        "# x=((MaxPooling2D( (1,2), padding='same')))(x)  \n",
        "# x=((Conv2D(256, (2,2) , padding='same')))(x)\n",
        "\n",
        "\n",
        "\n",
        "H=(Activation('elu'))(x) \n",
        "\n",
        "# x=((MaxPooling2D( (1,2), padding='same')))(x)  \n",
        "# x=((Conv2D(128, (2,3) , padding='same')))(x)\n",
        "# x=(BatchNormalization())(x)                \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# x=(Activation('relu'))(x) \n",
        "\n",
        "\n",
        "x=(Flatten())(H)  \n",
        "\n",
        "\n",
        "\n",
        "  \n",
        "   \n",
        "\n",
        "# x=(Dense(64))(x)\n",
        "        # model.add(GaussianNoise(0.05))\n",
        "        # x=(BatchNormalization())(x)\n",
        "# x=(Activation('relu'))(x)\n",
        "# x=(Dropout(0.3))(x)\n",
        "\n",
        "x=(Dense(128))(x)\n",
        "# x=(BatchNormalization())(x)\n",
        "x=(Activation('elu'))(x)\n",
        "# x=(Dropout(0.25))(x)\n",
        "\n",
        "attention=(Flatten())(H)        \n",
        "attention=(Dense(128))(attention)\n",
        "# attention=BatchNormalization()(attention)\n",
        "attention=(Activation('tanh'))(attention)\n",
        "attention=Softmax()(attention)\n",
        "# attention=(Dropout(0.25))(attention)\n",
        "# attention=(Dropout(0.3))(attention)\n",
        "# attention=(Dense(1))(attention)\n",
        "\n",
        "\n",
        "\n",
        "        \n",
        "output=tensorflow.keras.layers.Concatenate()([attention,x])\n",
        "\n",
        "output=(Dense(3))(output)\n",
        "output=(Activation('linear'))(output)\n",
        "\n",
        "        \n",
        "\n",
        "model = Model(Visible,outputs=[output])\n",
        "opt3=tensorflow.keras.optimizers.Adam(learning_rate=8*1e-4,decay=1e-6,amsgrad=True)\n",
        "\n",
        "model.compile(loss='mae', optimizer=opt3, metrics=['mae'])\n",
        "model.summary()\n",
        "history=model.fit(X_train,y_train,batch_size=64,steps_per_epoch=np.floor(X_train.shape[0]/64).astype('int'),\n",
        "                epochs=40,verbose=2,validation_data=(X_test,y_test))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2JvMCHLg887E"
      },
      "source": [
        "%tensorboard --logdir logs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kFZQuPO0uLD_"
      },
      "source": [
        "tensorflow.keras.models.save_model(\n",
        "    model, \"model.h5\", overwrite=True, include_optimizer=True, save_format=None,\n",
        "    signatures=None, options=None\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cMN_eMdoZ4MI"
      },
      "source": [
        "df_new=df_tot[(df_tot['UPDRS_22']==3)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rvUYA_B6ayKW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "448c689e-6a82-4755-f156-dadd10123021"
      },
      "source": [
        "df_new['Subject_id'].value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SUK03    419\n",
              "SGR07    101\n",
              "Name: Subject_id, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3CRyQ3O3a9HH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8821d9b5-83d4-48b3-98cf-7d22938ac6fa"
      },
      "source": [
        "df_tot=df_tot[(df_tot['Subject_id']!='SGR07')]\n",
        "df_tot"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>y</th>\n",
              "      <th>X_flight</th>\n",
              "      <th>ids</th>\n",
              "      <th>X_hold</th>\n",
              "      <th>Subject_id</th>\n",
              "      <th>Protocol ID</th>\n",
              "      <th>Subject Status</th>\n",
              "      <th>Sex</th>\n",
              "      <th>UPDRS_22_UEXTR_RIGHT_E1_1_C1</th>\n",
              "      <th>UPDRS_22_UEXTR_LEFT_E1_1_C1</th>\n",
              "      <th>UPDRS_23_RIGHT_E1_1_C1</th>\n",
              "      <th>UPDRS_23_LEFT_E1_1_C1</th>\n",
              "      <th>UPDRS_31_E1_1_C1</th>\n",
              "      <th>SUM_PART_3_E1_1_C1</th>\n",
              "      <th>UPDRS_22</th>\n",
              "      <th>UPDRS_23</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>739</td>\n",
              "      <td>2.0</td>\n",
              "      <td>[0.911, 0.494, 0.635, 0.343, 0.326, 0.16, 0.43...</td>\n",
              "      <td>11a9e65564567d48</td>\n",
              "      <td>[0.04999999998835847, 0.07600000000093132, 0.0...</td>\n",
              "      <td>Leontios</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>m</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>740</td>\n",
              "      <td>2.0</td>\n",
              "      <td>[0.827, 0.604, 0.519, 0.142, 0.442, 0.557, 0.4...</td>\n",
              "      <td>11a9e65564567d48</td>\n",
              "      <td>[0.06700000003911555, 0.08300000004237518, 0.0...</td>\n",
              "      <td>Leontios</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>m</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>741</td>\n",
              "      <td>2.0</td>\n",
              "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
              "      <td>11a9e65564567d48</td>\n",
              "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
              "      <td>Leontios</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>m</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>742</td>\n",
              "      <td>2.0</td>\n",
              "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
              "      <td>11a9e65564567d48</td>\n",
              "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
              "      <td>Leontios</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>m</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>743</td>\n",
              "      <td>2.0</td>\n",
              "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
              "      <td>11a9e65564567d48</td>\n",
              "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
              "      <td>Leontios</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>m</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4162</th>\n",
              "      <td>23180</td>\n",
              "      <td>1.0</td>\n",
              "      <td>[0.185, 0.242, 1.973, 0.477, 1.066, 0.134, 0.2...</td>\n",
              "      <td>f96144638dcc1018</td>\n",
              "      <td>[0.10199999995529652, 0.07500000018626451, 0.1...</td>\n",
              "      <td>SUK13</td>\n",
              "      <td>iprognosis-sdata-study - IRAS 237157</td>\n",
              "      <td>available</td>\n",
              "      <td>m</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4163</th>\n",
              "      <td>23181</td>\n",
              "      <td>1.0</td>\n",
              "      <td>[0.513, 0.376, 0.541, 2.85, 0.234, 0.284, 0.25...</td>\n",
              "      <td>f96144638dcc1018</td>\n",
              "      <td>[0.08100000000558794, 0.08500000019557774, 0.0...</td>\n",
              "      <td>SUK13</td>\n",
              "      <td>iprognosis-sdata-study - IRAS 237157</td>\n",
              "      <td>available</td>\n",
              "      <td>m</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4164</th>\n",
              "      <td>23182</td>\n",
              "      <td>1.0</td>\n",
              "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
              "      <td>f96144638dcc1018</td>\n",
              "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
              "      <td>SUK13</td>\n",
              "      <td>iprognosis-sdata-study - IRAS 237157</td>\n",
              "      <td>available</td>\n",
              "      <td>m</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4165</th>\n",
              "      <td>23507</td>\n",
              "      <td>0.0</td>\n",
              "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
              "      <td>fa1ca92b6d69c880</td>\n",
              "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
              "      <td>SDE14</td>\n",
              "      <td>iprognosis-sdata-study - EK 451112017</td>\n",
              "      <td>available</td>\n",
              "      <td>m</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4166</th>\n",
              "      <td>23508</td>\n",
              "      <td>0.0</td>\n",
              "      <td>[1.591, 0.83, 0.633, 2.93, 1.414, 2.256, 0.746...</td>\n",
              "      <td>fa1ca92b6d69c880</td>\n",
              "      <td>[0.14400000000023283, 0.16999999999825377, 0.0...</td>\n",
              "      <td>SDE14</td>\n",
              "      <td>iprognosis-sdata-study - EK 451112017</td>\n",
              "      <td>available</td>\n",
              "      <td>m</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>4066 rows × 17 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      index    y  ... UPDRS_22 UPDRS_23\n",
              "0       739  2.0  ...      0.0      0.0\n",
              "1       740  2.0  ...      0.0      0.0\n",
              "2       741  2.0  ...      0.0      0.0\n",
              "3       742  2.0  ...      0.0      0.0\n",
              "4       743  2.0  ...      0.0      0.0\n",
              "...     ...  ...  ...      ...      ...\n",
              "4162  23180  1.0  ...      1.0      0.0\n",
              "4163  23181  1.0  ...      1.0      0.0\n",
              "4164  23182  1.0  ...      1.0      0.0\n",
              "4165  23507  0.0  ...      0.0      0.0\n",
              "4166  23508  0.0  ...      0.0      0.0\n",
              "\n",
              "[4066 rows x 17 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LoChb_OhNx-W"
      },
      "source": [
        "df_tot=df2\n",
        "df_new=df2[df2['UPDRS_22']>6]\n",
        "df_tot=df2[df2['UPDRS_22']<6]\n",
        "df_tot=df_tot.reset_index()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "__0V11s1tn8v"
      },
      "source": [
        "def scheduler(epoch, lr):\n",
        "   if epoch < 30:\n",
        "     return lr\n",
        "   elif epoch <50 and epoch >30 :\n",
        "     return 0.0005\n",
        "   else :\n",
        "     return 0.0005\n",
        "callback = tensorflow.keras.callbacks.LearningRateScheduler(scheduler)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "7Jq9ojEcF02-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1a7a59f4-e38d-42ac-909a-eaf3ea228961"
      },
      "source": [
        "import json\n",
        "from tensorflow.keras.layers import multiply,Permute,Dot\n",
        "from tensorflow.keras.layers import GaussianNoise   \n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from tensorflow.keras.models import model_from_json,load_model\n",
        "import numpy as np\n",
        "import sklearn\n",
        "from sklearn.model_selection import GroupKFold\n",
        "group_kfold = GroupKFold(n_splits=27)\n",
        "groups=df_tot['Subject_id']\n",
        "GroupKFold(n_splits=27)\n",
        "scores=[]\n",
        "scores1=[]\n",
        "scores2=[]\n",
        "scores3=[]\n",
        "scores4=[]\n",
        "\n",
        "for train_index, test_index in group_kfold.split(df_tot,df_tot, groups):\n",
        "\n",
        "        temp1 = df_tot['X_flight'].apply(pd.Series)\n",
        "        temp2 = df_tot['X_hold'].apply(pd.Series)\n",
        "\n",
        "        npa=np.dstack((temp1,temp2))\n",
        "\n",
        "        npa=np.nan_to_num(npa)\n",
        "        npa=npa.reshape(-1,100,2,1)\n",
        "        X=npa\n",
        "        X = np.asarray(X).astype(np.float32)\n",
        "        temp4= np.asarray(df_tot['UPDRS_22'])\n",
        "        temp5= np.asarray(df_tot['UPDRS_23'])\n",
        "        temp6= np.asarray(df_tot['UPDRS_31_E1_1_C1'])\n",
        "        temp7=np.dstack((temp4,temp5,temp6))\n",
        "        temp7=np.nan_to_num(temp7)\n",
        "        temp7=temp7.swapaxes(0,1).swapaxes(1,2)\n",
        "        print(temp7.shape)\n",
        "        temp7=temp7.reshape(-1,3)\n",
        "\n",
        "\n",
        "        y=temp7\n",
        "        y = np.asarray(y).astype(np.float32)\n",
        "        print(X.shape)\n",
        "\n",
        "        X_train, X_test = X[train_index], X[test_index]\n",
        "        y_train, y_test = y[train_index], y[test_index]\n",
        "\n",
        "\n",
        "\n",
        "        \n",
        "\n",
        "\n",
        "\n",
        "       \n",
        "        \n",
        "        print(X.shape)\n",
        "\n",
        "        \n",
        "        tensorflow.random.set_seed(\n",
        "            10)\n",
        "        Visible=Input((100,2,1))\n",
        "                                  \n",
        "        x=((Conv2D(16,(3,2),padding='same')))(Visible)\n",
        "\n",
        "        x=(BatchNormalization())(x)                \n",
        "                  \n",
        "        x=(Activation('relu'))(x)\n",
        "                  \n",
        "        x=((MaxPooling2D( (2,1) ,padding='same')))(x)  \n",
        "        x=((Conv2D(12, (3,2), padding='same')))(x)\n",
        "        x=(BatchNormalization())(x)\n",
        "\n",
        "        encoded=(Activation('relu'))(x)\n",
        "                  \n",
        "\n",
        "\n",
        "\n",
        "        x=((UpSampling2D((2,1))))(x)\n",
        "        x=((Conv2D(16, (3,2), padding='same')))(x)\n",
        "        x=(BatchNormalization())(x)\n",
        "\n",
        "        x=Activation('relu')(x)\n",
        "\n",
        "\n",
        "        decoded=((Conv2D(1, (3,2), activation='linear', padding='same')))(x)\n",
        "                  \n",
        "\n",
        "          #                 # Encoder Layers\n",
        "          # #                 # autoencoder.add(LSTM(25,return_sequences=False)\n",
        "        autoencoder = Model(Visible, decoded) \n",
        "        opt = tensorflow.keras.optimizers.Adam(learning_rate=1e-2 )\n",
        "        autoencoder.compile(loss='mse',optimizer=opt,metrics=['mae'])\n",
        "        autoencoder.summary()\n",
        "        autoencoder.fit(X_train,X_train,epochs=30,batch_size=32,shuffle=True,verbose=2)\n",
        "\n",
        "        H=Flatten()(encoded)\n",
        "        output=(Dense(16))(H)\n",
        "        output=BatchNormalization()(output)\n",
        "        output=(Activation('relu'))(output)\n",
        "        output=Dropout(0.5)(output)\n",
        "                  # output=(Dense(16))(output)\n",
        "                  # output=BatchNormalization()(output)\n",
        "                  # output=(Activation('relu'))(output)\n",
        "                  # output=Dropout(0.5)(output)\n",
        "          # attention=Dense(16)(H)\n",
        "          # attention=BatchNormalization()(attention)\n",
        "          # attention=Activation('tanh')(attention)\n",
        "          #         # attention=Dropout(0.5)(attention)\n",
        "          # attention=Dense(3)(attention)\n",
        "          # attention=tensorflow.transpose(attention,perm=[0, 2, 1])\n",
        "          # attention=Softmax()(attention)\n",
        "\n",
        "          # output=tensorflow.matmul(attention,output)\n",
        "          # output=Flatten()(output)\n",
        "\n",
        "        output2=(Dense(3))(output)\n",
        "        output2=(Activation('linear'))(output2)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "                  \n",
        "        model = Model(Visible,outputs=output2)\n",
        "\n",
        "                  # steps_per_epoch=np.floor(X_train.shape[0]/256).astype('int')\n",
        "                  # model=load_model('model.h5')\n",
        "        opt3=tensorflow.keras.optimizers.Adam(learning_rate=0.001)\n",
        "\n",
        "        model.compile(loss=['mse'], optimizer=opt3, metrics=['mae'])\n",
        "        model.summary()\n",
        "        history=model.fit(X_train,y_train,batch_size=32,steps_per_epoch=np.floor(X_train.shape[0]/32).astype('int'),\n",
        "                          epochs=60,verbose=2)\n",
        "          # from tensorflow.keras.models import model_from_json,load_model\n",
        "          # model= load_model('/content/gdrive/My Drive/models/best_model_11_10.h5')\n",
        "          # tensorflow.keras.utils.plot_model(model,show_shapes=True)\n",
        "          # tensorflow.keras.models.save_model(model,\"model_%d.h5\"%(i))\n",
        "\n",
        "        # sum1=0\n",
        "        # sum2=0\n",
        "        # sum3=0\n",
        "        \n",
        "        # for i in range (1,11):\n",
        "        #   saved_model=load_model('model_%d.h5'%(i))\n",
        "        #   y_hat = saved_model.predict(X_test)\n",
        "        #   y_hat1=np.median(y_hat[:,0])\n",
        "        #   y_hat2=np.median(y_hat[:,1])\n",
        "        #   y_hat3=np.median(y_hat[:,2])\n",
        "        #   sum1=sum1+y_hat1\n",
        "        #   sum2=sum2+y_hat2\n",
        "        #   sum3=sum3+y_hat3\n",
        "\n",
        "\n",
        "        # y_hat1=sum1/10.0\n",
        "        # y_hat2=sum2/10.0\n",
        "        # y_hat3=sum3/10.0\n",
        "        \n",
        "        y_hat = model.predict(X_test)\n",
        "\n",
        "\n",
        "        \n",
        "        y_test1=np.median(y_test[:,0])\n",
        "        y_test2=np.median(y_test[:,1])\n",
        "        y_test3=np.median(y_test[:,2])\n",
        "        y_test4=[]\n",
        "        y_test4.append(y_test1)\n",
        "        y_test4.append(y_test2)\n",
        "        y_test4.append(y_test3)\n",
        "        scores1.append(y_test4)\n",
        "        print(y_test4)\n",
        "        print(y_test4)\n",
        "        y_hat1=y_hat[:,0].mean()\n",
        "        y_hat2=y_hat[:,1].mean()\n",
        "        y_hat3=y_hat[:,2].mean()\n",
        "        y_hatm=[]\n",
        "        y_hatm.append(y_hat1)\n",
        "        y_hatm.append(y_hat2)\n",
        "        y_hatm.append(y_hat3)\n",
        "        print(y_hatm)\n",
        "        scores2.append(y_hatm)\n",
        "\n",
        "        y_hat1=np.median(y_hat[:,0])\n",
        "        y_hat2=np.median(y_hat[:,1])\n",
        "        y_hat3=np.median(y_hat[:,2])\n",
        "        y_hatmm=[]\n",
        "        y_hatmm.append(y_hat1)\n",
        "        y_hatmm.append(y_hat2)\n",
        "        y_hatmm.append(y_hat3)\n",
        "        scores3.append(y_hatmm)\n",
        "        print(y_hatmm)\n",
        "        \n",
        "        y_hat1=np.percentile(y_hat[:,0],75)\n",
        "        y_hat2=np.percentile(y_hat[:,1],75)\n",
        "        y_hat3=np.percentile(y_hat[:,2],75)\n",
        "        y_hatq75=[]\n",
        "        y_hatq75.append(y_hat1)\n",
        "        y_hatq75.append(y_hat2)\n",
        "        y_hatq75.append(y_hat3)\n",
        "        scores4.append(y_hatq75)\n",
        "        print(y_hatq75)\n",
        "        "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "126/126 - 0s - loss: 0.5657 - mae: 0.5585\n",
            "Epoch 23/60\n",
            "126/126 - 0s - loss: 0.5548 - mae: 0.5520\n",
            "Epoch 24/60\n",
            "126/126 - 0s - loss: 0.5581 - mae: 0.5510\n",
            "Epoch 25/60\n",
            "126/126 - 0s - loss: 0.5440 - mae: 0.5471\n",
            "Epoch 26/60\n",
            "126/126 - 0s - loss: 0.5310 - mae: 0.5437\n",
            "Epoch 27/60\n",
            "126/126 - 0s - loss: 0.5375 - mae: 0.5423\n",
            "Epoch 28/60\n",
            "126/126 - 0s - loss: 0.5374 - mae: 0.5429\n",
            "Epoch 29/60\n",
            "126/126 - 0s - loss: 0.5105 - mae: 0.5307\n",
            "Epoch 30/60\n",
            "126/126 - 0s - loss: 0.5109 - mae: 0.5301\n",
            "Epoch 31/60\n",
            "126/126 - 0s - loss: 0.5122 - mae: 0.5293\n",
            "Epoch 32/60\n",
            "126/126 - 0s - loss: 0.5205 - mae: 0.5348\n",
            "Epoch 33/60\n",
            "126/126 - 0s - loss: 0.4958 - mae: 0.5240\n",
            "Epoch 34/60\n",
            "126/126 - 0s - loss: 0.5099 - mae: 0.5278\n",
            "Epoch 35/60\n",
            "126/126 - 0s - loss: 0.5067 - mae: 0.5228\n",
            "Epoch 36/60\n",
            "126/126 - 0s - loss: 0.5008 - mae: 0.5192\n",
            "Epoch 37/60\n",
            "126/126 - 0s - loss: 0.4949 - mae: 0.5210\n",
            "Epoch 38/60\n",
            "126/126 - 0s - loss: 0.4998 - mae: 0.5241\n",
            "Epoch 39/60\n",
            "126/126 - 0s - loss: 0.4789 - mae: 0.5116\n",
            "Epoch 40/60\n",
            "126/126 - 0s - loss: 0.4920 - mae: 0.5175\n",
            "Epoch 41/60\n",
            "126/126 - 0s - loss: 0.4827 - mae: 0.5122\n",
            "Epoch 42/60\n",
            "126/126 - 0s - loss: 0.4815 - mae: 0.5149\n",
            "Epoch 43/60\n",
            "126/126 - 0s - loss: 0.4820 - mae: 0.5099\n",
            "Epoch 44/60\n",
            "126/126 - 0s - loss: 0.5007 - mae: 0.5184\n",
            "Epoch 45/60\n",
            "126/126 - 0s - loss: 0.4853 - mae: 0.5149\n",
            "Epoch 46/60\n",
            "126/126 - 0s - loss: 0.4686 - mae: 0.5010\n",
            "Epoch 47/60\n",
            "126/126 - 0s - loss: 0.4635 - mae: 0.5039\n",
            "Epoch 48/60\n",
            "126/126 - 0s - loss: 0.4772 - mae: 0.5092\n",
            "Epoch 49/60\n",
            "126/126 - 0s - loss: 0.4599 - mae: 0.4980\n",
            "Epoch 50/60\n",
            "126/126 - 0s - loss: 0.4707 - mae: 0.5070\n",
            "Epoch 51/60\n",
            "126/126 - 0s - loss: 0.4701 - mae: 0.5104\n",
            "Epoch 52/60\n",
            "126/126 - 0s - loss: 0.4524 - mae: 0.4989\n",
            "Epoch 53/60\n",
            "126/126 - 0s - loss: 0.4415 - mae: 0.4901\n",
            "Epoch 54/60\n",
            "126/126 - 0s - loss: 0.4618 - mae: 0.5020\n",
            "Epoch 55/60\n",
            "126/126 - 0s - loss: 0.4556 - mae: 0.4950\n",
            "Epoch 56/60\n",
            "126/126 - 0s - loss: 0.4638 - mae: 0.5009\n",
            "Epoch 57/60\n",
            "126/126 - 0s - loss: 0.4471 - mae: 0.4911\n",
            "Epoch 58/60\n",
            "126/126 - 0s - loss: 0.4388 - mae: 0.4894\n",
            "Epoch 59/60\n",
            "126/126 - 0s - loss: 0.4542 - mae: 0.4966\n",
            "Epoch 60/60\n",
            "126/126 - 0s - loss: 0.4372 - mae: 0.4882\n",
            "[0.0, 0.0, 0.0]\n",
            "[0.0, 0.0, 0.0]\n",
            "[0.9772955, 0.7175878, 0.41956776]\n",
            "[0.76051635, 0.6282832, 0.44793904]\n",
            "[1.3605087399482727, 1.0543249249458313, 0.5997087359428406]\n",
            "(4167, 3, 1)\n",
            "(4167, 100, 2, 1)\n",
            "(4167, 100, 2, 1)\n",
            "Model: \"functional_1437\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_360 (InputLayer)       [(None, 100, 2, 1)]       0         \n",
            "_________________________________________________________________\n",
            "conv2d_1436 (Conv2D)         (None, 100, 2, 16)        112       \n",
            "_________________________________________________________________\n",
            "batch_normalization_1436 (Ba (None, 100, 2, 16)        64        \n",
            "_________________________________________________________________\n",
            "activation_1795 (Activation) (None, 100, 2, 16)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_359 (MaxPoolin (None, 50, 2, 16)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_1437 (Conv2D)         (None, 50, 2, 12)         1164      \n",
            "_________________________________________________________________\n",
            "batch_normalization_1437 (Ba (None, 50, 2, 12)         48        \n",
            "_________________________________________________________________\n",
            "up_sampling2d_359 (UpSamplin (None, 100, 2, 12)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1438 (Conv2D)         (None, 100, 2, 16)        1168      \n",
            "_________________________________________________________________\n",
            "batch_normalization_1438 (Ba (None, 100, 2, 16)        64        \n",
            "_________________________________________________________________\n",
            "activation_1797 (Activation) (None, 100, 2, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1439 (Conv2D)         (None, 100, 2, 1)         97        \n",
            "=================================================================\n",
            "Total params: 2,717\n",
            "Trainable params: 2,629\n",
            "Non-trainable params: 88\n",
            "_________________________________________________________________\n",
            "Epoch 1/30\n",
            "128/128 - 0s - loss: 0.0262 - mae: 0.0816\n",
            "Epoch 2/30\n",
            "128/128 - 0s - loss: 0.0064 - mae: 0.0449\n",
            "Epoch 3/30\n",
            "128/128 - 0s - loss: 0.0049 - mae: 0.0415\n",
            "Epoch 4/30\n",
            "128/128 - 0s - loss: 0.0048 - mae: 0.0415\n",
            "Epoch 5/30\n",
            "128/128 - 0s - loss: 0.0037 - mae: 0.0362\n",
            "Epoch 6/30\n",
            "128/128 - 0s - loss: 0.0046 - mae: 0.0407\n",
            "Epoch 7/30\n",
            "128/128 - 0s - loss: 0.0036 - mae: 0.0353\n",
            "Epoch 8/30\n",
            "128/128 - 0s - loss: 0.0029 - mae: 0.0303\n",
            "Epoch 9/30\n",
            "128/128 - 0s - loss: 0.0027 - mae: 0.0320\n",
            "Epoch 10/30\n",
            "128/128 - 0s - loss: 0.0023 - mae: 0.0284\n",
            "Epoch 11/30\n",
            "128/128 - 0s - loss: 0.0030 - mae: 0.0307\n",
            "Epoch 12/30\n",
            "128/128 - 0s - loss: 0.0027 - mae: 0.0305\n",
            "Epoch 13/30\n",
            "128/128 - 0s - loss: 0.0025 - mae: 0.0303\n",
            "Epoch 14/30\n",
            "128/128 - 0s - loss: 0.0024 - mae: 0.0289\n",
            "Epoch 15/30\n",
            "128/128 - 0s - loss: 0.0024 - mae: 0.0284\n",
            "Epoch 16/30\n",
            "128/128 - 0s - loss: 0.0020 - mae: 0.0265\n",
            "Epoch 17/30\n",
            "128/128 - 0s - loss: 0.0020 - mae: 0.0262\n",
            "Epoch 18/30\n",
            "128/128 - 0s - loss: 0.0024 - mae: 0.0295\n",
            "Epoch 19/30\n",
            "128/128 - 0s - loss: 0.0020 - mae: 0.0274\n",
            "Epoch 20/30\n",
            "128/128 - 0s - loss: 0.0018 - mae: 0.0246\n",
            "Epoch 21/30\n",
            "128/128 - 0s - loss: 0.0021 - mae: 0.0260\n",
            "Epoch 22/30\n",
            "128/128 - 0s - loss: 0.0019 - mae: 0.0261\n",
            "Epoch 23/30\n",
            "128/128 - 0s - loss: 0.0026 - mae: 0.0302\n",
            "Epoch 24/30\n",
            "128/128 - 0s - loss: 0.0018 - mae: 0.0252\n",
            "Epoch 25/30\n",
            "128/128 - 0s - loss: 0.0019 - mae: 0.0257\n",
            "Epoch 26/30\n",
            "128/128 - 0s - loss: 0.0017 - mae: 0.0247\n",
            "Epoch 27/30\n",
            "128/128 - 0s - loss: 0.0024 - mae: 0.0285\n",
            "Epoch 28/30\n",
            "128/128 - 0s - loss: 0.0026 - mae: 0.0299\n",
            "Epoch 29/30\n",
            "128/128 - 0s - loss: 0.0018 - mae: 0.0240\n",
            "Epoch 30/30\n",
            "128/128 - 0s - loss: 0.0021 - mae: 0.0260\n",
            "Model: \"functional_1439\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_360 (InputLayer)       [(None, 100, 2, 1)]       0         \n",
            "_________________________________________________________________\n",
            "conv2d_1436 (Conv2D)         (None, 100, 2, 16)        112       \n",
            "_________________________________________________________________\n",
            "batch_normalization_1436 (Ba (None, 100, 2, 16)        64        \n",
            "_________________________________________________________________\n",
            "activation_1795 (Activation) (None, 100, 2, 16)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_359 (MaxPoolin (None, 50, 2, 16)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_1437 (Conv2D)         (None, 50, 2, 12)         1164      \n",
            "_________________________________________________________________\n",
            "batch_normalization_1437 (Ba (None, 50, 2, 12)         48        \n",
            "_________________________________________________________________\n",
            "activation_1796 (Activation) (None, 50, 2, 12)         0         \n",
            "_________________________________________________________________\n",
            "flatten_359 (Flatten)        (None, 1200)              0         \n",
            "_________________________________________________________________\n",
            "dense_718 (Dense)            (None, 16)                19216     \n",
            "_________________________________________________________________\n",
            "batch_normalization_1439 (Ba (None, 16)                64        \n",
            "_________________________________________________________________\n",
            "activation_1798 (Activation) (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dropout_359 (Dropout)        (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_719 (Dense)            (None, 3)                 51        \n",
            "_________________________________________________________________\n",
            "activation_1799 (Activation) (None, 3)                 0         \n",
            "=================================================================\n",
            "Total params: 20,719\n",
            "Trainable params: 20,631\n",
            "Non-trainable params: 88\n",
            "_________________________________________________________________\n",
            "Epoch 1/60\n",
            "127/127 - 0s - loss: 1.9369 - mae: 1.0248\n",
            "Epoch 2/60\n",
            "127/127 - 0s - loss: 1.0388 - mae: 0.7420\n",
            "Epoch 3/60\n",
            "127/127 - 0s - loss: 0.8245 - mae: 0.6580\n",
            "Epoch 4/60\n",
            "127/127 - 0s - loss: 0.7516 - mae: 0.6251\n",
            "Epoch 5/60\n",
            "127/127 - 0s - loss: 0.6920 - mae: 0.6011\n",
            "Epoch 6/60\n",
            "127/127 - 0s - loss: 0.6572 - mae: 0.5910\n",
            "Epoch 7/60\n",
            "127/127 - 0s - loss: 0.6476 - mae: 0.5852\n",
            "Epoch 8/60\n",
            "127/127 - 0s - loss: 0.6155 - mae: 0.5747\n",
            "Epoch 9/60\n",
            "127/127 - 0s - loss: 0.5926 - mae: 0.5704\n",
            "Epoch 10/60\n",
            "127/127 - 0s - loss: 0.5640 - mae: 0.5621\n",
            "Epoch 11/60\n",
            "127/127 - 0s - loss: 0.5680 - mae: 0.5642\n",
            "Epoch 12/60\n",
            "127/127 - 0s - loss: 0.5578 - mae: 0.5582\n",
            "Epoch 13/60\n",
            "127/127 - 0s - loss: 0.5504 - mae: 0.5505\n",
            "Epoch 14/60\n",
            "127/127 - 0s - loss: 0.5561 - mae: 0.5575\n",
            "Epoch 15/60\n",
            "127/127 - 0s - loss: 0.5281 - mae: 0.5452\n",
            "Epoch 16/60\n",
            "127/127 - 0s - loss: 0.5351 - mae: 0.5484\n",
            "Epoch 17/60\n",
            "127/127 - 0s - loss: 0.5321 - mae: 0.5465\n",
            "Epoch 18/60\n",
            "127/127 - 0s - loss: 0.5277 - mae: 0.5454\n",
            "Epoch 19/60\n",
            "127/127 - 0s - loss: 0.5238 - mae: 0.5402\n",
            "Epoch 20/60\n",
            "127/127 - 0s - loss: 0.5319 - mae: 0.5421\n",
            "Epoch 21/60\n",
            "127/127 - 0s - loss: 0.5216 - mae: 0.5379\n",
            "Epoch 22/60\n",
            "127/127 - 0s - loss: 0.5036 - mae: 0.5330\n",
            "Epoch 23/60\n",
            "127/127 - 0s - loss: 0.5117 - mae: 0.5359\n",
            "Epoch 24/60\n",
            "127/127 - 0s - loss: 0.5123 - mae: 0.5335\n",
            "Epoch 25/60\n",
            "127/127 - 0s - loss: 0.5120 - mae: 0.5336\n",
            "Epoch 26/60\n",
            "127/127 - 0s - loss: 0.5105 - mae: 0.5332\n",
            "Epoch 27/60\n",
            "127/127 - 0s - loss: 0.4846 - mae: 0.5194\n",
            "Epoch 28/60\n",
            "127/127 - 0s - loss: 0.4993 - mae: 0.5257\n",
            "Epoch 29/60\n",
            "127/127 - 0s - loss: 0.4800 - mae: 0.5127\n",
            "Epoch 30/60\n",
            "127/127 - 0s - loss: 0.4593 - mae: 0.5067\n",
            "Epoch 31/60\n",
            "127/127 - 0s - loss: 0.4875 - mae: 0.5173\n",
            "Epoch 32/60\n",
            "127/127 - 0s - loss: 0.4808 - mae: 0.5140\n",
            "Epoch 33/60\n",
            "127/127 - 0s - loss: 0.4819 - mae: 0.5150\n",
            "Epoch 34/60\n",
            "127/127 - 0s - loss: 0.4714 - mae: 0.5093\n",
            "Epoch 35/60\n",
            "127/127 - 0s - loss: 0.4724 - mae: 0.5087\n",
            "Epoch 36/60\n",
            "127/127 - 0s - loss: 0.4700 - mae: 0.5087\n",
            "Epoch 37/60\n",
            "127/127 - 0s - loss: 0.4671 - mae: 0.5085\n",
            "Epoch 38/60\n",
            "127/127 - 0s - loss: 0.4457 - mae: 0.4975\n",
            "Epoch 39/60\n",
            "127/127 - 0s - loss: 0.4579 - mae: 0.5039\n",
            "Epoch 40/60\n",
            "127/127 - 0s - loss: 0.4667 - mae: 0.5042\n",
            "Epoch 41/60\n",
            "127/127 - 0s - loss: 0.4453 - mae: 0.4926\n",
            "Epoch 42/60\n",
            "127/127 - 0s - loss: 0.4462 - mae: 0.4937\n",
            "Epoch 43/60\n",
            "127/127 - 0s - loss: 0.4333 - mae: 0.4893\n",
            "Epoch 44/60\n",
            "127/127 - 0s - loss: 0.4575 - mae: 0.5006\n",
            "Epoch 45/60\n",
            "127/127 - 0s - loss: 0.4301 - mae: 0.4870\n",
            "Epoch 46/60\n",
            "127/127 - 0s - loss: 0.4450 - mae: 0.4951\n",
            "Epoch 47/60\n",
            "127/127 - 0s - loss: 0.4362 - mae: 0.4878\n",
            "Epoch 48/60\n",
            "127/127 - 0s - loss: 0.4184 - mae: 0.4834\n",
            "Epoch 49/60\n",
            "127/127 - 0s - loss: 0.4233 - mae: 0.4838\n",
            "Epoch 50/60\n",
            "127/127 - 0s - loss: 0.4308 - mae: 0.4841\n",
            "Epoch 51/60\n",
            "127/127 - 0s - loss: 0.4362 - mae: 0.4955\n",
            "Epoch 52/60\n",
            "127/127 - 0s - loss: 0.4362 - mae: 0.4933\n",
            "Epoch 53/60\n",
            "127/127 - 0s - loss: 0.4198 - mae: 0.4791\n",
            "Epoch 54/60\n",
            "127/127 - 0s - loss: 0.4179 - mae: 0.4761\n",
            "Epoch 55/60\n",
            "127/127 - 0s - loss: 0.4243 - mae: 0.4817\n",
            "Epoch 56/60\n",
            "127/127 - 0s - loss: 0.4203 - mae: 0.4779\n",
            "Epoch 57/60\n",
            "127/127 - 0s - loss: 0.4087 - mae: 0.4728\n",
            "Epoch 58/60\n",
            "127/127 - 0s - loss: 0.3990 - mae: 0.4716\n",
            "Epoch 59/60\n",
            "127/127 - 0s - loss: 0.4129 - mae: 0.4740\n",
            "Epoch 60/60\n",
            "127/127 - 0s - loss: 0.4183 - mae: 0.4757\n",
            "[3.0, 3.0, 1.0]\n",
            "[3.0, 3.0, 1.0]\n",
            "[0.19655971, 0.2600777, 0.25388962]\n",
            "[0.1529502, 0.2119092, 0.23799881]\n",
            "[0.287283718585968, 0.4411472678184509, 0.3987579643726349]\n",
            "(4167, 3, 1)\n",
            "(4167, 100, 2, 1)\n",
            "(4167, 100, 2, 1)\n",
            "Model: \"functional_1441\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_361 (InputLayer)       [(None, 100, 2, 1)]       0         \n",
            "_________________________________________________________________\n",
            "conv2d_1440 (Conv2D)         (None, 100, 2, 16)        112       \n",
            "_________________________________________________________________\n",
            "batch_normalization_1440 (Ba (None, 100, 2, 16)        64        \n",
            "_________________________________________________________________\n",
            "activation_1800 (Activation) (None, 100, 2, 16)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_360 (MaxPoolin (None, 50, 2, 16)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_1441 (Conv2D)         (None, 50, 2, 12)         1164      \n",
            "_________________________________________________________________\n",
            "batch_normalization_1441 (Ba (None, 50, 2, 12)         48        \n",
            "_________________________________________________________________\n",
            "up_sampling2d_360 (UpSamplin (None, 100, 2, 12)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1442 (Conv2D)         (None, 100, 2, 16)        1168      \n",
            "_________________________________________________________________\n",
            "batch_normalization_1442 (Ba (None, 100, 2, 16)        64        \n",
            "_________________________________________________________________\n",
            "activation_1802 (Activation) (None, 100, 2, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1443 (Conv2D)         (None, 100, 2, 1)         97        \n",
            "=================================================================\n",
            "Total params: 2,717\n",
            "Trainable params: 2,629\n",
            "Non-trainable params: 88\n",
            "_________________________________________________________________\n",
            "Epoch 1/30\n",
            "128/128 - 0s - loss: 0.0259 - mae: 0.0807\n",
            "Epoch 2/30\n",
            "128/128 - 0s - loss: 0.0088 - mae: 0.0507\n",
            "Epoch 3/30\n",
            "128/128 - 0s - loss: 0.0061 - mae: 0.0469\n",
            "Epoch 4/30\n",
            "128/128 - 0s - loss: 0.0046 - mae: 0.0408\n",
            "Epoch 5/30\n",
            "128/128 - 0s - loss: 0.0032 - mae: 0.0337\n",
            "Epoch 6/30\n",
            "128/128 - 0s - loss: 0.0037 - mae: 0.0382\n",
            "Epoch 7/30\n",
            "128/128 - 0s - loss: 0.0026 - mae: 0.0308\n",
            "Epoch 8/30\n",
            "128/128 - 0s - loss: 0.0026 - mae: 0.0310\n",
            "Epoch 9/30\n",
            "128/128 - 0s - loss: 0.0027 - mae: 0.0327\n",
            "Epoch 10/30\n",
            "128/128 - 0s - loss: 0.0021 - mae: 0.0286\n",
            "Epoch 11/30\n",
            "128/128 - 0s - loss: 0.0023 - mae: 0.0293\n",
            "Epoch 12/30\n",
            "128/128 - 0s - loss: 0.0024 - mae: 0.0281\n",
            "Epoch 13/30\n",
            "128/128 - 0s - loss: 0.0022 - mae: 0.0284\n",
            "Epoch 14/30\n",
            "128/128 - 0s - loss: 0.0021 - mae: 0.0279\n",
            "Epoch 15/30\n",
            "128/128 - 0s - loss: 0.0020 - mae: 0.0262\n",
            "Epoch 16/30\n",
            "128/128 - 0s - loss: 0.0017 - mae: 0.0254\n",
            "Epoch 17/30\n",
            "128/128 - 0s - loss: 0.0020 - mae: 0.0278\n",
            "Epoch 18/30\n",
            "128/128 - 0s - loss: 0.0023 - mae: 0.0287\n",
            "Epoch 19/30\n",
            "128/128 - 0s - loss: 0.0018 - mae: 0.0256\n",
            "Epoch 20/30\n",
            "128/128 - 0s - loss: 0.0021 - mae: 0.0270\n",
            "Epoch 21/30\n",
            "128/128 - 0s - loss: 0.0021 - mae: 0.0276\n",
            "Epoch 22/30\n",
            "128/128 - 0s - loss: 0.0018 - mae: 0.0252\n",
            "Epoch 23/30\n",
            "128/128 - 0s - loss: 0.0019 - mae: 0.0258\n",
            "Epoch 24/30\n",
            "128/128 - 0s - loss: 0.0021 - mae: 0.0264\n",
            "Epoch 25/30\n",
            "128/128 - 0s - loss: 0.0019 - mae: 0.0252\n",
            "Epoch 26/30\n",
            "128/128 - 0s - loss: 0.0017 - mae: 0.0251\n",
            "Epoch 27/30\n",
            "128/128 - 0s - loss: 0.0021 - mae: 0.0275\n",
            "Epoch 28/30\n",
            "128/128 - 0s - loss: 0.0016 - mae: 0.0248\n",
            "Epoch 29/30\n",
            "128/128 - 0s - loss: 0.0024 - mae: 0.0276\n",
            "Epoch 30/30\n",
            "128/128 - 0s - loss: 0.0018 - mae: 0.0251\n",
            "Model: \"functional_1443\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_361 (InputLayer)       [(None, 100, 2, 1)]       0         \n",
            "_________________________________________________________________\n",
            "conv2d_1440 (Conv2D)         (None, 100, 2, 16)        112       \n",
            "_________________________________________________________________\n",
            "batch_normalization_1440 (Ba (None, 100, 2, 16)        64        \n",
            "_________________________________________________________________\n",
            "activation_1800 (Activation) (None, 100, 2, 16)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_360 (MaxPoolin (None, 50, 2, 16)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_1441 (Conv2D)         (None, 50, 2, 12)         1164      \n",
            "_________________________________________________________________\n",
            "batch_normalization_1441 (Ba (None, 50, 2, 12)         48        \n",
            "_________________________________________________________________\n",
            "activation_1801 (Activation) (None, 50, 2, 12)         0         \n",
            "_________________________________________________________________\n",
            "flatten_360 (Flatten)        (None, 1200)              0         \n",
            "_________________________________________________________________\n",
            "dense_720 (Dense)            (None, 16)                19216     \n",
            "_________________________________________________________________\n",
            "batch_normalization_1443 (Ba (None, 16)                64        \n",
            "_________________________________________________________________\n",
            "activation_1803 (Activation) (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dropout_360 (Dropout)        (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_721 (Dense)            (None, 3)                 51        \n",
            "_________________________________________________________________\n",
            "activation_1804 (Activation) (None, 3)                 0         \n",
            "=================================================================\n",
            "Total params: 20,719\n",
            "Trainable params: 20,631\n",
            "Non-trainable params: 88\n",
            "_________________________________________________________________\n",
            "Epoch 1/60\n",
            "127/127 - 0s - loss: 1.8842 - mae: 1.0075\n",
            "Epoch 2/60\n",
            "127/127 - 0s - loss: 1.0984 - mae: 0.7600\n",
            "Epoch 3/60\n",
            "127/127 - 0s - loss: 0.8930 - mae: 0.6863\n",
            "Epoch 4/60\n",
            "127/127 - 0s - loss: 0.7708 - mae: 0.6435\n",
            "Epoch 5/60\n",
            "127/127 - 0s - loss: 0.6883 - mae: 0.6139\n",
            "Epoch 6/60\n",
            "127/127 - 0s - loss: 0.6605 - mae: 0.6034\n",
            "Epoch 7/60\n",
            "127/127 - 0s - loss: 0.6386 - mae: 0.5973\n",
            "Epoch 8/60\n",
            "127/127 - 0s - loss: 0.6070 - mae: 0.5864\n",
            "Epoch 9/60\n",
            "127/127 - 0s - loss: 0.5957 - mae: 0.5817\n",
            "Epoch 10/60\n",
            "127/127 - 0s - loss: 0.5824 - mae: 0.5760\n",
            "Epoch 11/60\n",
            "127/127 - 0s - loss: 0.5650 - mae: 0.5664\n",
            "Epoch 12/60\n",
            "127/127 - 0s - loss: 0.5586 - mae: 0.5659\n",
            "Epoch 13/60\n",
            "127/127 - 0s - loss: 0.5434 - mae: 0.5597\n",
            "Epoch 14/60\n",
            "127/127 - 0s - loss: 0.5429 - mae: 0.5545\n",
            "Epoch 15/60\n",
            "127/127 - 0s - loss: 0.5356 - mae: 0.5508\n",
            "Epoch 16/60\n",
            "127/127 - 0s - loss: 0.5353 - mae: 0.5517\n",
            "Epoch 17/60\n",
            "127/127 - 0s - loss: 0.5228 - mae: 0.5449\n",
            "Epoch 18/60\n",
            "127/127 - 0s - loss: 0.5009 - mae: 0.5336\n",
            "Epoch 19/60\n",
            "127/127 - 0s - loss: 0.5085 - mae: 0.5373\n",
            "Epoch 20/60\n",
            "127/127 - 0s - loss: 0.4855 - mae: 0.5247\n",
            "Epoch 21/60\n",
            "127/127 - 0s - loss: 0.4950 - mae: 0.5317\n",
            "Epoch 22/60\n",
            "127/127 - 0s - loss: 0.5058 - mae: 0.5361\n",
            "Epoch 23/60\n",
            "127/127 - 0s - loss: 0.4746 - mae: 0.5222\n",
            "Epoch 24/60\n",
            "127/127 - 0s - loss: 0.4762 - mae: 0.5185\n",
            "Epoch 25/60\n",
            "127/127 - 0s - loss: 0.4868 - mae: 0.5242\n",
            "Epoch 26/60\n",
            "127/127 - 0s - loss: 0.4646 - mae: 0.5156\n",
            "Epoch 27/60\n",
            "127/127 - 0s - loss: 0.4724 - mae: 0.5179\n",
            "Epoch 28/60\n",
            "127/127 - 0s - loss: 0.4737 - mae: 0.5174\n",
            "Epoch 29/60\n",
            "127/127 - 0s - loss: 0.4596 - mae: 0.5104\n",
            "Epoch 30/60\n",
            "127/127 - 0s - loss: 0.4446 - mae: 0.4979\n",
            "Epoch 31/60\n",
            "127/127 - 0s - loss: 0.4709 - mae: 0.5144\n",
            "Epoch 32/60\n",
            "127/127 - 0s - loss: 0.4622 - mae: 0.5090\n",
            "Epoch 33/60\n",
            "127/127 - 0s - loss: 0.4561 - mae: 0.5072\n",
            "Epoch 34/60\n",
            "127/127 - 0s - loss: 0.4302 - mae: 0.4920\n",
            "Epoch 35/60\n",
            "127/127 - 0s - loss: 0.4564 - mae: 0.5018\n",
            "Epoch 36/60\n",
            "127/127 - 0s - loss: 0.4444 - mae: 0.4989\n",
            "Epoch 37/60\n",
            "127/127 - 0s - loss: 0.4372 - mae: 0.4936\n",
            "Epoch 38/60\n",
            "127/127 - 0s - loss: 0.4401 - mae: 0.4956\n",
            "Epoch 39/60\n",
            "127/127 - 0s - loss: 0.4231 - mae: 0.4875\n",
            "Epoch 40/60\n",
            "127/127 - 0s - loss: 0.4285 - mae: 0.4923\n",
            "Epoch 41/60\n",
            "127/127 - 0s - loss: 0.4151 - mae: 0.4852\n",
            "Epoch 42/60\n",
            "127/127 - 0s - loss: 0.4139 - mae: 0.4820\n",
            "Epoch 43/60\n",
            "127/127 - 0s - loss: 0.4318 - mae: 0.4905\n",
            "Epoch 44/60\n",
            "127/127 - 0s - loss: 0.4213 - mae: 0.4871\n",
            "Epoch 45/60\n",
            "127/127 - 0s - loss: 0.4261 - mae: 0.4861\n",
            "Epoch 46/60\n",
            "127/127 - 0s - loss: 0.4258 - mae: 0.4864\n",
            "Epoch 47/60\n",
            "127/127 - 0s - loss: 0.4084 - mae: 0.4763\n",
            "Epoch 48/60\n",
            "127/127 - 0s - loss: 0.4090 - mae: 0.4769\n",
            "Epoch 49/60\n",
            "127/127 - 0s - loss: 0.4092 - mae: 0.4747\n",
            "Epoch 50/60\n",
            "127/127 - 0s - loss: 0.4084 - mae: 0.4754\n",
            "Epoch 51/60\n",
            "127/127 - 0s - loss: 0.4050 - mae: 0.4746\n",
            "Epoch 52/60\n",
            "127/127 - 0s - loss: 0.4073 - mae: 0.4733\n",
            "Epoch 53/60\n",
            "127/127 - 0s - loss: 0.4260 - mae: 0.4859\n",
            "Epoch 54/60\n",
            "127/127 - 0s - loss: 0.3935 - mae: 0.4670\n",
            "Epoch 55/60\n",
            "127/127 - 0s - loss: 0.4038 - mae: 0.4704\n",
            "Epoch 56/60\n",
            "127/127 - 0s - loss: 0.4193 - mae: 0.4798\n",
            "Epoch 57/60\n",
            "127/127 - 0s - loss: 0.4057 - mae: 0.4725\n",
            "Epoch 58/60\n",
            "127/127 - 0s - loss: 0.3833 - mae: 0.4676\n",
            "Epoch 59/60\n",
            "127/127 - 0s - loss: 0.4027 - mae: 0.4744\n",
            "Epoch 60/60\n",
            "127/127 - 0s - loss: 0.4075 - mae: 0.4746\n",
            "[4.0, 4.0, 1.0]\n",
            "[4.0, 4.0, 1.0]\n",
            "[1.6698786, 2.0097363, 0.9027473]\n",
            "[1.5753453, 2.0681648, 0.9159365]\n",
            "[1.9994252920150757, 2.453758716583252, 1.0317995250225067]\n",
            "(4167, 3, 1)\n",
            "(4167, 100, 2, 1)\n",
            "(4167, 100, 2, 1)\n",
            "Model: \"functional_1445\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_362 (InputLayer)       [(None, 100, 2, 1)]       0         \n",
            "_________________________________________________________________\n",
            "conv2d_1444 (Conv2D)         (None, 100, 2, 16)        112       \n",
            "_________________________________________________________________\n",
            "batch_normalization_1444 (Ba (None, 100, 2, 16)        64        \n",
            "_________________________________________________________________\n",
            "activation_1805 (Activation) (None, 100, 2, 16)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_361 (MaxPoolin (None, 50, 2, 16)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_1445 (Conv2D)         (None, 50, 2, 12)         1164      \n",
            "_________________________________________________________________\n",
            "batch_normalization_1445 (Ba (None, 50, 2, 12)         48        \n",
            "_________________________________________________________________\n",
            "up_sampling2d_361 (UpSamplin (None, 100, 2, 12)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1446 (Conv2D)         (None, 100, 2, 16)        1168      \n",
            "_________________________________________________________________\n",
            "batch_normalization_1446 (Ba (None, 100, 2, 16)        64        \n",
            "_________________________________________________________________\n",
            "activation_1807 (Activation) (None, 100, 2, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1447 (Conv2D)         (None, 100, 2, 1)         97        \n",
            "=================================================================\n",
            "Total params: 2,717\n",
            "Trainable params: 2,629\n",
            "Non-trainable params: 88\n",
            "_________________________________________________________________\n",
            "Epoch 1/30\n",
            "128/128 - 0s - loss: 0.0255 - mae: 0.0787\n",
            "Epoch 2/30\n",
            "128/128 - 0s - loss: 0.0060 - mae: 0.0435\n",
            "Epoch 3/30\n",
            "128/128 - 0s - loss: 0.0058 - mae: 0.0453\n",
            "Epoch 4/30\n",
            "128/128 - 0s - loss: 0.0060 - mae: 0.0455\n",
            "Epoch 5/30\n",
            "128/128 - 0s - loss: 0.0040 - mae: 0.0391\n",
            "Epoch 6/30\n",
            "128/128 - 0s - loss: 0.0034 - mae: 0.0365\n",
            "Epoch 7/30\n",
            "128/128 - 0s - loss: 0.0036 - mae: 0.0357\n",
            "Epoch 8/30\n",
            "128/128 - 0s - loss: 0.0026 - mae: 0.0310\n",
            "Epoch 9/30\n",
            "128/128 - 0s - loss: 0.0027 - mae: 0.0295\n",
            "Epoch 10/30\n",
            "128/128 - 0s - loss: 0.0028 - mae: 0.0313\n",
            "Epoch 11/30\n",
            "128/128 - 0s - loss: 0.0025 - mae: 0.0293\n",
            "Epoch 12/30\n",
            "128/128 - 0s - loss: 0.0023 - mae: 0.0290\n",
            "Epoch 13/30\n",
            "128/128 - 0s - loss: 0.0025 - mae: 0.0290\n",
            "Epoch 14/30\n",
            "128/128 - 0s - loss: 0.0025 - mae: 0.0301\n",
            "Epoch 15/30\n",
            "128/128 - 0s - loss: 0.0025 - mae: 0.0294\n",
            "Epoch 16/30\n",
            "128/128 - 0s - loss: 0.0024 - mae: 0.0280\n",
            "Epoch 17/30\n",
            "128/128 - 0s - loss: 0.0024 - mae: 0.0288\n",
            "Epoch 18/30\n",
            "128/128 - 0s - loss: 0.0020 - mae: 0.0257\n",
            "Epoch 19/30\n",
            "128/128 - 0s - loss: 0.0025 - mae: 0.0294\n",
            "Epoch 20/30\n",
            "128/128 - 0s - loss: 0.0021 - mae: 0.0273\n",
            "Epoch 21/30\n",
            "128/128 - 0s - loss: 0.0017 - mae: 0.0248\n",
            "Epoch 22/30\n",
            "128/128 - 0s - loss: 0.0021 - mae: 0.0263\n",
            "Epoch 23/30\n",
            "128/128 - 0s - loss: 0.0019 - mae: 0.0257\n",
            "Epoch 24/30\n",
            "128/128 - 0s - loss: 0.0020 - mae: 0.0267\n",
            "Epoch 25/30\n",
            "128/128 - 0s - loss: 0.0018 - mae: 0.0246\n",
            "Epoch 26/30\n",
            "128/128 - 0s - loss: 0.0019 - mae: 0.0259\n",
            "Epoch 27/30\n",
            "128/128 - 0s - loss: 0.0019 - mae: 0.0254\n",
            "Epoch 28/30\n",
            "128/128 - 0s - loss: 0.0021 - mae: 0.0266\n",
            "Epoch 29/30\n",
            "128/128 - 0s - loss: 0.0019 - mae: 0.0259\n",
            "Epoch 30/30\n",
            "128/128 - 0s - loss: 0.0019 - mae: 0.0263\n",
            "Model: \"functional_1447\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_362 (InputLayer)       [(None, 100, 2, 1)]       0         \n",
            "_________________________________________________________________\n",
            "conv2d_1444 (Conv2D)         (None, 100, 2, 16)        112       \n",
            "_________________________________________________________________\n",
            "batch_normalization_1444 (Ba (None, 100, 2, 16)        64        \n",
            "_________________________________________________________________\n",
            "activation_1805 (Activation) (None, 100, 2, 16)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_361 (MaxPoolin (None, 50, 2, 16)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_1445 (Conv2D)         (None, 50, 2, 12)         1164      \n",
            "_________________________________________________________________\n",
            "batch_normalization_1445 (Ba (None, 50, 2, 12)         48        \n",
            "_________________________________________________________________\n",
            "activation_1806 (Activation) (None, 50, 2, 12)         0         \n",
            "_________________________________________________________________\n",
            "flatten_361 (Flatten)        (None, 1200)              0         \n",
            "_________________________________________________________________\n",
            "dense_722 (Dense)            (None, 16)                19216     \n",
            "_________________________________________________________________\n",
            "batch_normalization_1447 (Ba (None, 16)                64        \n",
            "_________________________________________________________________\n",
            "activation_1808 (Activation) (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dropout_361 (Dropout)        (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_723 (Dense)            (None, 3)                 51        \n",
            "_________________________________________________________________\n",
            "activation_1809 (Activation) (None, 3)                 0         \n",
            "=================================================================\n",
            "Total params: 20,719\n",
            "Trainable params: 20,631\n",
            "Non-trainable params: 88\n",
            "_________________________________________________________________\n",
            "Epoch 1/60\n",
            "128/128 - 0s - loss: 2.0919 - mae: 1.0629\n",
            "Epoch 2/60\n",
            "128/128 - 0s - loss: 1.1778 - mae: 0.7900\n",
            "Epoch 3/60\n",
            "128/128 - 0s - loss: 0.9390 - mae: 0.7015\n",
            "Epoch 4/60\n",
            "128/128 - 0s - loss: 0.7934 - mae: 0.6469\n",
            "Epoch 5/60\n",
            "128/128 - 0s - loss: 0.7565 - mae: 0.6254\n",
            "Epoch 6/60\n",
            "128/128 - 0s - loss: 0.7175 - mae: 0.6135\n",
            "Epoch 7/60\n",
            "128/128 - 0s - loss: 0.6814 - mae: 0.6021\n",
            "Epoch 8/60\n",
            "128/128 - 0s - loss: 0.6708 - mae: 0.6001\n",
            "Epoch 9/60\n",
            "128/128 - 0s - loss: 0.6329 - mae: 0.5843\n",
            "Epoch 10/60\n",
            "128/128 - 0s - loss: 0.6395 - mae: 0.5886\n",
            "Epoch 11/60\n",
            "128/128 - 0s - loss: 0.6200 - mae: 0.5818\n",
            "Epoch 12/60\n",
            "128/128 - 0s - loss: 0.6047 - mae: 0.5761\n",
            "Epoch 13/60\n",
            "128/128 - 0s - loss: 0.5902 - mae: 0.5712\n",
            "Epoch 14/60\n",
            "128/128 - 0s - loss: 0.5913 - mae: 0.5697\n",
            "Epoch 15/60\n",
            "128/128 - 0s - loss: 0.5850 - mae: 0.5670\n",
            "Epoch 16/60\n",
            "128/128 - 0s - loss: 0.5799 - mae: 0.5658\n",
            "Epoch 17/60\n",
            "128/128 - 0s - loss: 0.5694 - mae: 0.5627\n",
            "Epoch 18/60\n",
            "128/128 - 0s - loss: 0.5672 - mae: 0.5591\n",
            "Epoch 19/60\n",
            "128/128 - 0s - loss: 0.5532 - mae: 0.5534\n",
            "Epoch 20/60\n",
            "128/128 - 0s - loss: 0.5592 - mae: 0.5532\n",
            "Epoch 21/60\n",
            "128/128 - 0s - loss: 0.5522 - mae: 0.5513\n",
            "Epoch 22/60\n",
            "128/128 - 0s - loss: 0.5415 - mae: 0.5505\n",
            "Epoch 23/60\n",
            "128/128 - 0s - loss: 0.5522 - mae: 0.5559\n",
            "Epoch 24/60\n",
            "128/128 - 0s - loss: 0.5368 - mae: 0.5457\n",
            "Epoch 25/60\n",
            "128/128 - 0s - loss: 0.5273 - mae: 0.5429\n",
            "Epoch 26/60\n",
            "128/128 - 0s - loss: 0.5363 - mae: 0.5438\n",
            "Epoch 27/60\n",
            "128/128 - 0s - loss: 0.5151 - mae: 0.5353\n",
            "Epoch 28/60\n",
            "128/128 - 0s - loss: 0.5198 - mae: 0.5365\n",
            "Epoch 29/60\n",
            "128/128 - 0s - loss: 0.5126 - mae: 0.5348\n",
            "Epoch 30/60\n",
            "128/128 - 0s - loss: 0.5007 - mae: 0.5268\n",
            "Epoch 31/60\n",
            "128/128 - 0s - loss: 0.5104 - mae: 0.5301\n",
            "Epoch 32/60\n",
            "128/128 - 0s - loss: 0.5119 - mae: 0.5286\n",
            "Epoch 33/60\n",
            "128/128 - 0s - loss: 0.5106 - mae: 0.5325\n",
            "Epoch 34/60\n",
            "128/128 - 0s - loss: 0.5054 - mae: 0.5251\n",
            "Epoch 35/60\n",
            "128/128 - 0s - loss: 0.4951 - mae: 0.5258\n",
            "Epoch 36/60\n",
            "128/128 - 0s - loss: 0.4928 - mae: 0.5227\n",
            "Epoch 37/60\n",
            "128/128 - 0s - loss: 0.4835 - mae: 0.5173\n",
            "Epoch 38/60\n",
            "128/128 - 0s - loss: 0.4938 - mae: 0.5221\n",
            "Epoch 39/60\n",
            "128/128 - 0s - loss: 0.4800 - mae: 0.5166\n",
            "Epoch 40/60\n",
            "128/128 - 0s - loss: 0.4851 - mae: 0.5197\n",
            "Epoch 41/60\n",
            "128/128 - 0s - loss: 0.4703 - mae: 0.5096\n",
            "Epoch 42/60\n",
            "128/128 - 0s - loss: 0.4782 - mae: 0.5137\n",
            "Epoch 43/60\n",
            "128/128 - 0s - loss: 0.4775 - mae: 0.5144\n",
            "Epoch 44/60\n",
            "128/128 - 0s - loss: 0.4752 - mae: 0.5107\n",
            "Epoch 45/60\n",
            "128/128 - 0s - loss: 0.4749 - mae: 0.5098\n",
            "Epoch 46/60\n",
            "128/128 - 0s - loss: 0.4617 - mae: 0.5091\n",
            "Epoch 47/60\n",
            "128/128 - 0s - loss: 0.4632 - mae: 0.5059\n",
            "Epoch 48/60\n",
            "128/128 - 0s - loss: 0.4617 - mae: 0.5034\n",
            "Epoch 49/60\n",
            "128/128 - 0s - loss: 0.4574 - mae: 0.4989\n",
            "Epoch 50/60\n",
            "128/128 - 0s - loss: 0.4723 - mae: 0.5066\n",
            "Epoch 51/60\n",
            "128/128 - 0s - loss: 0.4411 - mae: 0.4920\n",
            "Epoch 52/60\n",
            "128/128 - 0s - loss: 0.4755 - mae: 0.5101\n",
            "Epoch 53/60\n",
            "128/128 - 0s - loss: 0.4664 - mae: 0.5045\n",
            "Epoch 54/60\n",
            "128/128 - 0s - loss: 0.4494 - mae: 0.4962\n",
            "Epoch 55/60\n",
            "128/128 - 0s - loss: 0.4545 - mae: 0.4986\n",
            "Epoch 56/60\n",
            "128/128 - 0s - loss: 0.4617 - mae: 0.5047\n",
            "Epoch 57/60\n",
            "128/128 - 0s - loss: 0.4475 - mae: 0.4925\n",
            "Epoch 58/60\n",
            "128/128 - 0s - loss: 0.4334 - mae: 0.4881\n",
            "Epoch 59/60\n",
            "128/128 - 0s - loss: 0.4522 - mae: 0.4961\n",
            "Epoch 60/60\n",
            "128/128 - 0s - loss: 0.4413 - mae: 0.4915\n",
            "[0.0, 0.0, 0.0]\n",
            "[0.0, 0.0, 0.0]\n",
            "[0.6717694, 0.63753074, 0.5243607]\n",
            "[0.5456947, 0.58978206, 0.5365105]\n",
            "[1.0500528812408447, 0.8422544002532959, 0.6701592206954956]\n",
            "(4167, 3, 1)\n",
            "(4167, 100, 2, 1)\n",
            "(4167, 100, 2, 1)\n",
            "Model: \"functional_1449\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_363 (InputLayer)       [(None, 100, 2, 1)]       0         \n",
            "_________________________________________________________________\n",
            "conv2d_1448 (Conv2D)         (None, 100, 2, 16)        112       \n",
            "_________________________________________________________________\n",
            "batch_normalization_1448 (Ba (None, 100, 2, 16)        64        \n",
            "_________________________________________________________________\n",
            "activation_1810 (Activation) (None, 100, 2, 16)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_362 (MaxPoolin (None, 50, 2, 16)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_1449 (Conv2D)         (None, 50, 2, 12)         1164      \n",
            "_________________________________________________________________\n",
            "batch_normalization_1449 (Ba (None, 50, 2, 12)         48        \n",
            "_________________________________________________________________\n",
            "up_sampling2d_362 (UpSamplin (None, 100, 2, 12)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1450 (Conv2D)         (None, 100, 2, 16)        1168      \n",
            "_________________________________________________________________\n",
            "batch_normalization_1450 (Ba (None, 100, 2, 16)        64        \n",
            "_________________________________________________________________\n",
            "activation_1812 (Activation) (None, 100, 2, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1451 (Conv2D)         (None, 100, 2, 1)         97        \n",
            "=================================================================\n",
            "Total params: 2,717\n",
            "Trainable params: 2,629\n",
            "Non-trainable params: 88\n",
            "_________________________________________________________________\n",
            "Epoch 1/30\n",
            "129/129 - 0s - loss: 0.0281 - mae: 0.0846\n",
            "Epoch 2/30\n",
            "129/129 - 0s - loss: 0.0060 - mae: 0.0440\n",
            "Epoch 3/30\n",
            "129/129 - 0s - loss: 0.0060 - mae: 0.0473\n",
            "Epoch 4/30\n",
            "129/129 - 0s - loss: 0.0060 - mae: 0.0436\n",
            "Epoch 5/30\n",
            "129/129 - 0s - loss: 0.0045 - mae: 0.0387\n",
            "Epoch 6/30\n",
            "129/129 - 0s - loss: 0.0037 - mae: 0.0379\n",
            "Epoch 7/30\n",
            "129/129 - 0s - loss: 0.0034 - mae: 0.0355\n",
            "Epoch 8/30\n",
            "129/129 - 0s - loss: 0.0030 - mae: 0.0338\n",
            "Epoch 9/30\n",
            "129/129 - 0s - loss: 0.0035 - mae: 0.0359\n",
            "Epoch 10/30\n",
            "129/129 - 0s - loss: 0.0024 - mae: 0.0294\n",
            "Epoch 11/30\n",
            "129/129 - 0s - loss: 0.0024 - mae: 0.0293\n",
            "Epoch 12/30\n",
            "129/129 - 0s - loss: 0.0024 - mae: 0.0288\n",
            "Epoch 13/30\n",
            "129/129 - 0s - loss: 0.0025 - mae: 0.0305\n",
            "Epoch 14/30\n",
            "129/129 - 0s - loss: 0.0024 - mae: 0.0293\n",
            "Epoch 15/30\n",
            "129/129 - 0s - loss: 0.0027 - mae: 0.0312\n",
            "Epoch 16/30\n",
            "129/129 - 0s - loss: 0.0023 - mae: 0.0287\n",
            "Epoch 17/30\n",
            "129/129 - 0s - loss: 0.0033 - mae: 0.0321\n",
            "Epoch 18/30\n",
            "129/129 - 0s - loss: 0.0023 - mae: 0.0277\n",
            "Epoch 19/30\n",
            "129/129 - 0s - loss: 0.0024 - mae: 0.0278\n",
            "Epoch 20/30\n",
            "129/129 - 0s - loss: 0.0021 - mae: 0.0269\n",
            "Epoch 21/30\n",
            "129/129 - 0s - loss: 0.0020 - mae: 0.0272\n",
            "Epoch 22/30\n",
            "129/129 - 0s - loss: 0.0020 - mae: 0.0264\n",
            "Epoch 23/30\n",
            "129/129 - 0s - loss: 0.0027 - mae: 0.0293\n",
            "Epoch 24/30\n",
            "129/129 - 0s - loss: 0.0022 - mae: 0.0274\n",
            "Epoch 25/30\n",
            "129/129 - 0s - loss: 0.0020 - mae: 0.0262\n",
            "Epoch 26/30\n",
            "129/129 - 0s - loss: 0.0019 - mae: 0.0262\n",
            "Epoch 27/30\n",
            "129/129 - 0s - loss: 0.0017 - mae: 0.0234\n",
            "Epoch 28/30\n",
            "129/129 - 0s - loss: 0.0017 - mae: 0.0245\n",
            "Epoch 29/30\n",
            "129/129 - 0s - loss: 0.0018 - mae: 0.0249\n",
            "Epoch 30/30\n",
            "129/129 - 0s - loss: 0.0016 - mae: 0.0231\n",
            "Model: \"functional_1451\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_363 (InputLayer)       [(None, 100, 2, 1)]       0         \n",
            "_________________________________________________________________\n",
            "conv2d_1448 (Conv2D)         (None, 100, 2, 16)        112       \n",
            "_________________________________________________________________\n",
            "batch_normalization_1448 (Ba (None, 100, 2, 16)        64        \n",
            "_________________________________________________________________\n",
            "activation_1810 (Activation) (None, 100, 2, 16)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_362 (MaxPoolin (None, 50, 2, 16)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_1449 (Conv2D)         (None, 50, 2, 12)         1164      \n",
            "_________________________________________________________________\n",
            "batch_normalization_1449 (Ba (None, 50, 2, 12)         48        \n",
            "_________________________________________________________________\n",
            "activation_1811 (Activation) (None, 50, 2, 12)         0         \n",
            "_________________________________________________________________\n",
            "flatten_362 (Flatten)        (None, 1200)              0         \n",
            "_________________________________________________________________\n",
            "dense_724 (Dense)            (None, 16)                19216     \n",
            "_________________________________________________________________\n",
            "batch_normalization_1451 (Ba (None, 16)                64        \n",
            "_________________________________________________________________\n",
            "activation_1813 (Activation) (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dropout_362 (Dropout)        (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_725 (Dense)            (None, 3)                 51        \n",
            "_________________________________________________________________\n",
            "activation_1814 (Activation) (None, 3)                 0         \n",
            "=================================================================\n",
            "Total params: 20,719\n",
            "Trainable params: 20,631\n",
            "Non-trainable params: 88\n",
            "_________________________________________________________________\n",
            "Epoch 1/60\n",
            "128/128 - 0s - loss: 2.0396 - mae: 1.0413\n",
            "Epoch 2/60\n",
            "128/128 - 0s - loss: 1.1718 - mae: 0.7799\n",
            "Epoch 3/60\n",
            "128/128 - 0s - loss: 0.9265 - mae: 0.6944\n",
            "Epoch 4/60\n",
            "128/128 - 0s - loss: 0.8095 - mae: 0.6467\n",
            "Epoch 5/60\n",
            "128/128 - 0s - loss: 0.7447 - mae: 0.6257\n",
            "Epoch 6/60\n",
            "128/128 - 0s - loss: 0.7146 - mae: 0.6110\n",
            "Epoch 7/60\n",
            "128/128 - 0s - loss: 0.7068 - mae: 0.6115\n",
            "Epoch 8/60\n",
            "128/128 - 0s - loss: 0.6705 - mae: 0.6065\n",
            "Epoch 9/60\n",
            "128/128 - 0s - loss: 0.6562 - mae: 0.6023\n",
            "Epoch 10/60\n",
            "128/128 - 0s - loss: 0.6483 - mae: 0.5964\n",
            "Epoch 11/60\n",
            "128/128 - 0s - loss: 0.6281 - mae: 0.5912\n",
            "Epoch 12/60\n",
            "128/128 - 0s - loss: 0.6328 - mae: 0.5921\n",
            "Epoch 13/60\n",
            "128/128 - 0s - loss: 0.6228 - mae: 0.5883\n",
            "Epoch 14/60\n",
            "128/128 - 0s - loss: 0.6120 - mae: 0.5825\n",
            "Epoch 15/60\n",
            "128/128 - 0s - loss: 0.6003 - mae: 0.5799\n",
            "Epoch 16/60\n",
            "128/128 - 0s - loss: 0.5844 - mae: 0.5701\n",
            "Epoch 17/60\n",
            "128/128 - 0s - loss: 0.5852 - mae: 0.5692\n",
            "Epoch 18/60\n",
            "128/128 - 0s - loss: 0.5748 - mae: 0.5676\n",
            "Epoch 19/60\n",
            "128/128 - 0s - loss: 0.5812 - mae: 0.5690\n",
            "Epoch 20/60\n",
            "128/128 - 0s - loss: 0.5733 - mae: 0.5633\n",
            "Epoch 21/60\n",
            "128/128 - 0s - loss: 0.5761 - mae: 0.5687\n",
            "Epoch 22/60\n",
            "128/128 - 0s - loss: 0.5514 - mae: 0.5492\n",
            "Epoch 23/60\n",
            "128/128 - 0s - loss: 0.5539 - mae: 0.5499\n",
            "Epoch 24/60\n",
            "128/128 - 0s - loss: 0.5438 - mae: 0.5540\n",
            "Epoch 25/60\n",
            "128/128 - 0s - loss: 0.5493 - mae: 0.5494\n",
            "Epoch 26/60\n",
            "128/128 - 0s - loss: 0.5413 - mae: 0.5510\n",
            "Epoch 27/60\n",
            "128/128 - 0s - loss: 0.5261 - mae: 0.5378\n",
            "Epoch 28/60\n",
            "128/128 - 0s - loss: 0.5434 - mae: 0.5433\n",
            "Epoch 29/60\n",
            "128/128 - 0s - loss: 0.5371 - mae: 0.5434\n",
            "Epoch 30/60\n",
            "128/128 - 0s - loss: 0.5211 - mae: 0.5368\n",
            "Epoch 31/60\n",
            "128/128 - 0s - loss: 0.5128 - mae: 0.5341\n",
            "Epoch 32/60\n",
            "128/128 - 0s - loss: 0.5362 - mae: 0.5376\n",
            "Epoch 33/60\n",
            "128/128 - 0s - loss: 0.5156 - mae: 0.5319\n",
            "Epoch 34/60\n",
            "128/128 - 0s - loss: 0.5192 - mae: 0.5302\n",
            "Epoch 35/60\n",
            "128/128 - 0s - loss: 0.5197 - mae: 0.5368\n",
            "Epoch 36/60\n",
            "128/128 - 0s - loss: 0.5022 - mae: 0.5229\n",
            "Epoch 37/60\n",
            "128/128 - 0s - loss: 0.5152 - mae: 0.5319\n",
            "Epoch 38/60\n",
            "128/128 - 0s - loss: 0.5097 - mae: 0.5269\n",
            "Epoch 39/60\n",
            "128/128 - 0s - loss: 0.5153 - mae: 0.5297\n",
            "Epoch 40/60\n",
            "128/128 - 0s - loss: 0.4953 - mae: 0.5191\n",
            "Epoch 41/60\n",
            "128/128 - 0s - loss: 0.5004 - mae: 0.5226\n",
            "Epoch 42/60\n",
            "128/128 - 0s - loss: 0.4908 - mae: 0.5207\n",
            "Epoch 43/60\n",
            "128/128 - 0s - loss: 0.4970 - mae: 0.5199\n",
            "Epoch 44/60\n",
            "128/128 - 0s - loss: 0.4904 - mae: 0.5170\n",
            "Epoch 45/60\n",
            "128/128 - 0s - loss: 0.4821 - mae: 0.5154\n",
            "Epoch 46/60\n",
            "128/128 - 0s - loss: 0.4819 - mae: 0.5154\n",
            "Epoch 47/60\n",
            "128/128 - 0s - loss: 0.4754 - mae: 0.5081\n",
            "Epoch 48/60\n",
            "128/128 - 0s - loss: 0.4902 - mae: 0.5188\n",
            "Epoch 49/60\n",
            "128/128 - 0s - loss: 0.4912 - mae: 0.5150\n",
            "Epoch 50/60\n",
            "128/128 - 0s - loss: 0.4709 - mae: 0.5068\n",
            "Epoch 51/60\n",
            "128/128 - 0s - loss: 0.4765 - mae: 0.5116\n",
            "Epoch 52/60\n",
            "128/128 - 0s - loss: 0.4604 - mae: 0.5036\n",
            "Epoch 53/60\n",
            "128/128 - 0s - loss: 0.4616 - mae: 0.5043\n",
            "Epoch 54/60\n",
            "128/128 - 0s - loss: 0.4728 - mae: 0.5116\n",
            "Epoch 55/60\n",
            "128/128 - 0s - loss: 0.4653 - mae: 0.5060\n",
            "Epoch 56/60\n",
            "128/128 - 0s - loss: 0.4792 - mae: 0.5104\n",
            "Epoch 57/60\n",
            "128/128 - 0s - loss: 0.4595 - mae: 0.4992\n",
            "Epoch 58/60\n",
            "128/128 - 0s - loss: 0.4595 - mae: 0.5038\n",
            "Epoch 59/60\n",
            "128/128 - 0s - loss: 0.4449 - mae: 0.4953\n",
            "Epoch 60/60\n",
            "128/128 - 0s - loss: 0.4459 - mae: 0.4894\n",
            "[2.0, 2.0, 1.0]\n",
            "[2.0, 2.0, 1.0]\n",
            "[1.4243183, 1.6186545, 0.70355994]\n",
            "[1.1702005, 1.4131849, 0.73063326]\n",
            "[1.9054125547409058, 2.109163284301758, 0.8278377652168274]\n",
            "(4167, 3, 1)\n",
            "(4167, 100, 2, 1)\n",
            "(4167, 100, 2, 1)\n",
            "Model: \"functional_1453\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_364 (InputLayer)       [(None, 100, 2, 1)]       0         \n",
            "_________________________________________________________________\n",
            "conv2d_1452 (Conv2D)         (None, 100, 2, 16)        112       \n",
            "_________________________________________________________________\n",
            "batch_normalization_1452 (Ba (None, 100, 2, 16)        64        \n",
            "_________________________________________________________________\n",
            "activation_1815 (Activation) (None, 100, 2, 16)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_363 (MaxPoolin (None, 50, 2, 16)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_1453 (Conv2D)         (None, 50, 2, 12)         1164      \n",
            "_________________________________________________________________\n",
            "batch_normalization_1453 (Ba (None, 50, 2, 12)         48        \n",
            "_________________________________________________________________\n",
            "up_sampling2d_363 (UpSamplin (None, 100, 2, 12)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1454 (Conv2D)         (None, 100, 2, 16)        1168      \n",
            "_________________________________________________________________\n",
            "batch_normalization_1454 (Ba (None, 100, 2, 16)        64        \n",
            "_________________________________________________________________\n",
            "activation_1817 (Activation) (None, 100, 2, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1455 (Conv2D)         (None, 100, 2, 1)         97        \n",
            "=================================================================\n",
            "Total params: 2,717\n",
            "Trainable params: 2,629\n",
            "Non-trainable params: 88\n",
            "_________________________________________________________________\n",
            "Epoch 1/30\n",
            "129/129 - 0s - loss: 0.0255 - mae: 0.0799\n",
            "Epoch 2/30\n",
            "129/129 - 0s - loss: 0.0081 - mae: 0.0509\n",
            "Epoch 3/30\n",
            "129/129 - 0s - loss: 0.0058 - mae: 0.0461\n",
            "Epoch 4/30\n",
            "129/129 - 0s - loss: 0.0053 - mae: 0.0454\n",
            "Epoch 5/30\n",
            "129/129 - 0s - loss: 0.0053 - mae: 0.0446\n",
            "Epoch 6/30\n",
            "129/129 - 0s - loss: 0.0035 - mae: 0.0344\n",
            "Epoch 7/30\n",
            "129/129 - 0s - loss: 0.0031 - mae: 0.0344\n",
            "Epoch 8/30\n",
            "129/129 - 0s - loss: 0.0029 - mae: 0.0322\n",
            "Epoch 9/30\n",
            "129/129 - 0s - loss: 0.0036 - mae: 0.0355\n",
            "Epoch 10/30\n",
            "129/129 - 0s - loss: 0.0028 - mae: 0.0326\n",
            "Epoch 11/30\n",
            "129/129 - 0s - loss: 0.0028 - mae: 0.0326\n",
            "Epoch 12/30\n",
            "129/129 - 0s - loss: 0.0025 - mae: 0.0300\n",
            "Epoch 13/30\n",
            "129/129 - 0s - loss: 0.0027 - mae: 0.0306\n",
            "Epoch 14/30\n",
            "129/129 - 0s - loss: 0.0023 - mae: 0.0282\n",
            "Epoch 15/30\n",
            "129/129 - 0s - loss: 0.0025 - mae: 0.0290\n",
            "Epoch 16/30\n",
            "129/129 - 0s - loss: 0.0027 - mae: 0.0302\n",
            "Epoch 17/30\n",
            "129/129 - 0s - loss: 0.0021 - mae: 0.0275\n",
            "Epoch 18/30\n",
            "129/129 - 0s - loss: 0.0027 - mae: 0.0297\n",
            "Epoch 19/30\n",
            "129/129 - 0s - loss: 0.0022 - mae: 0.0274\n",
            "Epoch 20/30\n",
            "129/129 - 0s - loss: 0.0023 - mae: 0.0279\n",
            "Epoch 21/30\n",
            "129/129 - 0s - loss: 0.0020 - mae: 0.0264\n",
            "Epoch 22/30\n",
            "129/129 - 0s - loss: 0.0021 - mae: 0.0269\n",
            "Epoch 23/30\n",
            "129/129 - 0s - loss: 0.0026 - mae: 0.0293\n",
            "Epoch 24/30\n",
            "129/129 - 0s - loss: 0.0021 - mae: 0.0269\n",
            "Epoch 25/30\n",
            "129/129 - 0s - loss: 0.0019 - mae: 0.0259\n",
            "Epoch 26/30\n",
            "129/129 - 0s - loss: 0.0018 - mae: 0.0246\n",
            "Epoch 27/30\n",
            "129/129 - 0s - loss: 0.0017 - mae: 0.0239\n",
            "Epoch 28/30\n",
            "129/129 - 0s - loss: 0.0016 - mae: 0.0232\n",
            "Epoch 29/30\n",
            "129/129 - 0s - loss: 0.0017 - mae: 0.0240\n",
            "Epoch 30/30\n",
            "129/129 - 0s - loss: 0.0021 - mae: 0.0272\n",
            "Model: \"functional_1455\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_364 (InputLayer)       [(None, 100, 2, 1)]       0         \n",
            "_________________________________________________________________\n",
            "conv2d_1452 (Conv2D)         (None, 100, 2, 16)        112       \n",
            "_________________________________________________________________\n",
            "batch_normalization_1452 (Ba (None, 100, 2, 16)        64        \n",
            "_________________________________________________________________\n",
            "activation_1815 (Activation) (None, 100, 2, 16)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_363 (MaxPoolin (None, 50, 2, 16)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_1453 (Conv2D)         (None, 50, 2, 12)         1164      \n",
            "_________________________________________________________________\n",
            "batch_normalization_1453 (Ba (None, 50, 2, 12)         48        \n",
            "_________________________________________________________________\n",
            "activation_1816 (Activation) (None, 50, 2, 12)         0         \n",
            "_________________________________________________________________\n",
            "flatten_363 (Flatten)        (None, 1200)              0         \n",
            "_________________________________________________________________\n",
            "dense_726 (Dense)            (None, 16)                19216     \n",
            "_________________________________________________________________\n",
            "batch_normalization_1455 (Ba (None, 16)                64        \n",
            "_________________________________________________________________\n",
            "activation_1818 (Activation) (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dropout_363 (Dropout)        (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_727 (Dense)            (None, 3)                 51        \n",
            "_________________________________________________________________\n",
            "activation_1819 (Activation) (None, 3)                 0         \n",
            "=================================================================\n",
            "Total params: 20,719\n",
            "Trainable params: 20,631\n",
            "Non-trainable params: 88\n",
            "_________________________________________________________________\n",
            "Epoch 1/60\n",
            "128/128 - 0s - loss: 2.0372 - mae: 1.0404\n",
            "Epoch 2/60\n",
            "128/128 - 0s - loss: 1.1395 - mae: 0.7704\n",
            "Epoch 3/60\n",
            "128/128 - 0s - loss: 0.9066 - mae: 0.6816\n",
            "Epoch 4/60\n",
            "128/128 - 0s - loss: 0.8393 - mae: 0.6544\n",
            "Epoch 5/60\n",
            "128/128 - 0s - loss: 0.7551 - mae: 0.6308\n",
            "Epoch 6/60\n",
            "128/128 - 0s - loss: 0.7328 - mae: 0.6182\n",
            "Epoch 7/60\n",
            "128/128 - 0s - loss: 0.7005 - mae: 0.6119\n",
            "Epoch 8/60\n",
            "128/128 - 0s - loss: 0.6891 - mae: 0.6070\n",
            "Epoch 9/60\n",
            "128/128 - 0s - loss: 0.6579 - mae: 0.5962\n",
            "Epoch 10/60\n",
            "128/128 - 0s - loss: 0.6288 - mae: 0.5877\n",
            "Epoch 11/60\n",
            "128/128 - 0s - loss: 0.6298 - mae: 0.5890\n",
            "Epoch 12/60\n",
            "128/128 - 0s - loss: 0.6119 - mae: 0.5821\n",
            "Epoch 13/60\n",
            "128/128 - 0s - loss: 0.6154 - mae: 0.5859\n",
            "Epoch 14/60\n",
            "128/128 - 0s - loss: 0.6017 - mae: 0.5756\n",
            "Epoch 15/60\n",
            "128/128 - 0s - loss: 0.5868 - mae: 0.5695\n",
            "Epoch 16/60\n",
            "128/128 - 0s - loss: 0.5852 - mae: 0.5673\n",
            "Epoch 17/60\n",
            "128/128 - 0s - loss: 0.5769 - mae: 0.5667\n",
            "Epoch 18/60\n",
            "128/128 - 0s - loss: 0.5568 - mae: 0.5565\n",
            "Epoch 19/60\n",
            "128/128 - 0s - loss: 0.5781 - mae: 0.5633\n",
            "Epoch 20/60\n",
            "128/128 - 0s - loss: 0.5469 - mae: 0.5536\n",
            "Epoch 21/60\n",
            "128/128 - 0s - loss: 0.5424 - mae: 0.5489\n",
            "Epoch 22/60\n",
            "128/128 - 0s - loss: 0.5536 - mae: 0.5502\n",
            "Epoch 23/60\n",
            "128/128 - 0s - loss: 0.5464 - mae: 0.5470\n",
            "Epoch 24/60\n",
            "128/128 - 0s - loss: 0.5409 - mae: 0.5499\n",
            "Epoch 25/60\n",
            "128/128 - 0s - loss: 0.5382 - mae: 0.5455\n",
            "Epoch 26/60\n",
            "128/128 - 0s - loss: 0.5318 - mae: 0.5425\n",
            "Epoch 27/60\n",
            "128/128 - 0s - loss: 0.5166 - mae: 0.5323\n",
            "Epoch 28/60\n",
            "128/128 - 0s - loss: 0.5406 - mae: 0.5466\n",
            "Epoch 29/60\n",
            "128/128 - 0s - loss: 0.4992 - mae: 0.5248\n",
            "Epoch 30/60\n",
            "128/128 - 0s - loss: 0.5225 - mae: 0.5381\n",
            "Epoch 31/60\n",
            "128/128 - 0s - loss: 0.5229 - mae: 0.5344\n",
            "Epoch 32/60\n",
            "128/128 - 0s - loss: 0.5091 - mae: 0.5304\n",
            "Epoch 33/60\n",
            "128/128 - 0s - loss: 0.5039 - mae: 0.5294\n",
            "Epoch 34/60\n",
            "128/128 - 0s - loss: 0.4927 - mae: 0.5228\n",
            "Epoch 35/60\n",
            "128/128 - 0s - loss: 0.5002 - mae: 0.5201\n",
            "Epoch 36/60\n",
            "128/128 - 0s - loss: 0.4965 - mae: 0.5277\n",
            "Epoch 37/60\n",
            "128/128 - 0s - loss: 0.4799 - mae: 0.5157\n",
            "Epoch 38/60\n",
            "128/128 - 0s - loss: 0.4985 - mae: 0.5216\n",
            "Epoch 39/60\n",
            "128/128 - 0s - loss: 0.4685 - mae: 0.5097\n",
            "Epoch 40/60\n",
            "128/128 - 0s - loss: 0.4845 - mae: 0.5144\n",
            "Epoch 41/60\n",
            "128/128 - 0s - loss: 0.4811 - mae: 0.5160\n",
            "Epoch 42/60\n",
            "128/128 - 0s - loss: 0.4721 - mae: 0.5065\n",
            "Epoch 43/60\n",
            "128/128 - 0s - loss: 0.4802 - mae: 0.5147\n",
            "Epoch 44/60\n",
            "128/128 - 0s - loss: 0.4732 - mae: 0.5096\n",
            "Epoch 45/60\n",
            "128/128 - 0s - loss: 0.4597 - mae: 0.5012\n",
            "Epoch 46/60\n",
            "128/128 - 0s - loss: 0.4656 - mae: 0.5069\n",
            "Epoch 47/60\n",
            "128/128 - 0s - loss: 0.4727 - mae: 0.5064\n",
            "Epoch 48/60\n",
            "128/128 - 0s - loss: 0.4779 - mae: 0.5118\n",
            "Epoch 49/60\n",
            "128/128 - 0s - loss: 0.4591 - mae: 0.4987\n",
            "Epoch 50/60\n",
            "128/128 - 0s - loss: 0.4503 - mae: 0.4982\n",
            "Epoch 51/60\n",
            "128/128 - 0s - loss: 0.4647 - mae: 0.4998\n",
            "Epoch 52/60\n",
            "128/128 - 0s - loss: 0.4646 - mae: 0.5046\n",
            "Epoch 53/60\n",
            "128/128 - 0s - loss: 0.4574 - mae: 0.4988\n",
            "Epoch 54/60\n",
            "128/128 - 0s - loss: 0.4571 - mae: 0.4960\n",
            "Epoch 55/60\n",
            "128/128 - 0s - loss: 0.4645 - mae: 0.5036\n",
            "Epoch 56/60\n",
            "128/128 - 0s - loss: 0.4399 - mae: 0.4908\n",
            "Epoch 57/60\n",
            "128/128 - 0s - loss: 0.4472 - mae: 0.4948\n",
            "Epoch 58/60\n",
            "128/128 - 0s - loss: 0.4401 - mae: 0.4862\n",
            "Epoch 59/60\n",
            "128/128 - 0s - loss: 0.4369 - mae: 0.4901\n",
            "Epoch 60/60\n",
            "128/128 - 0s - loss: 0.4292 - mae: 0.4888\n",
            "WARNING:tensorflow:5 out of the last 14 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fa185cd1158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "[1.0, 0.0, 0.0]\n",
            "[1.0, 0.0, 0.0]\n",
            "[0.8803758, 0.66515106, 0.48800468]\n",
            "[0.7561583, 0.5865628, 0.45429528]\n",
            "[1.1010408401489258, 0.8193624317646027, 0.5545143485069275]\n",
            "(4167, 3, 1)\n",
            "(4167, 100, 2, 1)\n",
            "(4167, 100, 2, 1)\n",
            "Model: \"functional_1457\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_365 (InputLayer)       [(None, 100, 2, 1)]       0         \n",
            "_________________________________________________________________\n",
            "conv2d_1456 (Conv2D)         (None, 100, 2, 16)        112       \n",
            "_________________________________________________________________\n",
            "batch_normalization_1456 (Ba (None, 100, 2, 16)        64        \n",
            "_________________________________________________________________\n",
            "activation_1820 (Activation) (None, 100, 2, 16)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_364 (MaxPoolin (None, 50, 2, 16)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_1457 (Conv2D)         (None, 50, 2, 12)         1164      \n",
            "_________________________________________________________________\n",
            "batch_normalization_1457 (Ba (None, 50, 2, 12)         48        \n",
            "_________________________________________________________________\n",
            "up_sampling2d_364 (UpSamplin (None, 100, 2, 12)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1458 (Conv2D)         (None, 100, 2, 16)        1168      \n",
            "_________________________________________________________________\n",
            "batch_normalization_1458 (Ba (None, 100, 2, 16)        64        \n",
            "_________________________________________________________________\n",
            "activation_1822 (Activation) (None, 100, 2, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1459 (Conv2D)         (None, 100, 2, 1)         97        \n",
            "=================================================================\n",
            "Total params: 2,717\n",
            "Trainable params: 2,629\n",
            "Non-trainable params: 88\n",
            "_________________________________________________________________\n",
            "Epoch 1/30\n",
            "130/130 - 0s - loss: 0.0268 - mae: 0.0811\n",
            "Epoch 2/30\n",
            "130/130 - 0s - loss: 0.0072 - mae: 0.0498\n",
            "Epoch 3/30\n",
            "130/130 - 0s - loss: 0.0067 - mae: 0.0489\n",
            "Epoch 4/30\n",
            "130/130 - 0s - loss: 0.0041 - mae: 0.0385\n",
            "Epoch 5/30\n",
            "130/130 - 0s - loss: 0.0044 - mae: 0.0408\n",
            "Epoch 6/30\n",
            "130/130 - 0s - loss: 0.0036 - mae: 0.0364\n",
            "Epoch 7/30\n",
            "130/130 - 0s - loss: 0.0036 - mae: 0.0360\n",
            "Epoch 8/30\n",
            "130/130 - 0s - loss: 0.0027 - mae: 0.0308\n",
            "Epoch 9/30\n",
            "130/130 - 0s - loss: 0.0024 - mae: 0.0288\n",
            "Epoch 10/30\n",
            "130/130 - 0s - loss: 0.0032 - mae: 0.0336\n",
            "Epoch 11/30\n",
            "130/130 - 0s - loss: 0.0025 - mae: 0.0311\n",
            "Epoch 12/30\n",
            "130/130 - 0s - loss: 0.0023 - mae: 0.0282\n",
            "Epoch 13/30\n",
            "130/130 - 0s - loss: 0.0021 - mae: 0.0269\n",
            "Epoch 14/30\n",
            "130/130 - 0s - loss: 0.0022 - mae: 0.0284\n",
            "Epoch 15/30\n",
            "130/130 - 0s - loss: 0.0024 - mae: 0.0297\n",
            "Epoch 16/30\n",
            "130/130 - 0s - loss: 0.0022 - mae: 0.0278\n",
            "Epoch 17/30\n",
            "130/130 - 0s - loss: 0.0023 - mae: 0.0285\n",
            "Epoch 18/30\n",
            "130/130 - 0s - loss: 0.0021 - mae: 0.0268\n",
            "Epoch 19/30\n",
            "130/130 - 0s - loss: 0.0021 - mae: 0.0272\n",
            "Epoch 20/30\n",
            "130/130 - 0s - loss: 0.0020 - mae: 0.0258\n",
            "Epoch 21/30\n",
            "130/130 - 0s - loss: 0.0028 - mae: 0.0303\n",
            "Epoch 22/30\n",
            "130/130 - 0s - loss: 0.0019 - mae: 0.0256\n",
            "Epoch 23/30\n",
            "130/130 - 0s - loss: 0.0023 - mae: 0.0269\n",
            "Epoch 24/30\n",
            "130/130 - 0s - loss: 0.0024 - mae: 0.0283\n",
            "Epoch 25/30\n",
            "130/130 - 0s - loss: 0.0023 - mae: 0.0261\n",
            "Epoch 26/30\n",
            "130/130 - 0s - loss: 0.0029 - mae: 0.0294\n",
            "Epoch 27/30\n",
            "130/130 - 0s - loss: 0.0020 - mae: 0.0267\n",
            "Epoch 28/30\n",
            "130/130 - 0s - loss: 0.0017 - mae: 0.0239\n",
            "Epoch 29/30\n",
            "130/130 - 0s - loss: 0.0017 - mae: 0.0239\n",
            "Epoch 30/30\n",
            "130/130 - 0s - loss: 0.0017 - mae: 0.0245\n",
            "Model: \"functional_1459\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_365 (InputLayer)       [(None, 100, 2, 1)]       0         \n",
            "_________________________________________________________________\n",
            "conv2d_1456 (Conv2D)         (None, 100, 2, 16)        112       \n",
            "_________________________________________________________________\n",
            "batch_normalization_1456 (Ba (None, 100, 2, 16)        64        \n",
            "_________________________________________________________________\n",
            "activation_1820 (Activation) (None, 100, 2, 16)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_364 (MaxPoolin (None, 50, 2, 16)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_1457 (Conv2D)         (None, 50, 2, 12)         1164      \n",
            "_________________________________________________________________\n",
            "batch_normalization_1457 (Ba (None, 50, 2, 12)         48        \n",
            "_________________________________________________________________\n",
            "activation_1821 (Activation) (None, 50, 2, 12)         0         \n",
            "_________________________________________________________________\n",
            "flatten_364 (Flatten)        (None, 1200)              0         \n",
            "_________________________________________________________________\n",
            "dense_728 (Dense)            (None, 16)                19216     \n",
            "_________________________________________________________________\n",
            "batch_normalization_1459 (Ba (None, 16)                64        \n",
            "_________________________________________________________________\n",
            "activation_1823 (Activation) (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dropout_364 (Dropout)        (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_729 (Dense)            (None, 3)                 51        \n",
            "_________________________________________________________________\n",
            "activation_1824 (Activation) (None, 3)                 0         \n",
            "=================================================================\n",
            "Total params: 20,719\n",
            "Trainable params: 20,631\n",
            "Non-trainable params: 88\n",
            "_________________________________________________________________\n",
            "Epoch 1/60\n",
            "129/129 - 0s - loss: 2.0740 - mae: 1.0605\n",
            "Epoch 2/60\n",
            "129/129 - 0s - loss: 1.1305 - mae: 0.7724\n",
            "Epoch 3/60\n",
            "129/129 - 0s - loss: 0.9218 - mae: 0.6874\n",
            "Epoch 4/60\n",
            "129/129 - 0s - loss: 0.8175 - mae: 0.6513\n",
            "Epoch 5/60\n",
            "129/129 - 0s - loss: 0.7730 - mae: 0.6336\n",
            "Epoch 6/60\n",
            "129/129 - 0s - loss: 0.7270 - mae: 0.6230\n",
            "Epoch 7/60\n",
            "129/129 - 0s - loss: 0.7044 - mae: 0.6137\n",
            "Epoch 8/60\n",
            "129/129 - 0s - loss: 0.6763 - mae: 0.6047\n",
            "Epoch 9/60\n",
            "129/129 - 0s - loss: 0.6597 - mae: 0.6020\n",
            "Epoch 10/60\n",
            "129/129 - 0s - loss: 0.6341 - mae: 0.5915\n",
            "Epoch 11/60\n",
            "129/129 - 0s - loss: 0.6354 - mae: 0.5911\n",
            "Epoch 12/60\n",
            "129/129 - 0s - loss: 0.6188 - mae: 0.5825\n",
            "Epoch 13/60\n",
            "129/129 - 0s - loss: 0.5901 - mae: 0.5713\n",
            "Epoch 14/60\n",
            "129/129 - 0s - loss: 0.6071 - mae: 0.5789\n",
            "Epoch 15/60\n",
            "129/129 - 0s - loss: 0.6092 - mae: 0.5802\n",
            "Epoch 16/60\n",
            "129/129 - 0s - loss: 0.5865 - mae: 0.5711\n",
            "Epoch 17/60\n",
            "129/129 - 0s - loss: 0.5844 - mae: 0.5709\n",
            "Epoch 18/60\n",
            "129/129 - 0s - loss: 0.5840 - mae: 0.5747\n",
            "Epoch 19/60\n",
            "129/129 - 0s - loss: 0.5754 - mae: 0.5643\n",
            "Epoch 20/60\n",
            "129/129 - 0s - loss: 0.5718 - mae: 0.5633\n",
            "Epoch 21/60\n",
            "129/129 - 0s - loss: 0.5753 - mae: 0.5630\n",
            "Epoch 22/60\n",
            "129/129 - 0s - loss: 0.5670 - mae: 0.5558\n",
            "Epoch 23/60\n",
            "129/129 - 0s - loss: 0.5612 - mae: 0.5569\n",
            "Epoch 24/60\n",
            "129/129 - 0s - loss: 0.5450 - mae: 0.5507\n",
            "Epoch 25/60\n",
            "129/129 - 0s - loss: 0.5468 - mae: 0.5479\n",
            "Epoch 26/60\n",
            "129/129 - 0s - loss: 0.5440 - mae: 0.5516\n",
            "Epoch 27/60\n",
            "129/129 - 0s - loss: 0.5355 - mae: 0.5426\n",
            "Epoch 28/60\n",
            "129/129 - 0s - loss: 0.5427 - mae: 0.5455\n",
            "Epoch 29/60\n",
            "129/129 - 0s - loss: 0.5345 - mae: 0.5435\n",
            "Epoch 30/60\n",
            "129/129 - 0s - loss: 0.5229 - mae: 0.5358\n",
            "Epoch 31/60\n",
            "129/129 - 0s - loss: 0.5335 - mae: 0.5439\n",
            "Epoch 32/60\n",
            "129/129 - 0s - loss: 0.5278 - mae: 0.5382\n",
            "Epoch 33/60\n",
            "129/129 - 0s - loss: 0.5232 - mae: 0.5410\n",
            "Epoch 34/60\n",
            "129/129 - 0s - loss: 0.5116 - mae: 0.5298\n",
            "Epoch 35/60\n",
            "129/129 - 0s - loss: 0.5040 - mae: 0.5296\n",
            "Epoch 36/60\n",
            "129/129 - 0s - loss: 0.5156 - mae: 0.5332\n",
            "Epoch 37/60\n",
            "129/129 - 0s - loss: 0.5065 - mae: 0.5273\n",
            "Epoch 38/60\n",
            "129/129 - 0s - loss: 0.5177 - mae: 0.5336\n",
            "Epoch 39/60\n",
            "129/129 - 0s - loss: 0.4875 - mae: 0.5150\n",
            "Epoch 40/60\n",
            "129/129 - 0s - loss: 0.5054 - mae: 0.5233\n",
            "Epoch 41/60\n",
            "129/129 - 0s - loss: 0.4896 - mae: 0.5181\n",
            "Epoch 42/60\n",
            "129/129 - 0s - loss: 0.4872 - mae: 0.5163\n",
            "Epoch 43/60\n",
            "129/129 - 0s - loss: 0.5040 - mae: 0.5212\n",
            "Epoch 44/60\n",
            "129/129 - 0s - loss: 0.4671 - mae: 0.5100\n",
            "Epoch 45/60\n",
            "129/129 - 0s - loss: 0.4788 - mae: 0.5122\n",
            "Epoch 46/60\n",
            "129/129 - 0s - loss: 0.4776 - mae: 0.5092\n",
            "Epoch 47/60\n",
            "129/129 - 0s - loss: 0.4827 - mae: 0.5137\n",
            "Epoch 48/60\n",
            "129/129 - 0s - loss: 0.4831 - mae: 0.5156\n",
            "Epoch 49/60\n",
            "129/129 - 0s - loss: 0.4692 - mae: 0.5079\n",
            "Epoch 50/60\n",
            "129/129 - 0s - loss: 0.4630 - mae: 0.5079\n",
            "Epoch 51/60\n",
            "129/129 - 0s - loss: 0.4753 - mae: 0.5119\n",
            "Epoch 52/60\n",
            "129/129 - 0s - loss: 0.4728 - mae: 0.5086\n",
            "Epoch 53/60\n",
            "129/129 - 0s - loss: 0.4629 - mae: 0.5011\n",
            "Epoch 54/60\n",
            "129/129 - 0s - loss: 0.4673 - mae: 0.5056\n",
            "Epoch 55/60\n",
            "129/129 - 0s - loss: 0.4604 - mae: 0.5026\n",
            "Epoch 56/60\n",
            "129/129 - 0s - loss: 0.4687 - mae: 0.5057\n",
            "Epoch 57/60\n",
            "129/129 - 0s - loss: 0.4765 - mae: 0.5048\n",
            "Epoch 58/60\n",
            "129/129 - 0s - loss: 0.4581 - mae: 0.5002\n",
            "Epoch 59/60\n",
            "129/129 - 0s - loss: 0.4626 - mae: 0.5047\n",
            "Epoch 60/60\n",
            "129/129 - 0s - loss: 0.4467 - mae: 0.4923\n",
            "WARNING:tensorflow:5 out of the last 12 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fa1879fb950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "[1.0, 1.0, 1.0]\n",
            "[1.0, 1.0, 1.0]\n",
            "[1.9097764, 2.162056, 0.8799282]\n",
            "[1.9081845, 2.127539, 0.8881004]\n",
            "[2.325305700302124, 2.659395217895508, 0.9774683117866516]\n",
            "(4167, 3, 1)\n",
            "(4167, 100, 2, 1)\n",
            "(4167, 100, 2, 1)\n",
            "Model: \"functional_1461\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_366 (InputLayer)       [(None, 100, 2, 1)]       0         \n",
            "_________________________________________________________________\n",
            "conv2d_1460 (Conv2D)         (None, 100, 2, 16)        112       \n",
            "_________________________________________________________________\n",
            "batch_normalization_1460 (Ba (None, 100, 2, 16)        64        \n",
            "_________________________________________________________________\n",
            "activation_1825 (Activation) (None, 100, 2, 16)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_365 (MaxPoolin (None, 50, 2, 16)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_1461 (Conv2D)         (None, 50, 2, 12)         1164      \n",
            "_________________________________________________________________\n",
            "batch_normalization_1461 (Ba (None, 50, 2, 12)         48        \n",
            "_________________________________________________________________\n",
            "up_sampling2d_365 (UpSamplin (None, 100, 2, 12)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1462 (Conv2D)         (None, 100, 2, 16)        1168      \n",
            "_________________________________________________________________\n",
            "batch_normalization_1462 (Ba (None, 100, 2, 16)        64        \n",
            "_________________________________________________________________\n",
            "activation_1827 (Activation) (None, 100, 2, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1463 (Conv2D)         (None, 100, 2, 1)         97        \n",
            "=================================================================\n",
            "Total params: 2,717\n",
            "Trainable params: 2,629\n",
            "Non-trainable params: 88\n",
            "_________________________________________________________________\n",
            "Epoch 1/30\n",
            "130/130 - 0s - loss: 0.0266 - mae: 0.0818\n",
            "Epoch 2/30\n",
            "130/130 - 0s - loss: 0.0067 - mae: 0.0475\n",
            "Epoch 3/30\n",
            "130/130 - 0s - loss: 0.0057 - mae: 0.0457\n",
            "Epoch 4/30\n",
            "130/130 - 0s - loss: 0.0052 - mae: 0.0431\n",
            "Epoch 5/30\n",
            "130/130 - 0s - loss: 0.0035 - mae: 0.0344\n",
            "Epoch 6/30\n",
            "130/130 - 0s - loss: 0.0033 - mae: 0.0339\n",
            "Epoch 7/30\n",
            "130/130 - 0s - loss: 0.0036 - mae: 0.0390\n",
            "Epoch 8/30\n",
            "130/130 - 0s - loss: 0.0029 - mae: 0.0305\n",
            "Epoch 9/30\n",
            "130/130 - 0s - loss: 0.0032 - mae: 0.0337\n",
            "Epoch 10/30\n",
            "130/130 - 0s - loss: 0.0026 - mae: 0.0300\n",
            "Epoch 11/30\n",
            "130/130 - 0s - loss: 0.0026 - mae: 0.0300\n",
            "Epoch 12/30\n",
            "130/130 - 0s - loss: 0.0025 - mae: 0.0298\n",
            "Epoch 13/30\n",
            "130/130 - 0s - loss: 0.0023 - mae: 0.0279\n",
            "Epoch 14/30\n",
            "130/130 - 0s - loss: 0.0023 - mae: 0.0289\n",
            "Epoch 15/30\n",
            "130/130 - 0s - loss: 0.0021 - mae: 0.0272\n",
            "Epoch 16/30\n",
            "130/130 - 0s - loss: 0.0023 - mae: 0.0269\n",
            "Epoch 17/30\n",
            "130/130 - 0s - loss: 0.0022 - mae: 0.0279\n",
            "Epoch 18/30\n",
            "130/130 - 0s - loss: 0.0023 - mae: 0.0291\n",
            "Epoch 19/30\n",
            "130/130 - 0s - loss: 0.0022 - mae: 0.0280\n",
            "Epoch 20/30\n",
            "130/130 - 0s - loss: 0.0019 - mae: 0.0255\n",
            "Epoch 21/30\n",
            "130/130 - 0s - loss: 0.0020 - mae: 0.0261\n",
            "Epoch 22/30\n",
            "130/130 - 0s - loss: 0.0017 - mae: 0.0239\n",
            "Epoch 23/30\n",
            "130/130 - 0s - loss: 0.0029 - mae: 0.0323\n",
            "Epoch 24/30\n",
            "130/130 - 0s - loss: 0.0019 - mae: 0.0256\n",
            "Epoch 25/30\n",
            "130/130 - 0s - loss: 0.0017 - mae: 0.0241\n",
            "Epoch 26/30\n",
            "130/130 - 0s - loss: 0.0021 - mae: 0.0267\n",
            "Epoch 27/30\n",
            "130/130 - 0s - loss: 0.0022 - mae: 0.0280\n",
            "Epoch 28/30\n",
            "130/130 - 0s - loss: 0.0019 - mae: 0.0253\n",
            "Epoch 29/30\n",
            "130/130 - 0s - loss: 0.0021 - mae: 0.0266\n",
            "Epoch 30/30\n",
            "130/130 - 0s - loss: 0.0018 - mae: 0.0257\n",
            "Model: \"functional_1463\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_366 (InputLayer)       [(None, 100, 2, 1)]       0         \n",
            "_________________________________________________________________\n",
            "conv2d_1460 (Conv2D)         (None, 100, 2, 16)        112       \n",
            "_________________________________________________________________\n",
            "batch_normalization_1460 (Ba (None, 100, 2, 16)        64        \n",
            "_________________________________________________________________\n",
            "activation_1825 (Activation) (None, 100, 2, 16)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_365 (MaxPoolin (None, 50, 2, 16)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_1461 (Conv2D)         (None, 50, 2, 12)         1164      \n",
            "_________________________________________________________________\n",
            "batch_normalization_1461 (Ba (None, 50, 2, 12)         48        \n",
            "_________________________________________________________________\n",
            "activation_1826 (Activation) (None, 50, 2, 12)         0         \n",
            "_________________________________________________________________\n",
            "flatten_365 (Flatten)        (None, 1200)              0         \n",
            "_________________________________________________________________\n",
            "dense_730 (Dense)            (None, 16)                19216     \n",
            "_________________________________________________________________\n",
            "batch_normalization_1463 (Ba (None, 16)                64        \n",
            "_________________________________________________________________\n",
            "activation_1828 (Activation) (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dropout_365 (Dropout)        (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_731 (Dense)            (None, 3)                 51        \n",
            "_________________________________________________________________\n",
            "activation_1829 (Activation) (None, 3)                 0         \n",
            "=================================================================\n",
            "Total params: 20,719\n",
            "Trainable params: 20,631\n",
            "Non-trainable params: 88\n",
            "_________________________________________________________________\n",
            "Epoch 1/60\n",
            "129/129 - 0s - loss: 2.0660 - mae: 1.0537\n",
            "Epoch 2/60\n",
            "129/129 - 0s - loss: 1.1783 - mae: 0.7973\n",
            "Epoch 3/60\n",
            "129/129 - 0s - loss: 0.9229 - mae: 0.7015\n",
            "Epoch 4/60\n",
            "129/129 - 0s - loss: 0.8226 - mae: 0.6600\n",
            "Epoch 5/60\n",
            "129/129 - 0s - loss: 0.7696 - mae: 0.6344\n",
            "Epoch 6/60\n",
            "129/129 - 0s - loss: 0.7451 - mae: 0.6258\n",
            "Epoch 7/60\n",
            "129/129 - 0s - loss: 0.7081 - mae: 0.6162\n",
            "Epoch 8/60\n",
            "129/129 - 0s - loss: 0.6733 - mae: 0.6088\n",
            "Epoch 9/60\n",
            "129/129 - 0s - loss: 0.6476 - mae: 0.5963\n",
            "Epoch 10/60\n",
            "129/129 - 0s - loss: 0.6304 - mae: 0.5916\n",
            "Epoch 11/60\n",
            "129/129 - 0s - loss: 0.6327 - mae: 0.5915\n",
            "Epoch 12/60\n",
            "129/129 - 0s - loss: 0.6106 - mae: 0.5834\n",
            "Epoch 13/60\n",
            "129/129 - 0s - loss: 0.6040 - mae: 0.5832\n",
            "Epoch 14/60\n",
            "129/129 - 0s - loss: 0.6010 - mae: 0.5814\n",
            "Epoch 15/60\n",
            "129/129 - 0s - loss: 0.5955 - mae: 0.5761\n",
            "Epoch 16/60\n",
            "129/129 - 0s - loss: 0.5837 - mae: 0.5717\n",
            "Epoch 17/60\n",
            "129/129 - 0s - loss: 0.5860 - mae: 0.5694\n",
            "Epoch 18/60\n",
            "129/129 - 0s - loss: 0.5843 - mae: 0.5665\n",
            "Epoch 19/60\n",
            "129/129 - 0s - loss: 0.5515 - mae: 0.5543\n",
            "Epoch 20/60\n",
            "129/129 - 0s - loss: 0.5557 - mae: 0.5581\n",
            "Epoch 21/60\n",
            "129/129 - 0s - loss: 0.5448 - mae: 0.5520\n",
            "Epoch 22/60\n",
            "129/129 - 0s - loss: 0.5525 - mae: 0.5546\n",
            "Epoch 23/60\n",
            "129/129 - 0s - loss: 0.5323 - mae: 0.5426\n",
            "Epoch 24/60\n",
            "129/129 - 0s - loss: 0.5303 - mae: 0.5462\n",
            "Epoch 25/60\n",
            "129/129 - 0s - loss: 0.5300 - mae: 0.5438\n",
            "Epoch 26/60\n",
            "129/129 - 0s - loss: 0.5406 - mae: 0.5439\n",
            "Epoch 27/60\n",
            "129/129 - 0s - loss: 0.5198 - mae: 0.5400\n",
            "Epoch 28/60\n",
            "129/129 - 0s - loss: 0.5139 - mae: 0.5319\n",
            "Epoch 29/60\n",
            "129/129 - 0s - loss: 0.5172 - mae: 0.5370\n",
            "Epoch 30/60\n",
            "129/129 - 0s - loss: 0.5100 - mae: 0.5321\n",
            "Epoch 31/60\n",
            "129/129 - 0s - loss: 0.5004 - mae: 0.5282\n",
            "Epoch 32/60\n",
            "129/129 - 0s - loss: 0.5063 - mae: 0.5294\n",
            "Epoch 33/60\n",
            "129/129 - 0s - loss: 0.5028 - mae: 0.5294\n",
            "Epoch 34/60\n",
            "129/129 - 0s - loss: 0.5047 - mae: 0.5270\n",
            "Epoch 35/60\n",
            "129/129 - 0s - loss: 0.4799 - mae: 0.5155\n",
            "Epoch 36/60\n",
            "129/129 - 0s - loss: 0.4972 - mae: 0.5200\n",
            "Epoch 37/60\n",
            "129/129 - 0s - loss: 0.4768 - mae: 0.5157\n",
            "Epoch 38/60\n",
            "129/129 - 0s - loss: 0.4943 - mae: 0.5189\n",
            "Epoch 39/60\n",
            "129/129 - 0s - loss: 0.4917 - mae: 0.5224\n",
            "Epoch 40/60\n",
            "129/129 - 0s - loss: 0.4828 - mae: 0.5134\n",
            "Epoch 41/60\n",
            "129/129 - 0s - loss: 0.4751 - mae: 0.5108\n",
            "Epoch 42/60\n",
            "129/129 - 0s - loss: 0.4714 - mae: 0.5096\n",
            "Epoch 43/60\n",
            "129/129 - 0s - loss: 0.4668 - mae: 0.5060\n",
            "Epoch 44/60\n",
            "129/129 - 0s - loss: 0.4660 - mae: 0.5050\n",
            "Epoch 45/60\n",
            "129/129 - 0s - loss: 0.4805 - mae: 0.5118\n",
            "Epoch 46/60\n",
            "129/129 - 0s - loss: 0.4784 - mae: 0.5123\n",
            "Epoch 47/60\n",
            "129/129 - 0s - loss: 0.4552 - mae: 0.5002\n",
            "Epoch 48/60\n",
            "129/129 - 0s - loss: 0.4621 - mae: 0.5034\n",
            "Epoch 49/60\n",
            "129/129 - 0s - loss: 0.4644 - mae: 0.5025\n",
            "Epoch 50/60\n",
            "129/129 - 0s - loss: 0.4647 - mae: 0.5056\n",
            "Epoch 51/60\n",
            "129/129 - 0s - loss: 0.4681 - mae: 0.5017\n",
            "Epoch 52/60\n",
            "129/129 - 0s - loss: 0.4345 - mae: 0.4925\n",
            "Epoch 53/60\n",
            "129/129 - 0s - loss: 0.4627 - mae: 0.5065\n",
            "Epoch 54/60\n",
            "129/129 - 0s - loss: 0.4509 - mae: 0.4968\n",
            "Epoch 55/60\n",
            "129/129 - 0s - loss: 0.4452 - mae: 0.4918\n",
            "Epoch 56/60\n",
            "129/129 - 0s - loss: 0.4467 - mae: 0.4971\n",
            "Epoch 57/60\n",
            "129/129 - 0s - loss: 0.4324 - mae: 0.4872\n",
            "Epoch 58/60\n",
            "129/129 - 0s - loss: 0.4453 - mae: 0.4910\n",
            "Epoch 59/60\n",
            "129/129 - 0s - loss: 0.4468 - mae: 0.4942\n",
            "Epoch 60/60\n",
            "129/129 - 0s - loss: 0.4414 - mae: 0.4877\n",
            "WARNING:tensorflow:5 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fa188392268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "[0.0, 0.0, 1.0]\n",
            "[0.0, 0.0, 1.0]\n",
            "[1.0118613, 1.0235648, 0.62552804]\n",
            "[1.0147436, 1.058813, 0.6829954]\n",
            "[1.3521307408809662, 1.3084428310394287, 0.7404557466506958]\n",
            "(4167, 3, 1)\n",
            "(4167, 100, 2, 1)\n",
            "(4167, 100, 2, 1)\n",
            "Model: \"functional_1465\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_367 (InputLayer)       [(None, 100, 2, 1)]       0         \n",
            "_________________________________________________________________\n",
            "conv2d_1464 (Conv2D)         (None, 100, 2, 16)        112       \n",
            "_________________________________________________________________\n",
            "batch_normalization_1464 (Ba (None, 100, 2, 16)        64        \n",
            "_________________________________________________________________\n",
            "activation_1830 (Activation) (None, 100, 2, 16)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_366 (MaxPoolin (None, 50, 2, 16)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_1465 (Conv2D)         (None, 50, 2, 12)         1164      \n",
            "_________________________________________________________________\n",
            "batch_normalization_1465 (Ba (None, 50, 2, 12)         48        \n",
            "_________________________________________________________________\n",
            "up_sampling2d_366 (UpSamplin (None, 100, 2, 12)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1466 (Conv2D)         (None, 100, 2, 16)        1168      \n",
            "_________________________________________________________________\n",
            "batch_normalization_1466 (Ba (None, 100, 2, 16)        64        \n",
            "_________________________________________________________________\n",
            "activation_1832 (Activation) (None, 100, 2, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1467 (Conv2D)         (None, 100, 2, 1)         97        \n",
            "=================================================================\n",
            "Total params: 2,717\n",
            "Trainable params: 2,629\n",
            "Non-trainable params: 88\n",
            "_________________________________________________________________\n",
            "Epoch 1/30\n",
            "130/130 - 0s - loss: 0.0272 - mae: 0.0831\n",
            "Epoch 2/30\n",
            "130/130 - 0s - loss: 0.0087 - mae: 0.0533\n",
            "Epoch 3/30\n",
            "130/130 - 0s - loss: 0.0050 - mae: 0.0427\n",
            "Epoch 4/30\n",
            "130/130 - 0s - loss: 0.0045 - mae: 0.0409\n",
            "Epoch 5/30\n",
            "130/130 - 0s - loss: 0.0038 - mae: 0.0380\n",
            "Epoch 6/30\n",
            "130/130 - 0s - loss: 0.0041 - mae: 0.0378\n",
            "Epoch 7/30\n",
            "130/130 - 0s - loss: 0.0028 - mae: 0.0321\n",
            "Epoch 8/30\n",
            "130/130 - 0s - loss: 0.0038 - mae: 0.0383\n",
            "Epoch 9/30\n",
            "130/130 - 0s - loss: 0.0025 - mae: 0.0304\n",
            "Epoch 10/30\n",
            "130/130 - 0s - loss: 0.0031 - mae: 0.0343\n",
            "Epoch 11/30\n",
            "130/130 - 0s - loss: 0.0026 - mae: 0.0305\n",
            "Epoch 12/30\n",
            "130/130 - 0s - loss: 0.0025 - mae: 0.0294\n",
            "Epoch 13/30\n",
            "130/130 - 0s - loss: 0.0025 - mae: 0.0295\n",
            "Epoch 14/30\n",
            "130/130 - 0s - loss: 0.0025 - mae: 0.0298\n",
            "Epoch 15/30\n",
            "130/130 - 0s - loss: 0.0026 - mae: 0.0299\n",
            "Epoch 16/30\n",
            "130/130 - 0s - loss: 0.0020 - mae: 0.0264\n",
            "Epoch 17/30\n",
            "130/130 - 0s - loss: 0.0022 - mae: 0.0276\n",
            "Epoch 18/30\n",
            "130/130 - 0s - loss: 0.0025 - mae: 0.0301\n",
            "Epoch 19/30\n",
            "130/130 - 0s - loss: 0.0021 - mae: 0.0265\n",
            "Epoch 20/30\n",
            "130/130 - 0s - loss: 0.0017 - mae: 0.0247\n",
            "Epoch 21/30\n",
            "130/130 - 0s - loss: 0.0021 - mae: 0.0276\n",
            "Epoch 22/30\n",
            "130/130 - 0s - loss: 0.0020 - mae: 0.0273\n",
            "Epoch 23/30\n",
            "130/130 - 0s - loss: 0.0023 - mae: 0.0283\n",
            "Epoch 24/30\n",
            "130/130 - 0s - loss: 0.0019 - mae: 0.0253\n",
            "Epoch 25/30\n",
            "130/130 - 0s - loss: 0.0021 - mae: 0.0257\n",
            "Epoch 26/30\n",
            "130/130 - 0s - loss: 0.0024 - mae: 0.0282\n",
            "Epoch 27/30\n",
            "130/130 - 0s - loss: 0.0019 - mae: 0.0259\n",
            "Epoch 28/30\n",
            "130/130 - 0s - loss: 0.0021 - mae: 0.0269\n",
            "Epoch 29/30\n",
            "130/130 - 0s - loss: 0.0019 - mae: 0.0251\n",
            "Epoch 30/30\n",
            "130/130 - 0s - loss: 0.0019 - mae: 0.0252\n",
            "Model: \"functional_1467\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_367 (InputLayer)       [(None, 100, 2, 1)]       0         \n",
            "_________________________________________________________________\n",
            "conv2d_1464 (Conv2D)         (None, 100, 2, 16)        112       \n",
            "_________________________________________________________________\n",
            "batch_normalization_1464 (Ba (None, 100, 2, 16)        64        \n",
            "_________________________________________________________________\n",
            "activation_1830 (Activation) (None, 100, 2, 16)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_366 (MaxPoolin (None, 50, 2, 16)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_1465 (Conv2D)         (None, 50, 2, 12)         1164      \n",
            "_________________________________________________________________\n",
            "batch_normalization_1465 (Ba (None, 50, 2, 12)         48        \n",
            "_________________________________________________________________\n",
            "activation_1831 (Activation) (None, 50, 2, 12)         0         \n",
            "_________________________________________________________________\n",
            "flatten_366 (Flatten)        (None, 1200)              0         \n",
            "_________________________________________________________________\n",
            "dense_732 (Dense)            (None, 16)                19216     \n",
            "_________________________________________________________________\n",
            "batch_normalization_1467 (Ba (None, 16)                64        \n",
            "_________________________________________________________________\n",
            "activation_1833 (Activation) (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dropout_366 (Dropout)        (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_733 (Dense)            (None, 3)                 51        \n",
            "_________________________________________________________________\n",
            "activation_1834 (Activation) (None, 3)                 0         \n",
            "=================================================================\n",
            "Total params: 20,719\n",
            "Trainable params: 20,631\n",
            "Non-trainable params: 88\n",
            "_________________________________________________________________\n",
            "Epoch 1/60\n",
            "129/129 - 0s - loss: 1.9973 - mae: 1.0377\n",
            "Epoch 2/60\n",
            "129/129 - 0s - loss: 1.1290 - mae: 0.7735\n",
            "Epoch 3/60\n",
            "129/129 - 0s - loss: 0.9175 - mae: 0.6923\n",
            "Epoch 4/60\n",
            "129/129 - 0s - loss: 0.8091 - mae: 0.6461\n",
            "Epoch 5/60\n",
            "129/129 - 0s - loss: 0.7654 - mae: 0.6327\n",
            "Epoch 6/60\n",
            "129/129 - 0s - loss: 0.7079 - mae: 0.6157\n",
            "Epoch 7/60\n",
            "129/129 - 0s - loss: 0.7143 - mae: 0.6198\n",
            "Epoch 8/60\n",
            "129/129 - 0s - loss: 0.6866 - mae: 0.6117\n",
            "Epoch 9/60\n",
            "129/129 - 0s - loss: 0.6639 - mae: 0.6075\n",
            "Epoch 10/60\n",
            "129/129 - 0s - loss: 0.6466 - mae: 0.6019\n",
            "Epoch 11/60\n",
            "129/129 - 0s - loss: 0.6233 - mae: 0.5903\n",
            "Epoch 12/60\n",
            "129/129 - 0s - loss: 0.6183 - mae: 0.5887\n",
            "Epoch 13/60\n",
            "129/129 - 0s - loss: 0.6152 - mae: 0.5867\n",
            "Epoch 14/60\n",
            "129/129 - 0s - loss: 0.5917 - mae: 0.5784\n",
            "Epoch 15/60\n",
            "129/129 - 0s - loss: 0.5983 - mae: 0.5821\n",
            "Epoch 16/60\n",
            "129/129 - 0s - loss: 0.5848 - mae: 0.5707\n",
            "Epoch 17/60\n",
            "129/129 - 0s - loss: 0.5741 - mae: 0.5677\n",
            "Epoch 18/60\n",
            "129/129 - 0s - loss: 0.5600 - mae: 0.5615\n",
            "Epoch 19/60\n",
            "129/129 - 0s - loss: 0.5740 - mae: 0.5645\n",
            "Epoch 20/60\n",
            "129/129 - 0s - loss: 0.5602 - mae: 0.5556\n",
            "Epoch 21/60\n",
            "129/129 - 0s - loss: 0.5668 - mae: 0.5622\n",
            "Epoch 22/60\n",
            "129/129 - 0s - loss: 0.5568 - mae: 0.5594\n",
            "Epoch 23/60\n",
            "129/129 - 0s - loss: 0.5560 - mae: 0.5521\n",
            "Epoch 24/60\n",
            "129/129 - 0s - loss: 0.5620 - mae: 0.5604\n",
            "Epoch 25/60\n",
            "129/129 - 0s - loss: 0.5350 - mae: 0.5454\n",
            "Epoch 26/60\n",
            "129/129 - 0s - loss: 0.5329 - mae: 0.5426\n",
            "Epoch 27/60\n",
            "129/129 - 0s - loss: 0.5305 - mae: 0.5416\n",
            "Epoch 28/60\n",
            "129/129 - 0s - loss: 0.5211 - mae: 0.5367\n",
            "Epoch 29/60\n",
            "129/129 - 0s - loss: 0.5154 - mae: 0.5352\n",
            "Epoch 30/60\n",
            "129/129 - 0s - loss: 0.5324 - mae: 0.5404\n",
            "Epoch 31/60\n",
            "129/129 - 0s - loss: 0.5173 - mae: 0.5310\n",
            "Epoch 32/60\n",
            "129/129 - 0s - loss: 0.5165 - mae: 0.5328\n",
            "Epoch 33/60\n",
            "129/129 - 0s - loss: 0.5045 - mae: 0.5299\n",
            "Epoch 34/60\n",
            "129/129 - 0s - loss: 0.5148 - mae: 0.5342\n",
            "Epoch 35/60\n",
            "129/129 - 0s - loss: 0.4961 - mae: 0.5228\n",
            "Epoch 36/60\n",
            "129/129 - 0s - loss: 0.4992 - mae: 0.5251\n",
            "Epoch 37/60\n",
            "129/129 - 0s - loss: 0.4820 - mae: 0.5197\n",
            "Epoch 38/60\n",
            "129/129 - 0s - loss: 0.4984 - mae: 0.5215\n",
            "Epoch 39/60\n",
            "129/129 - 0s - loss: 0.4797 - mae: 0.5156\n",
            "Epoch 40/60\n",
            "129/129 - 0s - loss: 0.4925 - mae: 0.5176\n",
            "Epoch 41/60\n",
            "129/129 - 0s - loss: 0.4834 - mae: 0.5164\n",
            "Epoch 42/60\n",
            "129/129 - 0s - loss: 0.4869 - mae: 0.5193\n",
            "Epoch 43/60\n",
            "129/129 - 0s - loss: 0.4823 - mae: 0.5133\n",
            "Epoch 44/60\n",
            "129/129 - 0s - loss: 0.4712 - mae: 0.5098\n",
            "Epoch 45/60\n",
            "129/129 - 0s - loss: 0.4855 - mae: 0.5164\n",
            "Epoch 46/60\n",
            "129/129 - 0s - loss: 0.4682 - mae: 0.5094\n",
            "Epoch 47/60\n",
            "129/129 - 0s - loss: 0.4762 - mae: 0.5098\n",
            "Epoch 48/60\n",
            "129/129 - 0s - loss: 0.4661 - mae: 0.5062\n",
            "Epoch 49/60\n",
            "129/129 - 0s - loss: 0.4628 - mae: 0.5059\n",
            "Epoch 50/60\n",
            "129/129 - 0s - loss: 0.4525 - mae: 0.5011\n",
            "Epoch 51/60\n",
            "129/129 - 0s - loss: 0.4664 - mae: 0.5062\n",
            "Epoch 52/60\n",
            "129/129 - 0s - loss: 0.4504 - mae: 0.4972\n",
            "Epoch 53/60\n",
            "129/129 - 0s - loss: 0.4573 - mae: 0.4998\n",
            "Epoch 54/60\n",
            "129/129 - 0s - loss: 0.4498 - mae: 0.4964\n",
            "Epoch 55/60\n",
            "129/129 - 0s - loss: 0.4427 - mae: 0.4882\n",
            "Epoch 56/60\n",
            "129/129 - 0s - loss: 0.4493 - mae: 0.4977\n",
            "Epoch 57/60\n",
            "129/129 - 0s - loss: 0.4494 - mae: 0.4943\n",
            "Epoch 58/60\n",
            "129/129 - 0s - loss: 0.4486 - mae: 0.4961\n",
            "Epoch 59/60\n",
            "129/129 - 0s - loss: 0.4377 - mae: 0.4928\n",
            "Epoch 60/60\n",
            "129/129 - 0s - loss: 0.4506 - mae: 0.4982\n",
            "WARNING:tensorflow:6 out of the last 12 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fa1861a6048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "[0.0, 0.0, 0.0]\n",
            "[0.0, 0.0, 0.0]\n",
            "[1.1700364, 0.7961831, 0.533952]\n",
            "[1.0881633, 0.71965533, 0.60235614]\n",
            "[1.6271740198135376, 1.0301474928855896, 0.6967979669570923]\n",
            "(4167, 3, 1)\n",
            "(4167, 100, 2, 1)\n",
            "(4167, 100, 2, 1)\n",
            "Model: \"functional_1469\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_368 (InputLayer)       [(None, 100, 2, 1)]       0         \n",
            "_________________________________________________________________\n",
            "conv2d_1468 (Conv2D)         (None, 100, 2, 16)        112       \n",
            "_________________________________________________________________\n",
            "batch_normalization_1468 (Ba (None, 100, 2, 16)        64        \n",
            "_________________________________________________________________\n",
            "activation_1835 (Activation) (None, 100, 2, 16)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_367 (MaxPoolin (None, 50, 2, 16)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_1469 (Conv2D)         (None, 50, 2, 12)         1164      \n",
            "_________________________________________________________________\n",
            "batch_normalization_1469 (Ba (None, 50, 2, 12)         48        \n",
            "_________________________________________________________________\n",
            "up_sampling2d_367 (UpSamplin (None, 100, 2, 12)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1470 (Conv2D)         (None, 100, 2, 16)        1168      \n",
            "_________________________________________________________________\n",
            "batch_normalization_1470 (Ba (None, 100, 2, 16)        64        \n",
            "_________________________________________________________________\n",
            "activation_1837 (Activation) (None, 100, 2, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1471 (Conv2D)         (None, 100, 2, 1)         97        \n",
            "=================================================================\n",
            "Total params: 2,717\n",
            "Trainable params: 2,629\n",
            "Non-trainable params: 88\n",
            "_________________________________________________________________\n",
            "Epoch 1/30\n",
            "130/130 - 0s - loss: 0.0256 - mae: 0.0815\n",
            "Epoch 2/30\n",
            "130/130 - 0s - loss: 0.0108 - mae: 0.0553\n",
            "Epoch 3/30\n",
            "130/130 - 0s - loss: 0.0054 - mae: 0.0417\n",
            "Epoch 4/30\n",
            "130/130 - 0s - loss: 0.0047 - mae: 0.0430\n",
            "Epoch 5/30\n",
            "130/130 - 0s - loss: 0.0036 - mae: 0.0367\n",
            "Epoch 6/30\n",
            "130/130 - 0s - loss: 0.0033 - mae: 0.0350\n",
            "Epoch 7/30\n",
            "130/130 - 0s - loss: 0.0033 - mae: 0.0347\n",
            "Epoch 8/30\n",
            "130/130 - 0s - loss: 0.0025 - mae: 0.0296\n",
            "Epoch 9/30\n",
            "130/130 - 0s - loss: 0.0027 - mae: 0.0316\n",
            "Epoch 10/30\n",
            "130/130 - 0s - loss: 0.0025 - mae: 0.0286\n",
            "Epoch 11/30\n",
            "130/130 - 0s - loss: 0.0025 - mae: 0.0299\n",
            "Epoch 12/30\n",
            "130/130 - 0s - loss: 0.0028 - mae: 0.0320\n",
            "Epoch 13/30\n",
            "130/130 - 0s - loss: 0.0026 - mae: 0.0290\n",
            "Epoch 14/30\n",
            "130/130 - 0s - loss: 0.0023 - mae: 0.0292\n",
            "Epoch 15/30\n",
            "130/130 - 0s - loss: 0.0023 - mae: 0.0276\n",
            "Epoch 16/30\n",
            "130/130 - 0s - loss: 0.0024 - mae: 0.0286\n",
            "Epoch 17/30\n",
            "130/130 - 0s - loss: 0.0026 - mae: 0.0295\n",
            "Epoch 18/30\n",
            "130/130 - 0s - loss: 0.0020 - mae: 0.0265\n",
            "Epoch 19/30\n",
            "130/130 - 0s - loss: 0.0024 - mae: 0.0291\n",
            "Epoch 20/30\n",
            "130/130 - 0s - loss: 0.0021 - mae: 0.0267\n",
            "Epoch 21/30\n",
            "130/130 - 0s - loss: 0.0024 - mae: 0.0276\n",
            "Epoch 22/30\n",
            "130/130 - 0s - loss: 0.0021 - mae: 0.0265\n",
            "Epoch 23/30\n",
            "130/130 - 0s - loss: 0.0019 - mae: 0.0269\n",
            "Epoch 24/30\n",
            "130/130 - 0s - loss: 0.0016 - mae: 0.0238\n",
            "Epoch 25/30\n",
            "130/130 - 0s - loss: 0.0021 - mae: 0.0268\n",
            "Epoch 26/30\n",
            "130/130 - 0s - loss: 0.0016 - mae: 0.0237\n",
            "Epoch 27/30\n",
            "130/130 - 0s - loss: 0.0017 - mae: 0.0235\n",
            "Epoch 28/30\n",
            "130/130 - 0s - loss: 0.0018 - mae: 0.0254\n",
            "Epoch 29/30\n",
            "130/130 - 0s - loss: 0.0023 - mae: 0.0274\n",
            "Epoch 30/30\n",
            "130/130 - 0s - loss: 0.0017 - mae: 0.0245\n",
            "Model: \"functional_1471\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_368 (InputLayer)       [(None, 100, 2, 1)]       0         \n",
            "_________________________________________________________________\n",
            "conv2d_1468 (Conv2D)         (None, 100, 2, 16)        112       \n",
            "_________________________________________________________________\n",
            "batch_normalization_1468 (Ba (None, 100, 2, 16)        64        \n",
            "_________________________________________________________________\n",
            "activation_1835 (Activation) (None, 100, 2, 16)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_367 (MaxPoolin (None, 50, 2, 16)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_1469 (Conv2D)         (None, 50, 2, 12)         1164      \n",
            "_________________________________________________________________\n",
            "batch_normalization_1469 (Ba (None, 50, 2, 12)         48        \n",
            "_________________________________________________________________\n",
            "activation_1836 (Activation) (None, 50, 2, 12)         0         \n",
            "_________________________________________________________________\n",
            "flatten_367 (Flatten)        (None, 1200)              0         \n",
            "_________________________________________________________________\n",
            "dense_734 (Dense)            (None, 16)                19216     \n",
            "_________________________________________________________________\n",
            "batch_normalization_1471 (Ba (None, 16)                64        \n",
            "_________________________________________________________________\n",
            "activation_1838 (Activation) (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dropout_367 (Dropout)        (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_735 (Dense)            (None, 3)                 51        \n",
            "_________________________________________________________________\n",
            "activation_1839 (Activation) (None, 3)                 0         \n",
            "=================================================================\n",
            "Total params: 20,719\n",
            "Trainable params: 20,631\n",
            "Non-trainable params: 88\n",
            "_________________________________________________________________\n",
            "Epoch 1/60\n",
            "129/129 - 0s - loss: 2.0449 - mae: 1.0475\n",
            "Epoch 2/60\n",
            "129/129 - 0s - loss: 1.1708 - mae: 0.7845\n",
            "Epoch 3/60\n",
            "129/129 - 0s - loss: 0.9361 - mae: 0.6956\n",
            "Epoch 4/60\n",
            "129/129 - 0s - loss: 0.8385 - mae: 0.6601\n",
            "Epoch 5/60\n",
            "129/129 - 0s - loss: 0.7582 - mae: 0.6337\n",
            "Epoch 6/60\n",
            "129/129 - 0s - loss: 0.7053 - mae: 0.6138\n",
            "Epoch 7/60\n",
            "129/129 - 0s - loss: 0.6736 - mae: 0.6008\n",
            "Epoch 8/60\n",
            "129/129 - 0s - loss: 0.6504 - mae: 0.5939\n",
            "Epoch 9/60\n",
            "129/129 - 0s - loss: 0.6354 - mae: 0.5929\n",
            "Epoch 10/60\n",
            "129/129 - 0s - loss: 0.6332 - mae: 0.5925\n",
            "Epoch 11/60\n",
            "129/129 - 0s - loss: 0.6025 - mae: 0.5783\n",
            "Epoch 12/60\n",
            "129/129 - 0s - loss: 0.6075 - mae: 0.5841\n",
            "Epoch 13/60\n",
            "129/129 - 0s - loss: 0.6053 - mae: 0.5791\n",
            "Epoch 14/60\n",
            "129/129 - 0s - loss: 0.5700 - mae: 0.5668\n",
            "Epoch 15/60\n",
            "129/129 - 0s - loss: 0.5850 - mae: 0.5722\n",
            "Epoch 16/60\n",
            "129/129 - 0s - loss: 0.5831 - mae: 0.5721\n",
            "Epoch 17/60\n",
            "129/129 - 0s - loss: 0.5722 - mae: 0.5677\n",
            "Epoch 18/60\n",
            "129/129 - 0s - loss: 0.5737 - mae: 0.5649\n",
            "Epoch 19/60\n",
            "129/129 - 0s - loss: 0.5595 - mae: 0.5609\n",
            "Epoch 20/60\n",
            "129/129 - 0s - loss: 0.5533 - mae: 0.5563\n",
            "Epoch 21/60\n",
            "129/129 - 0s - loss: 0.5514 - mae: 0.5582\n",
            "Epoch 22/60\n",
            "129/129 - 0s - loss: 0.5475 - mae: 0.5539\n",
            "Epoch 23/60\n",
            "129/129 - 0s - loss: 0.5486 - mae: 0.5532\n",
            "Epoch 24/60\n",
            "129/129 - 0s - loss: 0.5275 - mae: 0.5429\n",
            "Epoch 25/60\n",
            "129/129 - 0s - loss: 0.5314 - mae: 0.5468\n",
            "Epoch 26/60\n",
            "129/129 - 0s - loss: 0.5159 - mae: 0.5389\n",
            "Epoch 27/60\n",
            "129/129 - 0s - loss: 0.5230 - mae: 0.5406\n",
            "Epoch 28/60\n",
            "129/129 - 0s - loss: 0.5086 - mae: 0.5326\n",
            "Epoch 29/60\n",
            "129/129 - 0s - loss: 0.5134 - mae: 0.5383\n",
            "Epoch 30/60\n",
            "129/129 - 0s - loss: 0.5229 - mae: 0.5390\n",
            "Epoch 31/60\n",
            "129/129 - 0s - loss: 0.4992 - mae: 0.5299\n",
            "Epoch 32/60\n",
            "129/129 - 0s - loss: 0.5076 - mae: 0.5361\n",
            "Epoch 33/60\n",
            "129/129 - 0s - loss: 0.4828 - mae: 0.5206\n",
            "Epoch 34/60\n",
            "129/129 - 0s - loss: 0.4844 - mae: 0.5197\n",
            "Epoch 35/60\n",
            "129/129 - 0s - loss: 0.4986 - mae: 0.5255\n",
            "Epoch 36/60\n",
            "129/129 - 0s - loss: 0.4972 - mae: 0.5285\n",
            "Epoch 37/60\n",
            "129/129 - 0s - loss: 0.4758 - mae: 0.5130\n",
            "Epoch 38/60\n",
            "129/129 - 0s - loss: 0.4515 - mae: 0.5027\n",
            "Epoch 39/60\n",
            "129/129 - 0s - loss: 0.4862 - mae: 0.5186\n",
            "Epoch 40/60\n",
            "129/129 - 0s - loss: 0.4659 - mae: 0.5099\n",
            "Epoch 41/60\n",
            "129/129 - 0s - loss: 0.4701 - mae: 0.5096\n",
            "Epoch 42/60\n",
            "129/129 - 0s - loss: 0.4524 - mae: 0.5034\n",
            "Epoch 43/60\n",
            "129/129 - 0s - loss: 0.4717 - mae: 0.5128\n",
            "Epoch 44/60\n",
            "129/129 - 0s - loss: 0.4687 - mae: 0.5089\n",
            "Epoch 45/60\n",
            "129/129 - 0s - loss: 0.4698 - mae: 0.5100\n",
            "Epoch 46/60\n",
            "129/129 - 0s - loss: 0.4507 - mae: 0.4980\n",
            "Epoch 47/60\n",
            "129/129 - 0s - loss: 0.4684 - mae: 0.5094\n",
            "Epoch 48/60\n",
            "129/129 - 0s - loss: 0.4627 - mae: 0.5041\n",
            "Epoch 49/60\n",
            "129/129 - 0s - loss: 0.4507 - mae: 0.4968\n",
            "Epoch 50/60\n",
            "129/129 - 0s - loss: 0.4505 - mae: 0.4969\n",
            "Epoch 51/60\n",
            "129/129 - 0s - loss: 0.4456 - mae: 0.4956\n",
            "Epoch 52/60\n",
            "129/129 - 0s - loss: 0.4578 - mae: 0.5000\n",
            "Epoch 53/60\n",
            "129/129 - 0s - loss: 0.4499 - mae: 0.4986\n",
            "Epoch 54/60\n",
            "129/129 - 0s - loss: 0.4343 - mae: 0.4871\n",
            "Epoch 55/60\n",
            "129/129 - 0s - loss: 0.4361 - mae: 0.4918\n",
            "Epoch 56/60\n",
            "129/129 - 0s - loss: 0.4285 - mae: 0.4857\n",
            "Epoch 57/60\n",
            "129/129 - 0s - loss: 0.4264 - mae: 0.4885\n",
            "Epoch 58/60\n",
            "129/129 - 0s - loss: 0.4252 - mae: 0.4827\n",
            "Epoch 59/60\n",
            "129/129 - 0s - loss: 0.4291 - mae: 0.4858\n",
            "Epoch 60/60\n",
            "129/129 - 0s - loss: 0.4230 - mae: 0.4825\n",
            "WARNING:tensorflow:7 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fa186604400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "[0.0, 3.0, 2.0]\n",
            "[0.0, 3.0, 2.0]\n",
            "[1.9880499, 2.2897196, 0.90478486]\n",
            "[1.972775, 2.3172183, 0.9170808]\n",
            "[2.238750994205475, 2.643244206905365, 0.9669373035430908]\n",
            "(4167, 3, 1)\n",
            "(4167, 100, 2, 1)\n",
            "(4167, 100, 2, 1)\n",
            "Model: \"functional_1473\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_369 (InputLayer)       [(None, 100, 2, 1)]       0         \n",
            "_________________________________________________________________\n",
            "conv2d_1472 (Conv2D)         (None, 100, 2, 16)        112       \n",
            "_________________________________________________________________\n",
            "batch_normalization_1472 (Ba (None, 100, 2, 16)        64        \n",
            "_________________________________________________________________\n",
            "activation_1840 (Activation) (None, 100, 2, 16)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_368 (MaxPoolin (None, 50, 2, 16)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_1473 (Conv2D)         (None, 50, 2, 12)         1164      \n",
            "_________________________________________________________________\n",
            "batch_normalization_1473 (Ba (None, 50, 2, 12)         48        \n",
            "_________________________________________________________________\n",
            "up_sampling2d_368 (UpSamplin (None, 100, 2, 12)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1474 (Conv2D)         (None, 100, 2, 16)        1168      \n",
            "_________________________________________________________________\n",
            "batch_normalization_1474 (Ba (None, 100, 2, 16)        64        \n",
            "_________________________________________________________________\n",
            "activation_1842 (Activation) (None, 100, 2, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1475 (Conv2D)         (None, 100, 2, 1)         97        \n",
            "=================================================================\n",
            "Total params: 2,717\n",
            "Trainable params: 2,629\n",
            "Non-trainable params: 88\n",
            "_________________________________________________________________\n",
            "Epoch 1/30\n",
            "130/130 - 0s - loss: 0.0281 - mae: 0.0836\n",
            "Epoch 2/30\n",
            "130/130 - 0s - loss: 0.0065 - mae: 0.0455\n",
            "Epoch 3/30\n",
            "130/130 - 0s - loss: 0.0061 - mae: 0.0469\n",
            "Epoch 4/30\n",
            "130/130 - 0s - loss: 0.0050 - mae: 0.0424\n",
            "Epoch 5/30\n",
            "130/130 - 0s - loss: 0.0043 - mae: 0.0402\n",
            "Epoch 6/30\n",
            "130/130 - 0s - loss: 0.0033 - mae: 0.0342\n",
            "Epoch 7/30\n",
            "130/130 - 0s - loss: 0.0040 - mae: 0.0371\n",
            "Epoch 8/30\n",
            "130/130 - 0s - loss: 0.0043 - mae: 0.0397\n",
            "Epoch 9/30\n",
            "130/130 - 0s - loss: 0.0035 - mae: 0.0355\n",
            "Epoch 10/30\n",
            "130/130 - 0s - loss: 0.0029 - mae: 0.0316\n",
            "Epoch 11/30\n",
            "130/130 - 0s - loss: 0.0029 - mae: 0.0314\n",
            "Epoch 12/30\n",
            "130/130 - 0s - loss: 0.0024 - mae: 0.0301\n",
            "Epoch 13/30\n",
            "130/130 - 0s - loss: 0.0028 - mae: 0.0306\n",
            "Epoch 14/30\n",
            "130/130 - 0s - loss: 0.0027 - mae: 0.0309\n",
            "Epoch 15/30\n",
            "130/130 - 0s - loss: 0.0025 - mae: 0.0295\n",
            "Epoch 16/30\n",
            "130/130 - 0s - loss: 0.0030 - mae: 0.0312\n",
            "Epoch 17/30\n",
            "130/130 - 0s - loss: 0.0027 - mae: 0.0311\n",
            "Epoch 18/30\n",
            "130/130 - 0s - loss: 0.0028 - mae: 0.0297\n",
            "Epoch 19/30\n",
            "130/130 - 0s - loss: 0.0029 - mae: 0.0315\n",
            "Epoch 20/30\n",
            "130/130 - 0s - loss: 0.0020 - mae: 0.0256\n",
            "Epoch 21/30\n",
            "130/130 - 0s - loss: 0.0019 - mae: 0.0255\n",
            "Epoch 22/30\n",
            "130/130 - 0s - loss: 0.0022 - mae: 0.0275\n",
            "Epoch 23/30\n",
            "130/130 - 0s - loss: 0.0020 - mae: 0.0260\n",
            "Epoch 24/30\n",
            "130/130 - 0s - loss: 0.0021 - mae: 0.0256\n",
            "Epoch 25/30\n",
            "130/130 - 0s - loss: 0.0019 - mae: 0.0254\n",
            "Epoch 26/30\n",
            "130/130 - 0s - loss: 0.0019 - mae: 0.0260\n",
            "Epoch 27/30\n",
            "130/130 - 0s - loss: 0.0018 - mae: 0.0243\n",
            "Epoch 28/30\n",
            "130/130 - 0s - loss: 0.0020 - mae: 0.0259\n",
            "Epoch 29/30\n",
            "130/130 - 0s - loss: 0.0018 - mae: 0.0251\n",
            "Epoch 30/30\n",
            "130/130 - 0s - loss: 0.0020 - mae: 0.0258\n",
            "Model: \"functional_1475\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_369 (InputLayer)       [(None, 100, 2, 1)]       0         \n",
            "_________________________________________________________________\n",
            "conv2d_1472 (Conv2D)         (None, 100, 2, 16)        112       \n",
            "_________________________________________________________________\n",
            "batch_normalization_1472 (Ba (None, 100, 2, 16)        64        \n",
            "_________________________________________________________________\n",
            "activation_1840 (Activation) (None, 100, 2, 16)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_368 (MaxPoolin (None, 50, 2, 16)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_1473 (Conv2D)         (None, 50, 2, 12)         1164      \n",
            "_________________________________________________________________\n",
            "batch_normalization_1473 (Ba (None, 50, 2, 12)         48        \n",
            "_________________________________________________________________\n",
            "activation_1841 (Activation) (None, 50, 2, 12)         0         \n",
            "_________________________________________________________________\n",
            "flatten_368 (Flatten)        (None, 1200)              0         \n",
            "_________________________________________________________________\n",
            "dense_736 (Dense)            (None, 16)                19216     \n",
            "_________________________________________________________________\n",
            "batch_normalization_1475 (Ba (None, 16)                64        \n",
            "_________________________________________________________________\n",
            "activation_1843 (Activation) (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dropout_368 (Dropout)        (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_737 (Dense)            (None, 3)                 51        \n",
            "_________________________________________________________________\n",
            "activation_1844 (Activation) (None, 3)                 0         \n",
            "=================================================================\n",
            "Total params: 20,719\n",
            "Trainable params: 20,631\n",
            "Non-trainable params: 88\n",
            "_________________________________________________________________\n",
            "Epoch 1/60\n",
            "129/129 - 0s - loss: 2.0172 - mae: 1.0499\n",
            "Epoch 2/60\n",
            "129/129 - 0s - loss: 1.1728 - mae: 0.7901\n",
            "Epoch 3/60\n",
            "129/129 - 0s - loss: 0.9329 - mae: 0.7043\n",
            "Epoch 4/60\n",
            "129/129 - 0s - loss: 0.8084 - mae: 0.6524\n",
            "Epoch 5/60\n",
            "129/129 - 0s - loss: 0.7579 - mae: 0.6338\n",
            "Epoch 6/60\n",
            "129/129 - 0s - loss: 0.7192 - mae: 0.6190\n",
            "Epoch 7/60\n",
            "129/129 - 0s - loss: 0.6983 - mae: 0.6128\n",
            "Epoch 8/60\n",
            "129/129 - 0s - loss: 0.6828 - mae: 0.6060\n",
            "Epoch 9/60\n",
            "129/129 - 0s - loss: 0.6681 - mae: 0.6033\n",
            "Epoch 10/60\n",
            "129/129 - 0s - loss: 0.6303 - mae: 0.5903\n",
            "Epoch 11/60\n",
            "129/129 - 0s - loss: 0.6250 - mae: 0.5887\n",
            "Epoch 12/60\n",
            "129/129 - 0s - loss: 0.6203 - mae: 0.5873\n",
            "Epoch 13/60\n",
            "129/129 - 0s - loss: 0.6040 - mae: 0.5794\n",
            "Epoch 14/60\n",
            "129/129 - 0s - loss: 0.6031 - mae: 0.5748\n",
            "Epoch 15/60\n",
            "129/129 - 0s - loss: 0.6053 - mae: 0.5795\n",
            "Epoch 16/60\n",
            "129/129 - 0s - loss: 0.5781 - mae: 0.5701\n",
            "Epoch 17/60\n",
            "129/129 - 0s - loss: 0.5838 - mae: 0.5733\n",
            "Epoch 18/60\n",
            "129/129 - 0s - loss: 0.5727 - mae: 0.5668\n",
            "Epoch 19/60\n",
            "129/129 - 0s - loss: 0.5731 - mae: 0.5659\n",
            "Epoch 20/60\n",
            "129/129 - 0s - loss: 0.5549 - mae: 0.5578\n",
            "Epoch 21/60\n",
            "129/129 - 0s - loss: 0.5538 - mae: 0.5575\n",
            "Epoch 22/60\n",
            "129/129 - 0s - loss: 0.5652 - mae: 0.5647\n",
            "Epoch 23/60\n",
            "129/129 - 0s - loss: 0.5555 - mae: 0.5547\n",
            "Epoch 24/60\n",
            "129/129 - 0s - loss: 0.5453 - mae: 0.5547\n",
            "Epoch 25/60\n",
            "129/129 - 0s - loss: 0.5425 - mae: 0.5519\n",
            "Epoch 26/60\n",
            "129/129 - 0s - loss: 0.5338 - mae: 0.5482\n",
            "Epoch 27/60\n",
            "129/129 - 0s - loss: 0.5286 - mae: 0.5428\n",
            "Epoch 28/60\n",
            "129/129 - 0s - loss: 0.5185 - mae: 0.5380\n",
            "Epoch 29/60\n",
            "129/129 - 0s - loss: 0.5001 - mae: 0.5268\n",
            "Epoch 30/60\n",
            "129/129 - 0s - loss: 0.5293 - mae: 0.5409\n",
            "Epoch 31/60\n",
            "129/129 - 0s - loss: 0.5064 - mae: 0.5326\n",
            "Epoch 32/60\n",
            "129/129 - 0s - loss: 0.5250 - mae: 0.5386\n",
            "Epoch 33/60\n",
            "129/129 - 0s - loss: 0.4991 - mae: 0.5300\n",
            "Epoch 34/60\n",
            "129/129 - 0s - loss: 0.4942 - mae: 0.5230\n",
            "Epoch 35/60\n",
            "129/129 - 0s - loss: 0.4978 - mae: 0.5228\n",
            "Epoch 36/60\n",
            "129/129 - 0s - loss: 0.5057 - mae: 0.5288\n",
            "Epoch 37/60\n",
            "129/129 - 0s - loss: 0.5045 - mae: 0.5293\n",
            "Epoch 38/60\n",
            "129/129 - 0s - loss: 0.4762 - mae: 0.5177\n",
            "Epoch 39/60\n",
            "129/129 - 0s - loss: 0.4890 - mae: 0.5240\n",
            "Epoch 40/60\n",
            "129/129 - 0s - loss: 0.4936 - mae: 0.5213\n",
            "Epoch 41/60\n",
            "129/129 - 0s - loss: 0.4917 - mae: 0.5202\n",
            "Epoch 42/60\n",
            "129/129 - 0s - loss: 0.4936 - mae: 0.5213\n",
            "Epoch 43/60\n",
            "129/129 - 0s - loss: 0.4660 - mae: 0.5088\n",
            "Epoch 44/60\n",
            "129/129 - 0s - loss: 0.4749 - mae: 0.5138\n",
            "Epoch 45/60\n",
            "129/129 - 0s - loss: 0.4658 - mae: 0.5063\n",
            "Epoch 46/60\n",
            "129/129 - 0s - loss: 0.4690 - mae: 0.5077\n",
            "Epoch 47/60\n",
            "129/129 - 0s - loss: 0.4632 - mae: 0.5031\n",
            "Epoch 48/60\n",
            "129/129 - 0s - loss: 0.4721 - mae: 0.5088\n",
            "Epoch 49/60\n",
            "129/129 - 0s - loss: 0.4522 - mae: 0.4996\n",
            "Epoch 50/60\n",
            "129/129 - 0s - loss: 0.4666 - mae: 0.5058\n",
            "Epoch 51/60\n",
            "129/129 - 0s - loss: 0.4608 - mae: 0.5021\n",
            "Epoch 52/60\n",
            "129/129 - 0s - loss: 0.4520 - mae: 0.4990\n",
            "Epoch 53/60\n",
            "129/129 - 0s - loss: 0.4551 - mae: 0.4982\n",
            "Epoch 54/60\n",
            "129/129 - 0s - loss: 0.4648 - mae: 0.5066\n",
            "Epoch 55/60\n",
            "129/129 - 0s - loss: 0.4556 - mae: 0.5006\n",
            "Epoch 56/60\n",
            "129/129 - 0s - loss: 0.4598 - mae: 0.5027\n",
            "Epoch 57/60\n",
            "129/129 - 0s - loss: 0.4419 - mae: 0.4925\n",
            "Epoch 58/60\n",
            "129/129 - 0s - loss: 0.4277 - mae: 0.4842\n",
            "Epoch 59/60\n",
            "129/129 - 0s - loss: 0.4494 - mae: 0.4975\n",
            "Epoch 60/60\n",
            "129/129 - 0s - loss: 0.4441 - mae: 0.4942\n",
            "WARNING:tensorflow:7 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fa1865a97b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "[0.0, 0.0, 0.0]\n",
            "[0.0, 0.0, 0.0]\n",
            "[0.59109676, 0.5746405, 0.46076602]\n",
            "[0.5993611, 0.56026024, 0.46878508]\n",
            "[0.8357707262039185, 0.7440535426139832, 0.557278037071228]\n",
            "(4167, 3, 1)\n",
            "(4167, 100, 2, 1)\n",
            "(4167, 100, 2, 1)\n",
            "Model: \"functional_1477\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_370 (InputLayer)       [(None, 100, 2, 1)]       0         \n",
            "_________________________________________________________________\n",
            "conv2d_1476 (Conv2D)         (None, 100, 2, 16)        112       \n",
            "_________________________________________________________________\n",
            "batch_normalization_1476 (Ba (None, 100, 2, 16)        64        \n",
            "_________________________________________________________________\n",
            "activation_1845 (Activation) (None, 100, 2, 16)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_369 (MaxPoolin (None, 50, 2, 16)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_1477 (Conv2D)         (None, 50, 2, 12)         1164      \n",
            "_________________________________________________________________\n",
            "batch_normalization_1477 (Ba (None, 50, 2, 12)         48        \n",
            "_________________________________________________________________\n",
            "up_sampling2d_369 (UpSamplin (None, 100, 2, 12)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1478 (Conv2D)         (None, 100, 2, 16)        1168      \n",
            "_________________________________________________________________\n",
            "batch_normalization_1478 (Ba (None, 100, 2, 16)        64        \n",
            "_________________________________________________________________\n",
            "activation_1847 (Activation) (None, 100, 2, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1479 (Conv2D)         (None, 100, 2, 1)         97        \n",
            "=================================================================\n",
            "Total params: 2,717\n",
            "Trainable params: 2,629\n",
            "Non-trainable params: 88\n",
            "_________________________________________________________________\n",
            "Epoch 1/30\n",
            "130/130 - 0s - loss: 0.0254 - mae: 0.0814\n",
            "Epoch 2/30\n",
            "130/130 - 0s - loss: 0.0066 - mae: 0.0457\n",
            "Epoch 3/30\n",
            "130/130 - 0s - loss: 0.0058 - mae: 0.0422\n",
            "Epoch 4/30\n",
            "130/130 - 0s - loss: 0.0047 - mae: 0.0405\n",
            "Epoch 5/30\n",
            "130/130 - 0s - loss: 0.0043 - mae: 0.0386\n",
            "Epoch 6/30\n",
            "130/130 - 0s - loss: 0.0033 - mae: 0.0342\n",
            "Epoch 7/30\n",
            "130/130 - 0s - loss: 0.0041 - mae: 0.0386\n",
            "Epoch 8/30\n",
            "130/130 - 0s - loss: 0.0043 - mae: 0.0359\n",
            "Epoch 9/30\n",
            "130/130 - 0s - loss: 0.0030 - mae: 0.0323\n",
            "Epoch 10/30\n",
            "130/130 - 0s - loss: 0.0026 - mae: 0.0306\n",
            "Epoch 11/30\n",
            "130/130 - 0s - loss: 0.0028 - mae: 0.0305\n",
            "Epoch 12/30\n",
            "130/130 - 0s - loss: 0.0024 - mae: 0.0284\n",
            "Epoch 13/30\n",
            "130/130 - 0s - loss: 0.0024 - mae: 0.0293\n",
            "Epoch 14/30\n",
            "130/130 - 0s - loss: 0.0023 - mae: 0.0291\n",
            "Epoch 15/30\n",
            "130/130 - 0s - loss: 0.0020 - mae: 0.0273\n",
            "Epoch 16/30\n",
            "130/130 - 0s - loss: 0.0022 - mae: 0.0289\n",
            "Epoch 17/30\n",
            "130/130 - 0s - loss: 0.0018 - mae: 0.0262\n",
            "Epoch 18/30\n",
            "130/130 - 0s - loss: 0.0024 - mae: 0.0295\n",
            "Epoch 19/30\n",
            "130/130 - 0s - loss: 0.0021 - mae: 0.0268\n",
            "Epoch 20/30\n",
            "130/130 - 0s - loss: 0.0028 - mae: 0.0314\n",
            "Epoch 21/30\n",
            "130/130 - 0s - loss: 0.0019 - mae: 0.0258\n",
            "Epoch 22/30\n",
            "130/130 - 0s - loss: 0.0025 - mae: 0.0297\n",
            "Epoch 23/30\n",
            "130/130 - 0s - loss: 0.0021 - mae: 0.0272\n",
            "Epoch 24/30\n",
            "130/130 - 0s - loss: 0.0019 - mae: 0.0256\n",
            "Epoch 25/30\n",
            "130/130 - 0s - loss: 0.0020 - mae: 0.0263\n",
            "Epoch 26/30\n",
            "130/130 - 0s - loss: 0.0024 - mae: 0.0287\n",
            "Epoch 27/30\n",
            "130/130 - 0s - loss: 0.0018 - mae: 0.0257\n",
            "Epoch 28/30\n",
            "130/130 - 0s - loss: 0.0023 - mae: 0.0281\n",
            "Epoch 29/30\n",
            "130/130 - 0s - loss: 0.0020 - mae: 0.0265\n",
            "Epoch 30/30\n",
            "130/130 - 0s - loss: 0.0018 - mae: 0.0244\n",
            "Model: \"functional_1479\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_370 (InputLayer)       [(None, 100, 2, 1)]       0         \n",
            "_________________________________________________________________\n",
            "conv2d_1476 (Conv2D)         (None, 100, 2, 16)        112       \n",
            "_________________________________________________________________\n",
            "batch_normalization_1476 (Ba (None, 100, 2, 16)        64        \n",
            "_________________________________________________________________\n",
            "activation_1845 (Activation) (None, 100, 2, 16)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_369 (MaxPoolin (None, 50, 2, 16)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_1477 (Conv2D)         (None, 50, 2, 12)         1164      \n",
            "_________________________________________________________________\n",
            "batch_normalization_1477 (Ba (None, 50, 2, 12)         48        \n",
            "_________________________________________________________________\n",
            "activation_1846 (Activation) (None, 50, 2, 12)         0         \n",
            "_________________________________________________________________\n",
            "flatten_369 (Flatten)        (None, 1200)              0         \n",
            "_________________________________________________________________\n",
            "dense_738 (Dense)            (None, 16)                19216     \n",
            "_________________________________________________________________\n",
            "batch_normalization_1479 (Ba (None, 16)                64        \n",
            "_________________________________________________________________\n",
            "activation_1848 (Activation) (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dropout_369 (Dropout)        (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_739 (Dense)            (None, 3)                 51        \n",
            "_________________________________________________________________\n",
            "activation_1849 (Activation) (None, 3)                 0         \n",
            "=================================================================\n",
            "Total params: 20,719\n",
            "Trainable params: 20,631\n",
            "Non-trainable params: 88\n",
            "_________________________________________________________________\n",
            "Epoch 1/60\n",
            "129/129 - 0s - loss: 1.9679 - mae: 1.0286\n",
            "Epoch 2/60\n",
            "129/129 - 0s - loss: 1.1333 - mae: 0.7676\n",
            "Epoch 3/60\n",
            "129/129 - 0s - loss: 0.9299 - mae: 0.6922\n",
            "Epoch 4/60\n",
            "129/129 - 0s - loss: 0.8396 - mae: 0.6538\n",
            "Epoch 5/60\n",
            "129/129 - 0s - loss: 0.7537 - mae: 0.6318\n",
            "Epoch 6/60\n",
            "129/129 - 0s - loss: 0.7251 - mae: 0.6175\n",
            "Epoch 7/60\n",
            "129/129 - 0s - loss: 0.7035 - mae: 0.6145\n",
            "Epoch 8/60\n",
            "129/129 - 0s - loss: 0.6822 - mae: 0.6080\n",
            "Epoch 9/60\n",
            "129/129 - 0s - loss: 0.6434 - mae: 0.5949\n",
            "Epoch 10/60\n",
            "129/129 - 0s - loss: 0.6180 - mae: 0.5870\n",
            "Epoch 11/60\n",
            "129/129 - 0s - loss: 0.6408 - mae: 0.5949\n",
            "Epoch 12/60\n",
            "129/129 - 0s - loss: 0.6258 - mae: 0.5900\n",
            "Epoch 13/60\n",
            "129/129 - 0s - loss: 0.6092 - mae: 0.5827\n",
            "Epoch 14/60\n",
            "129/129 - 0s - loss: 0.6174 - mae: 0.5873\n",
            "Epoch 15/60\n",
            "129/129 - 0s - loss: 0.5815 - mae: 0.5697\n",
            "Epoch 16/60\n",
            "129/129 - 0s - loss: 0.6095 - mae: 0.5784\n",
            "Epoch 17/60\n",
            "129/129 - 0s - loss: 0.5711 - mae: 0.5630\n",
            "Epoch 18/60\n",
            "129/129 - 0s - loss: 0.5775 - mae: 0.5650\n",
            "Epoch 19/60\n",
            "129/129 - 0s - loss: 0.5716 - mae: 0.5603\n",
            "Epoch 20/60\n",
            "129/129 - 0s - loss: 0.5661 - mae: 0.5557\n",
            "Epoch 21/60\n",
            "129/129 - 0s - loss: 0.5595 - mae: 0.5563\n",
            "Epoch 22/60\n",
            "129/129 - 0s - loss: 0.5420 - mae: 0.5509\n",
            "Epoch 23/60\n",
            "129/129 - 0s - loss: 0.5576 - mae: 0.5553\n",
            "Epoch 24/60\n",
            "129/129 - 0s - loss: 0.5307 - mae: 0.5401\n",
            "Epoch 25/60\n",
            "129/129 - 0s - loss: 0.5305 - mae: 0.5426\n",
            "Epoch 26/60\n",
            "129/129 - 0s - loss: 0.5281 - mae: 0.5398\n",
            "Epoch 27/60\n",
            "129/129 - 0s - loss: 0.5299 - mae: 0.5403\n",
            "Epoch 28/60\n",
            "129/129 - 0s - loss: 0.5356 - mae: 0.5430\n",
            "Epoch 29/60\n",
            "129/129 - 0s - loss: 0.5142 - mae: 0.5341\n",
            "Epoch 30/60\n",
            "129/129 - 0s - loss: 0.5214 - mae: 0.5330\n",
            "Epoch 31/60\n",
            "129/129 - 0s - loss: 0.4968 - mae: 0.5242\n",
            "Epoch 32/60\n",
            "129/129 - 0s - loss: 0.4901 - mae: 0.5178\n",
            "Epoch 33/60\n",
            "129/129 - 0s - loss: 0.5015 - mae: 0.5287\n",
            "Epoch 34/60\n",
            "129/129 - 0s - loss: 0.5018 - mae: 0.5261\n",
            "Epoch 35/60\n",
            "129/129 - 0s - loss: 0.4873 - mae: 0.5209\n",
            "Epoch 36/60\n",
            "129/129 - 0s - loss: 0.4921 - mae: 0.5194\n",
            "Epoch 37/60\n",
            "129/129 - 0s - loss: 0.5105 - mae: 0.5274\n",
            "Epoch 38/60\n",
            "129/129 - 0s - loss: 0.4881 - mae: 0.5188\n",
            "Epoch 39/60\n",
            "129/129 - 0s - loss: 0.4839 - mae: 0.5144\n",
            "Epoch 40/60\n",
            "129/129 - 0s - loss: 0.4854 - mae: 0.5179\n",
            "Epoch 41/60\n",
            "129/129 - 0s - loss: 0.4954 - mae: 0.5203\n",
            "Epoch 42/60\n",
            "129/129 - 0s - loss: 0.4671 - mae: 0.5020\n",
            "Epoch 43/60\n",
            "129/129 - 0s - loss: 0.4691 - mae: 0.5047\n",
            "Epoch 44/60\n",
            "129/129 - 0s - loss: 0.4560 - mae: 0.4995\n",
            "Epoch 45/60\n",
            "129/129 - 0s - loss: 0.4767 - mae: 0.5124\n",
            "Epoch 46/60\n",
            "129/129 - 0s - loss: 0.4608 - mae: 0.5034\n",
            "Epoch 47/60\n",
            "129/129 - 0s - loss: 0.4693 - mae: 0.5046\n",
            "Epoch 48/60\n",
            "129/129 - 0s - loss: 0.4559 - mae: 0.4977\n",
            "Epoch 49/60\n",
            "129/129 - 0s - loss: 0.4567 - mae: 0.4990\n",
            "Epoch 50/60\n",
            "129/129 - 0s - loss: 0.4562 - mae: 0.4961\n",
            "Epoch 51/60\n",
            "129/129 - 0s - loss: 0.4519 - mae: 0.4963\n",
            "Epoch 52/60\n",
            "129/129 - 0s - loss: 0.4680 - mae: 0.5055\n",
            "Epoch 53/60\n",
            "129/129 - 0s - loss: 0.4408 - mae: 0.4890\n",
            "Epoch 54/60\n",
            "129/129 - 0s - loss: 0.4341 - mae: 0.4911\n",
            "Epoch 55/60\n",
            "129/129 - 0s - loss: 0.4700 - mae: 0.5020\n",
            "Epoch 56/60\n",
            "129/129 - 0s - loss: 0.4456 - mae: 0.4908\n",
            "Epoch 57/60\n",
            "129/129 - 0s - loss: 0.4593 - mae: 0.4969\n",
            "Epoch 58/60\n",
            "129/129 - 0s - loss: 0.4456 - mae: 0.4908\n",
            "Epoch 59/60\n",
            "129/129 - 0s - loss: 0.4645 - mae: 0.4963\n",
            "Epoch 60/60\n",
            "129/129 - 0s - loss: 0.4427 - mae: 0.4863\n",
            "WARNING:tensorflow:8 out of the last 12 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fa1866558c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "[1.0, 1.0, 0.0]\n",
            "[1.0, 1.0, 0.0]\n",
            "[0.8852789, 0.97299725, 0.627847]\n",
            "[0.68733966, 0.8470671, 0.6180681]\n",
            "[1.1997328400611877, 1.283591240644455, 0.7657033056020737]\n",
            "(4167, 3, 1)\n",
            "(4167, 100, 2, 1)\n",
            "(4167, 100, 2, 1)\n",
            "Model: \"functional_1481\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_371 (InputLayer)       [(None, 100, 2, 1)]       0         \n",
            "_________________________________________________________________\n",
            "conv2d_1480 (Conv2D)         (None, 100, 2, 16)        112       \n",
            "_________________________________________________________________\n",
            "batch_normalization_1480 (Ba (None, 100, 2, 16)        64        \n",
            "_________________________________________________________________\n",
            "activation_1850 (Activation) (None, 100, 2, 16)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_370 (MaxPoolin (None, 50, 2, 16)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_1481 (Conv2D)         (None, 50, 2, 12)         1164      \n",
            "_________________________________________________________________\n",
            "batch_normalization_1481 (Ba (None, 50, 2, 12)         48        \n",
            "_________________________________________________________________\n",
            "up_sampling2d_370 (UpSamplin (None, 100, 2, 12)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1482 (Conv2D)         (None, 100, 2, 16)        1168      \n",
            "_________________________________________________________________\n",
            "batch_normalization_1482 (Ba (None, 100, 2, 16)        64        \n",
            "_________________________________________________________________\n",
            "activation_1852 (Activation) (None, 100, 2, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1483 (Conv2D)         (None, 100, 2, 1)         97        \n",
            "=================================================================\n",
            "Total params: 2,717\n",
            "Trainable params: 2,629\n",
            "Non-trainable params: 88\n",
            "_________________________________________________________________\n",
            "Epoch 1/30\n",
            "130/130 - 0s - loss: 0.0264 - mae: 0.0798\n",
            "Epoch 2/30\n",
            "130/130 - 0s - loss: 0.0073 - mae: 0.0490\n",
            "Epoch 3/30\n",
            "130/130 - 0s - loss: 0.0057 - mae: 0.0445\n",
            "Epoch 4/30\n",
            "130/130 - 0s - loss: 0.0039 - mae: 0.0365\n",
            "Epoch 5/30\n",
            "130/130 - 0s - loss: 0.0042 - mae: 0.0397\n",
            "Epoch 6/30\n",
            "130/130 - 0s - loss: 0.0049 - mae: 0.0387\n",
            "Epoch 7/30\n",
            "130/130 - 0s - loss: 0.0034 - mae: 0.0343\n",
            "Epoch 8/30\n",
            "130/130 - 0s - loss: 0.0034 - mae: 0.0350\n",
            "Epoch 9/30\n",
            "130/130 - 0s - loss: 0.0025 - mae: 0.0296\n",
            "Epoch 10/30\n",
            "130/130 - 0s - loss: 0.0022 - mae: 0.0277\n",
            "Epoch 11/30\n",
            "130/130 - 0s - loss: 0.0029 - mae: 0.0337\n",
            "Epoch 12/30\n",
            "130/130 - 0s - loss: 0.0022 - mae: 0.0283\n",
            "Epoch 13/30\n",
            "130/130 - 0s - loss: 0.0025 - mae: 0.0303\n",
            "Epoch 14/30\n",
            "130/130 - 0s - loss: 0.0025 - mae: 0.0305\n",
            "Epoch 15/30\n",
            "130/130 - 0s - loss: 0.0026 - mae: 0.0303\n",
            "Epoch 16/30\n",
            "130/130 - 0s - loss: 0.0021 - mae: 0.0270\n",
            "Epoch 17/30\n",
            "130/130 - 0s - loss: 0.0021 - mae: 0.0269\n",
            "Epoch 18/30\n",
            "130/130 - 0s - loss: 0.0021 - mae: 0.0265\n",
            "Epoch 19/30\n",
            "130/130 - 0s - loss: 0.0020 - mae: 0.0264\n",
            "Epoch 20/30\n",
            "130/130 - 0s - loss: 0.0018 - mae: 0.0253\n",
            "Epoch 21/30\n",
            "130/130 - 0s - loss: 0.0019 - mae: 0.0256\n",
            "Epoch 22/30\n",
            "130/130 - 0s - loss: 0.0019 - mae: 0.0257\n",
            "Epoch 23/30\n",
            "130/130 - 0s - loss: 0.0021 - mae: 0.0262\n",
            "Epoch 24/30\n",
            "130/130 - 0s - loss: 0.0019 - mae: 0.0269\n",
            "Epoch 25/30\n",
            "130/130 - 0s - loss: 0.0023 - mae: 0.0285\n",
            "Epoch 26/30\n",
            "130/130 - 0s - loss: 0.0018 - mae: 0.0257\n",
            "Epoch 27/30\n",
            "130/130 - 0s - loss: 0.0016 - mae: 0.0232\n",
            "Epoch 28/30\n",
            "130/130 - 0s - loss: 0.0023 - mae: 0.0282\n",
            "Epoch 29/30\n",
            "130/130 - 0s - loss: 0.0017 - mae: 0.0239\n",
            "Epoch 30/30\n",
            "130/130 - 0s - loss: 0.0019 - mae: 0.0260\n",
            "Model: \"functional_1483\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_371 (InputLayer)       [(None, 100, 2, 1)]       0         \n",
            "_________________________________________________________________\n",
            "conv2d_1480 (Conv2D)         (None, 100, 2, 16)        112       \n",
            "_________________________________________________________________\n",
            "batch_normalization_1480 (Ba (None, 100, 2, 16)        64        \n",
            "_________________________________________________________________\n",
            "activation_1850 (Activation) (None, 100, 2, 16)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_370 (MaxPoolin (None, 50, 2, 16)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_1481 (Conv2D)         (None, 50, 2, 12)         1164      \n",
            "_________________________________________________________________\n",
            "batch_normalization_1481 (Ba (None, 50, 2, 12)         48        \n",
            "_________________________________________________________________\n",
            "activation_1851 (Activation) (None, 50, 2, 12)         0         \n",
            "_________________________________________________________________\n",
            "flatten_370 (Flatten)        (None, 1200)              0         \n",
            "_________________________________________________________________\n",
            "dense_740 (Dense)            (None, 16)                19216     \n",
            "_________________________________________________________________\n",
            "batch_normalization_1483 (Ba (None, 16)                64        \n",
            "_________________________________________________________________\n",
            "activation_1853 (Activation) (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dropout_370 (Dropout)        (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_741 (Dense)            (None, 3)                 51        \n",
            "_________________________________________________________________\n",
            "activation_1854 (Activation) (None, 3)                 0         \n",
            "=================================================================\n",
            "Total params: 20,719\n",
            "Trainable params: 20,631\n",
            "Non-trainable params: 88\n",
            "_________________________________________________________________\n",
            "Epoch 1/60\n",
            "129/129 - 0s - loss: 2.0245 - mae: 1.0470\n",
            "Epoch 2/60\n",
            "129/129 - 0s - loss: 1.1635 - mae: 0.7853\n",
            "Epoch 3/60\n",
            "129/129 - 0s - loss: 0.9378 - mae: 0.7058\n",
            "Epoch 4/60\n",
            "129/129 - 0s - loss: 0.8274 - mae: 0.6600\n",
            "Epoch 5/60\n",
            "129/129 - 0s - loss: 0.7717 - mae: 0.6359\n",
            "Epoch 6/60\n",
            "129/129 - 0s - loss: 0.7104 - mae: 0.6161\n",
            "Epoch 7/60\n",
            "129/129 - 0s - loss: 0.6893 - mae: 0.6094\n",
            "Epoch 8/60\n",
            "129/129 - 0s - loss: 0.6758 - mae: 0.6077\n",
            "Epoch 9/60\n",
            "129/129 - 0s - loss: 0.6590 - mae: 0.5959\n",
            "Epoch 10/60\n",
            "129/129 - 0s - loss: 0.6408 - mae: 0.5915\n",
            "Epoch 11/60\n",
            "129/129 - 0s - loss: 0.6268 - mae: 0.5846\n",
            "Epoch 12/60\n",
            "129/129 - 0s - loss: 0.6209 - mae: 0.5842\n",
            "Epoch 13/60\n",
            "129/129 - 0s - loss: 0.6141 - mae: 0.5797\n",
            "Epoch 14/60\n",
            "129/129 - 0s - loss: 0.5850 - mae: 0.5688\n",
            "Epoch 15/60\n",
            "129/129 - 0s - loss: 0.6015 - mae: 0.5775\n",
            "Epoch 16/60\n",
            "129/129 - 0s - loss: 0.5693 - mae: 0.5618\n",
            "Epoch 17/60\n",
            "129/129 - 0s - loss: 0.5699 - mae: 0.5608\n",
            "Epoch 18/60\n",
            "129/129 - 0s - loss: 0.5706 - mae: 0.5623\n",
            "Epoch 19/60\n",
            "129/129 - 0s - loss: 0.5761 - mae: 0.5632\n",
            "Epoch 20/60\n",
            "129/129 - 0s - loss: 0.5498 - mae: 0.5552\n",
            "Epoch 21/60\n",
            "129/129 - 0s - loss: 0.5382 - mae: 0.5488\n",
            "Epoch 22/60\n",
            "129/129 - 0s - loss: 0.5454 - mae: 0.5484\n",
            "Epoch 23/60\n",
            "129/129 - 0s - loss: 0.5360 - mae: 0.5432\n",
            "Epoch 24/60\n",
            "129/129 - 0s - loss: 0.5535 - mae: 0.5538\n",
            "Epoch 25/60\n",
            "129/129 - 0s - loss: 0.5263 - mae: 0.5424\n",
            "Epoch 26/60\n",
            "129/129 - 0s - loss: 0.5302 - mae: 0.5420\n",
            "Epoch 27/60\n",
            "129/129 - 0s - loss: 0.5185 - mae: 0.5368\n",
            "Epoch 28/60\n",
            "129/129 - 0s - loss: 0.4987 - mae: 0.5285\n",
            "Epoch 29/60\n",
            "129/129 - 0s - loss: 0.5177 - mae: 0.5349\n",
            "Epoch 30/60\n",
            "129/129 - 0s - loss: 0.5250 - mae: 0.5356\n",
            "Epoch 31/60\n",
            "129/129 - 0s - loss: 0.5110 - mae: 0.5304\n",
            "Epoch 32/60\n",
            "129/129 - 0s - loss: 0.4995 - mae: 0.5260\n",
            "Epoch 33/60\n",
            "129/129 - 0s - loss: 0.4829 - mae: 0.5174\n",
            "Epoch 34/60\n",
            "129/129 - 0s - loss: 0.4931 - mae: 0.5198\n",
            "Epoch 35/60\n",
            "129/129 - 0s - loss: 0.4943 - mae: 0.5184\n",
            "Epoch 36/60\n",
            "129/129 - 0s - loss: 0.4924 - mae: 0.5197\n",
            "Epoch 37/60\n",
            "129/129 - 0s - loss: 0.4795 - mae: 0.5167\n",
            "Epoch 38/60\n",
            "129/129 - 0s - loss: 0.4843 - mae: 0.5181\n",
            "Epoch 39/60\n",
            "129/129 - 0s - loss: 0.4858 - mae: 0.5195\n",
            "Epoch 40/60\n",
            "129/129 - 0s - loss: 0.4748 - mae: 0.5140\n",
            "Epoch 41/60\n",
            "129/129 - 0s - loss: 0.4894 - mae: 0.5201\n",
            "Epoch 42/60\n",
            "129/129 - 0s - loss: 0.4799 - mae: 0.5160\n",
            "Epoch 43/60\n",
            "129/129 - 0s - loss: 0.4616 - mae: 0.5050\n",
            "Epoch 44/60\n",
            "129/129 - 0s - loss: 0.4581 - mae: 0.5039\n",
            "Epoch 45/60\n",
            "129/129 - 0s - loss: 0.4806 - mae: 0.5161\n",
            "Epoch 46/60\n",
            "129/129 - 0s - loss: 0.4676 - mae: 0.5081\n",
            "Epoch 47/60\n",
            "129/129 - 0s - loss: 0.4801 - mae: 0.5107\n",
            "Epoch 48/60\n",
            "129/129 - 0s - loss: 0.4886 - mae: 0.5172\n",
            "Epoch 49/60\n",
            "129/129 - 0s - loss: 0.4524 - mae: 0.5021\n",
            "Epoch 50/60\n",
            "129/129 - 0s - loss: 0.4706 - mae: 0.5081\n",
            "Epoch 51/60\n",
            "129/129 - 0s - loss: 0.4600 - mae: 0.5041\n",
            "Epoch 52/60\n",
            "129/129 - 0s - loss: 0.4587 - mae: 0.5032\n",
            "Epoch 53/60\n",
            "129/129 - 0s - loss: 0.4530 - mae: 0.5015\n",
            "Epoch 54/60\n",
            "129/129 - 0s - loss: 0.4669 - mae: 0.5077\n",
            "Epoch 55/60\n",
            "129/129 - 0s - loss: 0.4652 - mae: 0.5054\n",
            "Epoch 56/60\n",
            "129/129 - 0s - loss: 0.4412 - mae: 0.4969\n",
            "Epoch 57/60\n",
            "129/129 - 0s - loss: 0.4550 - mae: 0.5025\n",
            "Epoch 58/60\n",
            "129/129 - 0s - loss: 0.4341 - mae: 0.4889\n",
            "Epoch 59/60\n",
            "129/129 - 0s - loss: 0.4378 - mae: 0.4931\n",
            "Epoch 60/60\n",
            "129/129 - 0s - loss: 0.4476 - mae: 0.4950\n",
            "WARNING:tensorflow:9 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fa185f7e7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "[0.0, 0.0, 1.0]\n",
            "[0.0, 0.0, 1.0]\n",
            "[1.5108901, 1.588386, 0.742811]\n",
            "[1.4136806, 1.4713018, 0.76244617]\n",
            "[2.0514429211616516, 2.045310854911804, 0.8903369307518005]\n",
            "(4167, 3, 1)\n",
            "(4167, 100, 2, 1)\n",
            "(4167, 100, 2, 1)\n",
            "Model: \"functional_1485\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_372 (InputLayer)       [(None, 100, 2, 1)]       0         \n",
            "_________________________________________________________________\n",
            "conv2d_1484 (Conv2D)         (None, 100, 2, 16)        112       \n",
            "_________________________________________________________________\n",
            "batch_normalization_1484 (Ba (None, 100, 2, 16)        64        \n",
            "_________________________________________________________________\n",
            "activation_1855 (Activation) (None, 100, 2, 16)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_371 (MaxPoolin (None, 50, 2, 16)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_1485 (Conv2D)         (None, 50, 2, 12)         1164      \n",
            "_________________________________________________________________\n",
            "batch_normalization_1485 (Ba (None, 50, 2, 12)         48        \n",
            "_________________________________________________________________\n",
            "up_sampling2d_371 (UpSamplin (None, 100, 2, 12)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1486 (Conv2D)         (None, 100, 2, 16)        1168      \n",
            "_________________________________________________________________\n",
            "batch_normalization_1486 (Ba (None, 100, 2, 16)        64        \n",
            "_________________________________________________________________\n",
            "activation_1857 (Activation) (None, 100, 2, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1487 (Conv2D)         (None, 100, 2, 1)         97        \n",
            "=================================================================\n",
            "Total params: 2,717\n",
            "Trainable params: 2,629\n",
            "Non-trainable params: 88\n",
            "_________________________________________________________________\n",
            "Epoch 1/30\n",
            "130/130 - 0s - loss: 0.0282 - mae: 0.0869\n",
            "Epoch 2/30\n",
            "130/130 - 0s - loss: 0.0060 - mae: 0.0453\n",
            "Epoch 3/30\n",
            "130/130 - 0s - loss: 0.0050 - mae: 0.0415\n",
            "Epoch 4/30\n",
            "130/130 - 0s - loss: 0.0082 - mae: 0.0502\n",
            "Epoch 5/30\n",
            "130/130 - 0s - loss: 0.0042 - mae: 0.0390\n",
            "Epoch 6/30\n",
            "130/130 - 0s - loss: 0.0032 - mae: 0.0346\n",
            "Epoch 7/30\n",
            "130/130 - 0s - loss: 0.0033 - mae: 0.0342\n",
            "Epoch 8/30\n",
            "130/130 - 0s - loss: 0.0027 - mae: 0.0319\n",
            "Epoch 9/30\n",
            "130/130 - 0s - loss: 0.0027 - mae: 0.0313\n",
            "Epoch 10/30\n",
            "130/130 - 0s - loss: 0.0032 - mae: 0.0347\n",
            "Epoch 11/30\n",
            "130/130 - 0s - loss: 0.0027 - mae: 0.0317\n",
            "Epoch 12/30\n",
            "130/130 - 0s - loss: 0.0024 - mae: 0.0289\n",
            "Epoch 13/30\n",
            "130/130 - 0s - loss: 0.0022 - mae: 0.0279\n",
            "Epoch 14/30\n",
            "130/130 - 0s - loss: 0.0026 - mae: 0.0303\n",
            "Epoch 15/30\n",
            "130/130 - 0s - loss: 0.0021 - mae: 0.0277\n",
            "Epoch 16/30\n",
            "130/130 - 0s - loss: 0.0023 - mae: 0.0287\n",
            "Epoch 17/30\n",
            "130/130 - 0s - loss: 0.0021 - mae: 0.0277\n",
            "Epoch 18/30\n",
            "130/130 - 0s - loss: 0.0023 - mae: 0.0279\n",
            "Epoch 19/30\n",
            "130/130 - 0s - loss: 0.0021 - mae: 0.0269\n",
            "Epoch 20/30\n",
            "130/130 - 0s - loss: 0.0022 - mae: 0.0279\n",
            "Epoch 21/30\n",
            "130/130 - 0s - loss: 0.0018 - mae: 0.0257\n",
            "Epoch 22/30\n",
            "130/130 - 0s - loss: 0.0023 - mae: 0.0280\n",
            "Epoch 23/30\n",
            "130/130 - 0s - loss: 0.0021 - mae: 0.0261\n",
            "Epoch 24/30\n",
            "130/130 - 0s - loss: 0.0021 - mae: 0.0270\n",
            "Epoch 25/30\n",
            "130/130 - 0s - loss: 0.0021 - mae: 0.0274\n",
            "Epoch 26/30\n",
            "130/130 - 0s - loss: 0.0018 - mae: 0.0249\n",
            "Epoch 27/30\n",
            "130/130 - 0s - loss: 0.0021 - mae: 0.0266\n",
            "Epoch 28/30\n",
            "130/130 - 0s - loss: 0.0021 - mae: 0.0263\n",
            "Epoch 29/30\n",
            "130/130 - 0s - loss: 0.0019 - mae: 0.0255\n",
            "Epoch 30/30\n",
            "130/130 - 0s - loss: 0.0018 - mae: 0.0257\n",
            "Model: \"functional_1487\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_372 (InputLayer)       [(None, 100, 2, 1)]       0         \n",
            "_________________________________________________________________\n",
            "conv2d_1484 (Conv2D)         (None, 100, 2, 16)        112       \n",
            "_________________________________________________________________\n",
            "batch_normalization_1484 (Ba (None, 100, 2, 16)        64        \n",
            "_________________________________________________________________\n",
            "activation_1855 (Activation) (None, 100, 2, 16)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_371 (MaxPoolin (None, 50, 2, 16)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_1485 (Conv2D)         (None, 50, 2, 12)         1164      \n",
            "_________________________________________________________________\n",
            "batch_normalization_1485 (Ba (None, 50, 2, 12)         48        \n",
            "_________________________________________________________________\n",
            "activation_1856 (Activation) (None, 50, 2, 12)         0         \n",
            "_________________________________________________________________\n",
            "flatten_371 (Flatten)        (None, 1200)              0         \n",
            "_________________________________________________________________\n",
            "dense_742 (Dense)            (None, 16)                19216     \n",
            "_________________________________________________________________\n",
            "batch_normalization_1487 (Ba (None, 16)                64        \n",
            "_________________________________________________________________\n",
            "activation_1858 (Activation) (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dropout_371 (Dropout)        (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_743 (Dense)            (None, 3)                 51        \n",
            "_________________________________________________________________\n",
            "activation_1859 (Activation) (None, 3)                 0         \n",
            "=================================================================\n",
            "Total params: 20,719\n",
            "Trainable params: 20,631\n",
            "Non-trainable params: 88\n",
            "_________________________________________________________________\n",
            "Epoch 1/60\n",
            "129/129 - 0s - loss: 1.9296 - mae: 1.0223\n",
            "Epoch 2/60\n",
            "129/129 - 0s - loss: 1.0995 - mae: 0.7611\n",
            "Epoch 3/60\n",
            "129/129 - 0s - loss: 0.8991 - mae: 0.6833\n",
            "Epoch 4/60\n",
            "129/129 - 0s - loss: 0.8016 - mae: 0.6455\n",
            "Epoch 5/60\n",
            "129/129 - 0s - loss: 0.7479 - mae: 0.6296\n",
            "Epoch 6/60\n",
            "129/129 - 0s - loss: 0.7203 - mae: 0.6164\n",
            "Epoch 7/60\n",
            "129/129 - 0s - loss: 0.6745 - mae: 0.6057\n",
            "Epoch 8/60\n",
            "129/129 - 0s - loss: 0.6579 - mae: 0.5986\n",
            "Epoch 9/60\n",
            "129/129 - 0s - loss: 0.6463 - mae: 0.5947\n",
            "Epoch 10/60\n",
            "129/129 - 0s - loss: 0.6385 - mae: 0.5929\n",
            "Epoch 11/60\n",
            "129/129 - 0s - loss: 0.6105 - mae: 0.5850\n",
            "Epoch 12/60\n",
            "129/129 - 0s - loss: 0.6163 - mae: 0.5812\n",
            "Epoch 13/60\n",
            "129/129 - 0s - loss: 0.6018 - mae: 0.5759\n",
            "Epoch 14/60\n",
            "129/129 - 0s - loss: 0.5905 - mae: 0.5733\n",
            "Epoch 15/60\n",
            "129/129 - 0s - loss: 0.5796 - mae: 0.5698\n",
            "Epoch 16/60\n",
            "129/129 - 0s - loss: 0.5724 - mae: 0.5658\n",
            "Epoch 17/60\n",
            "129/129 - 0s - loss: 0.5609 - mae: 0.5562\n",
            "Epoch 18/60\n",
            "129/129 - 0s - loss: 0.5645 - mae: 0.5600\n",
            "Epoch 19/60\n",
            "129/129 - 0s - loss: 0.5572 - mae: 0.5551\n",
            "Epoch 20/60\n",
            "129/129 - 0s - loss: 0.5736 - mae: 0.5602\n",
            "Epoch 21/60\n",
            "129/129 - 0s - loss: 0.5260 - mae: 0.5412\n",
            "Epoch 22/60\n",
            "129/129 - 0s - loss: 0.5551 - mae: 0.5531\n",
            "Epoch 23/60\n",
            "129/129 - 0s - loss: 0.5258 - mae: 0.5412\n",
            "Epoch 24/60\n",
            "129/129 - 0s - loss: 0.5370 - mae: 0.5439\n",
            "Epoch 25/60\n",
            "129/129 - 0s - loss: 0.5279 - mae: 0.5406\n",
            "Epoch 26/60\n",
            "129/129 - 0s - loss: 0.5152 - mae: 0.5344\n",
            "Epoch 27/60\n",
            "129/129 - 0s - loss: 0.5247 - mae: 0.5373\n",
            "Epoch 28/60\n",
            "129/129 - 0s - loss: 0.5130 - mae: 0.5304\n",
            "Epoch 29/60\n",
            "129/129 - 0s - loss: 0.5130 - mae: 0.5330\n",
            "Epoch 30/60\n",
            "129/129 - 0s - loss: 0.4895 - mae: 0.5174\n",
            "Epoch 31/60\n",
            "129/129 - 0s - loss: 0.5103 - mae: 0.5300\n",
            "Epoch 32/60\n",
            "129/129 - 0s - loss: 0.5001 - mae: 0.5263\n",
            "Epoch 33/60\n",
            "129/129 - 0s - loss: 0.5027 - mae: 0.5243\n",
            "Epoch 34/60\n",
            "129/129 - 0s - loss: 0.5015 - mae: 0.5210\n",
            "Epoch 35/60\n",
            "129/129 - 0s - loss: 0.4869 - mae: 0.5133\n",
            "Epoch 36/60\n",
            "129/129 - 0s - loss: 0.4850 - mae: 0.5150\n",
            "Epoch 37/60\n",
            "129/129 - 0s - loss: 0.4865 - mae: 0.5146\n",
            "Epoch 38/60\n",
            "129/129 - 0s - loss: 0.4837 - mae: 0.5143\n",
            "Epoch 39/60\n",
            "129/129 - 0s - loss: 0.4774 - mae: 0.5130\n",
            "Epoch 40/60\n",
            "129/129 - 0s - loss: 0.4783 - mae: 0.5119\n",
            "Epoch 41/60\n",
            "129/129 - 0s - loss: 0.4728 - mae: 0.5093\n",
            "Epoch 42/60\n",
            "129/129 - 0s - loss: 0.4726 - mae: 0.5103\n",
            "Epoch 43/60\n",
            "129/129 - 0s - loss: 0.4749 - mae: 0.5075\n",
            "Epoch 44/60\n",
            "129/129 - 0s - loss: 0.4694 - mae: 0.5047\n",
            "Epoch 45/60\n",
            "129/129 - 0s - loss: 0.4557 - mae: 0.5016\n",
            "Epoch 46/60\n",
            "129/129 - 0s - loss: 0.4674 - mae: 0.5010\n",
            "Epoch 47/60\n",
            "129/129 - 0s - loss: 0.4613 - mae: 0.5008\n",
            "Epoch 48/60\n",
            "129/129 - 0s - loss: 0.4633 - mae: 0.5036\n",
            "Epoch 49/60\n",
            "129/129 - 0s - loss: 0.4692 - mae: 0.5042\n",
            "Epoch 50/60\n",
            "129/129 - 0s - loss: 0.4683 - mae: 0.5067\n",
            "Epoch 51/60\n",
            "129/129 - 0s - loss: 0.4571 - mae: 0.5025\n",
            "Epoch 52/60\n",
            "129/129 - 0s - loss: 0.4442 - mae: 0.4924\n",
            "Epoch 53/60\n",
            "129/129 - 0s - loss: 0.4396 - mae: 0.4889\n",
            "Epoch 54/60\n",
            "129/129 - 0s - loss: 0.4341 - mae: 0.4882\n",
            "Epoch 55/60\n",
            "129/129 - 0s - loss: 0.4408 - mae: 0.4878\n",
            "Epoch 56/60\n",
            "129/129 - 0s - loss: 0.4501 - mae: 0.4936\n",
            "Epoch 57/60\n",
            "129/129 - 0s - loss: 0.4404 - mae: 0.4901\n",
            "Epoch 58/60\n",
            "129/129 - 0s - loss: 0.4481 - mae: 0.4915\n",
            "Epoch 59/60\n",
            "129/129 - 0s - loss: 0.4360 - mae: 0.4856\n",
            "Epoch 60/60\n",
            "129/129 - 0s - loss: 0.4524 - mae: 0.4926\n",
            "WARNING:tensorflow:9 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fa185e241e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "[4.0, 4.0, 1.0]\n",
            "[4.0, 4.0, 1.0]\n",
            "[1.8368062, 1.8268626, 0.8629347]\n",
            "[1.4855404, 1.6701791, 0.8753487]\n",
            "[2.3054003715515137, 2.099419593811035, 0.9931408762931824]\n",
            "(4167, 3, 1)\n",
            "(4167, 100, 2, 1)\n",
            "(4167, 100, 2, 1)\n",
            "Model: \"functional_1489\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_373 (InputLayer)       [(None, 100, 2, 1)]       0         \n",
            "_________________________________________________________________\n",
            "conv2d_1488 (Conv2D)         (None, 100, 2, 16)        112       \n",
            "_________________________________________________________________\n",
            "batch_normalization_1488 (Ba (None, 100, 2, 16)        64        \n",
            "_________________________________________________________________\n",
            "activation_1860 (Activation) (None, 100, 2, 16)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_372 (MaxPoolin (None, 50, 2, 16)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_1489 (Conv2D)         (None, 50, 2, 12)         1164      \n",
            "_________________________________________________________________\n",
            "batch_normalization_1489 (Ba (None, 50, 2, 12)         48        \n",
            "_________________________________________________________________\n",
            "up_sampling2d_372 (UpSamplin (None, 100, 2, 12)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1490 (Conv2D)         (None, 100, 2, 16)        1168      \n",
            "_________________________________________________________________\n",
            "batch_normalization_1490 (Ba (None, 100, 2, 16)        64        \n",
            "_________________________________________________________________\n",
            "activation_1862 (Activation) (None, 100, 2, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1491 (Conv2D)         (None, 100, 2, 1)         97        \n",
            "=================================================================\n",
            "Total params: 2,717\n",
            "Trainable params: 2,629\n",
            "Non-trainable params: 88\n",
            "_________________________________________________________________\n",
            "Epoch 1/30\n",
            "130/130 - 0s - loss: 0.0272 - mae: 0.0827\n",
            "Epoch 2/30\n",
            "130/130 - 0s - loss: 0.0075 - mae: 0.0501\n",
            "Epoch 3/30\n",
            "130/130 - 0s - loss: 0.0055 - mae: 0.0434\n",
            "Epoch 4/30\n",
            "130/130 - 0s - loss: 0.0051 - mae: 0.0429\n",
            "Epoch 5/30\n",
            "130/130 - 0s - loss: 0.0047 - mae: 0.0432\n",
            "Epoch 6/30\n",
            "130/130 - 0s - loss: 0.0030 - mae: 0.0326\n",
            "Epoch 7/30\n",
            "130/130 - 0s - loss: 0.0035 - mae: 0.0365\n",
            "Epoch 8/30\n",
            "130/130 - 0s - loss: 0.0044 - mae: 0.0405\n",
            "Epoch 9/30\n",
            "130/130 - 0s - loss: 0.0031 - mae: 0.0327\n",
            "Epoch 10/30\n",
            "130/130 - 0s - loss: 0.0028 - mae: 0.0312\n",
            "Epoch 11/30\n",
            "130/130 - 0s - loss: 0.0025 - mae: 0.0293\n",
            "Epoch 12/30\n",
            "130/130 - 0s - loss: 0.0024 - mae: 0.0290\n",
            "Epoch 13/30\n",
            "130/130 - 0s - loss: 0.0026 - mae: 0.0302\n",
            "Epoch 14/30\n",
            "130/130 - 0s - loss: 0.0029 - mae: 0.0300\n",
            "Epoch 15/30\n",
            "130/130 - 0s - loss: 0.0024 - mae: 0.0288\n",
            "Epoch 16/30\n",
            "130/130 - 0s - loss: 0.0024 - mae: 0.0292\n",
            "Epoch 17/30\n",
            "130/130 - 0s - loss: 0.0018 - mae: 0.0250\n",
            "Epoch 18/30\n",
            "130/130 - 0s - loss: 0.0021 - mae: 0.0279\n",
            "Epoch 19/30\n",
            "130/130 - 0s - loss: 0.0024 - mae: 0.0288\n",
            "Epoch 20/30\n",
            "130/130 - 0s - loss: 0.0020 - mae: 0.0256\n",
            "Epoch 21/30\n",
            "130/130 - 0s - loss: 0.0018 - mae: 0.0246\n",
            "Epoch 22/30\n",
            "130/130 - 0s - loss: 0.0021 - mae: 0.0271\n",
            "Epoch 23/30\n",
            "130/130 - 0s - loss: 0.0022 - mae: 0.0279\n",
            "Epoch 24/30\n",
            "130/130 - 0s - loss: 0.0018 - mae: 0.0251\n",
            "Epoch 25/30\n",
            "130/130 - 0s - loss: 0.0015 - mae: 0.0230\n",
            "Epoch 26/30\n",
            "130/130 - 0s - loss: 0.0021 - mae: 0.0269\n",
            "Epoch 27/30\n",
            "130/130 - 0s - loss: 0.0017 - mae: 0.0239\n",
            "Epoch 28/30\n",
            "130/130 - 0s - loss: 0.0022 - mae: 0.0272\n",
            "Epoch 29/30\n",
            "130/130 - 0s - loss: 0.0018 - mae: 0.0250\n",
            "Epoch 30/30\n",
            "130/130 - 0s - loss: 0.0019 - mae: 0.0258\n",
            "Model: \"functional_1491\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_373 (InputLayer)       [(None, 100, 2, 1)]       0         \n",
            "_________________________________________________________________\n",
            "conv2d_1488 (Conv2D)         (None, 100, 2, 16)        112       \n",
            "_________________________________________________________________\n",
            "batch_normalization_1488 (Ba (None, 100, 2, 16)        64        \n",
            "_________________________________________________________________\n",
            "activation_1860 (Activation) (None, 100, 2, 16)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_372 (MaxPoolin (None, 50, 2, 16)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_1489 (Conv2D)         (None, 50, 2, 12)         1164      \n",
            "_________________________________________________________________\n",
            "batch_normalization_1489 (Ba (None, 50, 2, 12)         48        \n",
            "_________________________________________________________________\n",
            "activation_1861 (Activation) (None, 50, 2, 12)         0         \n",
            "_________________________________________________________________\n",
            "flatten_372 (Flatten)        (None, 1200)              0         \n",
            "_________________________________________________________________\n",
            "dense_744 (Dense)            (None, 16)                19216     \n",
            "_________________________________________________________________\n",
            "batch_normalization_1491 (Ba (None, 16)                64        \n",
            "_________________________________________________________________\n",
            "activation_1863 (Activation) (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dropout_372 (Dropout)        (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_745 (Dense)            (None, 3)                 51        \n",
            "_________________________________________________________________\n",
            "activation_1864 (Activation) (None, 3)                 0         \n",
            "=================================================================\n",
            "Total params: 20,719\n",
            "Trainable params: 20,631\n",
            "Non-trainable params: 88\n",
            "_________________________________________________________________\n",
            "Epoch 1/60\n",
            "130/130 - 0s - loss: 1.9936 - mae: 1.0336\n",
            "Epoch 2/60\n",
            "130/130 - 0s - loss: 1.1168 - mae: 0.7678\n",
            "Epoch 3/60\n",
            "130/130 - 0s - loss: 0.9314 - mae: 0.6899\n",
            "Epoch 4/60\n",
            "130/130 - 0s - loss: 0.8184 - mae: 0.6504\n",
            "Epoch 5/60\n",
            "130/130 - 0s - loss: 0.7650 - mae: 0.6378\n",
            "Epoch 6/60\n",
            "130/130 - 0s - loss: 0.7098 - mae: 0.6173\n",
            "Epoch 7/60\n",
            "130/130 - 0s - loss: 0.7015 - mae: 0.6196\n",
            "Epoch 8/60\n",
            "130/130 - 0s - loss: 0.6778 - mae: 0.6099\n",
            "Epoch 9/60\n",
            "130/130 - 0s - loss: 0.6470 - mae: 0.6005\n",
            "Epoch 10/60\n",
            "130/130 - 0s - loss: 0.6432 - mae: 0.6025\n",
            "Epoch 11/60\n",
            "130/130 - 0s - loss: 0.6233 - mae: 0.5952\n",
            "Epoch 12/60\n",
            "130/130 - 0s - loss: 0.6225 - mae: 0.5953\n",
            "Epoch 13/60\n",
            "130/130 - 0s - loss: 0.6220 - mae: 0.5952\n",
            "Epoch 14/60\n",
            "130/130 - 0s - loss: 0.5976 - mae: 0.5840\n",
            "Epoch 15/60\n",
            "130/130 - 0s - loss: 0.6016 - mae: 0.5831\n",
            "Epoch 16/60\n",
            "130/130 - 0s - loss: 0.5996 - mae: 0.5830\n",
            "Epoch 17/60\n",
            "130/130 - 0s - loss: 0.5906 - mae: 0.5796\n",
            "Epoch 18/60\n",
            "130/130 - 0s - loss: 0.5967 - mae: 0.5740\n",
            "Epoch 19/60\n",
            "130/130 - 0s - loss: 0.5868 - mae: 0.5749\n",
            "Epoch 20/60\n",
            "130/130 - 0s - loss: 0.5850 - mae: 0.5749\n",
            "Epoch 21/60\n",
            "130/130 - 0s - loss: 0.5626 - mae: 0.5618\n",
            "Epoch 22/60\n",
            "130/130 - 0s - loss: 0.5758 - mae: 0.5648\n",
            "Epoch 23/60\n",
            "130/130 - 0s - loss: 0.5542 - mae: 0.5567\n",
            "Epoch 24/60\n",
            "130/130 - 0s - loss: 0.5569 - mae: 0.5556\n",
            "Epoch 25/60\n",
            "130/130 - 0s - loss: 0.5362 - mae: 0.5486\n",
            "Epoch 26/60\n",
            "130/130 - 0s - loss: 0.5416 - mae: 0.5518\n",
            "Epoch 27/60\n",
            "130/130 - 0s - loss: 0.5264 - mae: 0.5427\n",
            "Epoch 28/60\n",
            "130/130 - 0s - loss: 0.5294 - mae: 0.5440\n",
            "Epoch 29/60\n",
            "130/130 - 0s - loss: 0.5377 - mae: 0.5504\n",
            "Epoch 30/60\n",
            "130/130 - 0s - loss: 0.5275 - mae: 0.5387\n",
            "Epoch 31/60\n",
            "130/130 - 0s - loss: 0.5242 - mae: 0.5403\n",
            "Epoch 32/60\n",
            "130/130 - 0s - loss: 0.5163 - mae: 0.5383\n",
            "Epoch 33/60\n",
            "130/130 - 0s - loss: 0.5167 - mae: 0.5357\n",
            "Epoch 34/60\n",
            "130/130 - 0s - loss: 0.5058 - mae: 0.5327\n",
            "Epoch 35/60\n",
            "130/130 - 0s - loss: 0.5098 - mae: 0.5340\n",
            "Epoch 36/60\n",
            "130/130 - 0s - loss: 0.5022 - mae: 0.5283\n",
            "Epoch 37/60\n",
            "130/130 - 0s - loss: 0.4874 - mae: 0.5204\n",
            "Epoch 38/60\n",
            "130/130 - 0s - loss: 0.5055 - mae: 0.5278\n",
            "Epoch 39/60\n",
            "130/130 - 0s - loss: 0.4831 - mae: 0.5187\n",
            "Epoch 40/60\n",
            "130/130 - 0s - loss: 0.4874 - mae: 0.5184\n",
            "Epoch 41/60\n",
            "130/130 - 0s - loss: 0.4727 - mae: 0.5126\n",
            "Epoch 42/60\n",
            "130/130 - 0s - loss: 0.4792 - mae: 0.5157\n",
            "Epoch 43/60\n",
            "130/130 - 0s - loss: 0.4751 - mae: 0.5128\n",
            "Epoch 44/60\n",
            "130/130 - 0s - loss: 0.4840 - mae: 0.5163\n",
            "Epoch 45/60\n",
            "130/130 - 0s - loss: 0.4623 - mae: 0.5062\n",
            "Epoch 46/60\n",
            "130/130 - 0s - loss: 0.4783 - mae: 0.5139\n",
            "Epoch 47/60\n",
            "130/130 - 0s - loss: 0.4634 - mae: 0.5054\n",
            "Epoch 48/60\n",
            "130/130 - 0s - loss: 0.4707 - mae: 0.5109\n",
            "Epoch 49/60\n",
            "130/130 - 0s - loss: 0.4570 - mae: 0.5053\n",
            "Epoch 50/60\n",
            "130/130 - 0s - loss: 0.4588 - mae: 0.5045\n",
            "Epoch 51/60\n",
            "130/130 - 0s - loss: 0.4635 - mae: 0.5050\n",
            "Epoch 52/60\n",
            "130/130 - 0s - loss: 0.4683 - mae: 0.5066\n",
            "Epoch 53/60\n",
            "130/130 - 0s - loss: 0.4617 - mae: 0.5027\n",
            "Epoch 54/60\n",
            "130/130 - 0s - loss: 0.4475 - mae: 0.4956\n",
            "Epoch 55/60\n",
            "130/130 - 0s - loss: 0.4459 - mae: 0.4957\n",
            "Epoch 56/60\n",
            "130/130 - 0s - loss: 0.4452 - mae: 0.4919\n",
            "Epoch 57/60\n",
            "130/130 - 0s - loss: 0.4573 - mae: 0.4981\n",
            "Epoch 58/60\n",
            "130/130 - 0s - loss: 0.4475 - mae: 0.4921\n",
            "Epoch 59/60\n",
            "130/130 - 0s - loss: 0.4462 - mae: 0.4947\n",
            "Epoch 60/60\n",
            "130/130 - 0s - loss: 0.4357 - mae: 0.4859\n",
            "WARNING:tensorflow:10 out of the last 12 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fa1874de048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "[0.0, 0.0, 0.0]\n",
            "[0.0, 0.0, 0.0]\n",
            "[0.71818954, 0.67990077, 0.54312277]\n",
            "[0.6043531, 0.6452893, 0.5710634]\n",
            "[0.9583122134208679, 0.774230569601059, 0.6110691130161285]\n",
            "(4167, 3, 1)\n",
            "(4167, 100, 2, 1)\n",
            "(4167, 100, 2, 1)\n",
            "Model: \"functional_1493\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_374 (InputLayer)       [(None, 100, 2, 1)]       0         \n",
            "_________________________________________________________________\n",
            "conv2d_1492 (Conv2D)         (None, 100, 2, 16)        112       \n",
            "_________________________________________________________________\n",
            "batch_normalization_1492 (Ba (None, 100, 2, 16)        64        \n",
            "_________________________________________________________________\n",
            "activation_1865 (Activation) (None, 100, 2, 16)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_373 (MaxPoolin (None, 50, 2, 16)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_1493 (Conv2D)         (None, 50, 2, 12)         1164      \n",
            "_________________________________________________________________\n",
            "batch_normalization_1493 (Ba (None, 50, 2, 12)         48        \n",
            "_________________________________________________________________\n",
            "up_sampling2d_373 (UpSamplin (None, 100, 2, 12)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1494 (Conv2D)         (None, 100, 2, 16)        1168      \n",
            "_________________________________________________________________\n",
            "batch_normalization_1494 (Ba (None, 100, 2, 16)        64        \n",
            "_________________________________________________________________\n",
            "activation_1867 (Activation) (None, 100, 2, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1495 (Conv2D)         (None, 100, 2, 1)         97        \n",
            "=================================================================\n",
            "Total params: 2,717\n",
            "Trainable params: 2,629\n",
            "Non-trainable params: 88\n",
            "_________________________________________________________________\n",
            "Epoch 1/30\n",
            "131/131 - 0s - loss: 0.0274 - mae: 0.0856\n",
            "Epoch 2/30\n",
            "131/131 - 0s - loss: 0.0067 - mae: 0.0470\n",
            "Epoch 3/30\n",
            "131/131 - 0s - loss: 0.0058 - mae: 0.0430\n",
            "Epoch 4/30\n",
            "131/131 - 0s - loss: 0.0045 - mae: 0.0395\n",
            "Epoch 5/30\n",
            "131/131 - 0s - loss: 0.0040 - mae: 0.0374\n",
            "Epoch 6/30\n",
            "131/131 - 0s - loss: 0.0040 - mae: 0.0390\n",
            "Epoch 7/30\n",
            "131/131 - 0s - loss: 0.0047 - mae: 0.0398\n",
            "Epoch 8/30\n",
            "131/131 - 0s - loss: 0.0035 - mae: 0.0349\n",
            "Epoch 9/30\n",
            "131/131 - 0s - loss: 0.0027 - mae: 0.0320\n",
            "Epoch 10/30\n",
            "131/131 - 0s - loss: 0.0029 - mae: 0.0336\n",
            "Epoch 11/30\n",
            "131/131 - 0s - loss: 0.0024 - mae: 0.0298\n",
            "Epoch 12/30\n",
            "131/131 - 0s - loss: 0.0026 - mae: 0.0310\n",
            "Epoch 13/30\n",
            "131/131 - 0s - loss: 0.0023 - mae: 0.0288\n",
            "Epoch 14/30\n",
            "131/131 - 0s - loss: 0.0031 - mae: 0.0335\n",
            "Epoch 15/30\n",
            "131/131 - 0s - loss: 0.0026 - mae: 0.0305\n",
            "Epoch 16/30\n",
            "131/131 - 0s - loss: 0.0025 - mae: 0.0303\n",
            "Epoch 17/30\n",
            "131/131 - 0s - loss: 0.0028 - mae: 0.0311\n",
            "Epoch 18/30\n",
            "131/131 - 0s - loss: 0.0019 - mae: 0.0260\n",
            "Epoch 19/30\n",
            "131/131 - 0s - loss: 0.0021 - mae: 0.0276\n",
            "Epoch 20/30\n",
            "131/131 - 0s - loss: 0.0021 - mae: 0.0279\n",
            "Epoch 21/30\n",
            "131/131 - 0s - loss: 0.0018 - mae: 0.0250\n",
            "Epoch 22/30\n",
            "131/131 - 0s - loss: 0.0023 - mae: 0.0273\n",
            "Epoch 23/30\n",
            "131/131 - 0s - loss: 0.0019 - mae: 0.0254\n",
            "Epoch 24/30\n",
            "131/131 - 0s - loss: 0.0021 - mae: 0.0282\n",
            "Epoch 25/30\n",
            "131/131 - 0s - loss: 0.0019 - mae: 0.0256\n",
            "Epoch 26/30\n",
            "131/131 - 0s - loss: 0.0018 - mae: 0.0249\n",
            "Epoch 27/30\n",
            "131/131 - 0s - loss: 0.0021 - mae: 0.0275\n",
            "Epoch 28/30\n",
            "131/131 - 0s - loss: 0.0020 - mae: 0.0260\n",
            "Epoch 29/30\n",
            "131/131 - 0s - loss: 0.0016 - mae: 0.0235\n",
            "Epoch 30/30\n",
            "131/131 - 0s - loss: 0.0015 - mae: 0.0232\n",
            "Model: \"functional_1495\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_374 (InputLayer)       [(None, 100, 2, 1)]       0         \n",
            "_________________________________________________________________\n",
            "conv2d_1492 (Conv2D)         (None, 100, 2, 16)        112       \n",
            "_________________________________________________________________\n",
            "batch_normalization_1492 (Ba (None, 100, 2, 16)        64        \n",
            "_________________________________________________________________\n",
            "activation_1865 (Activation) (None, 100, 2, 16)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_373 (MaxPoolin (None, 50, 2, 16)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_1493 (Conv2D)         (None, 50, 2, 12)         1164      \n",
            "_________________________________________________________________\n",
            "batch_normalization_1493 (Ba (None, 50, 2, 12)         48        \n",
            "_________________________________________________________________\n",
            "activation_1866 (Activation) (None, 50, 2, 12)         0         \n",
            "_________________________________________________________________\n",
            "flatten_373 (Flatten)        (None, 1200)              0         \n",
            "_________________________________________________________________\n",
            "dense_746 (Dense)            (None, 16)                19216     \n",
            "_________________________________________________________________\n",
            "batch_normalization_1495 (Ba (None, 16)                64        \n",
            "_________________________________________________________________\n",
            "activation_1868 (Activation) (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dropout_373 (Dropout)        (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_747 (Dense)            (None, 3)                 51        \n",
            "_________________________________________________________________\n",
            "activation_1869 (Activation) (None, 3)                 0         \n",
            "=================================================================\n",
            "Total params: 20,719\n",
            "Trainable params: 20,631\n",
            "Non-trainable params: 88\n",
            "_________________________________________________________________\n",
            "Epoch 1/60\n",
            "130/130 - 0s - loss: 2.0438 - mae: 1.0530\n",
            "Epoch 2/60\n",
            "130/130 - 0s - loss: 1.1333 - mae: 0.7788\n",
            "Epoch 3/60\n",
            "130/130 - 0s - loss: 0.9212 - mae: 0.6971\n",
            "Epoch 4/60\n",
            "130/130 - 0s - loss: 0.8128 - mae: 0.6576\n",
            "Epoch 5/60\n",
            "130/130 - 0s - loss: 0.7547 - mae: 0.6332\n",
            "Epoch 6/60\n",
            "130/130 - 0s - loss: 0.7205 - mae: 0.6168\n",
            "Epoch 7/60\n",
            "130/130 - 0s - loss: 0.6804 - mae: 0.6067\n",
            "Epoch 8/60\n",
            "130/130 - 0s - loss: 0.6886 - mae: 0.6092\n",
            "Epoch 9/60\n",
            "130/130 - 0s - loss: 0.6545 - mae: 0.5994\n",
            "Epoch 10/60\n",
            "130/130 - 0s - loss: 0.6518 - mae: 0.5987\n",
            "Epoch 11/60\n",
            "130/130 - 0s - loss: 0.6459 - mae: 0.5945\n",
            "Epoch 12/60\n",
            "130/130 - 0s - loss: 0.6271 - mae: 0.5857\n",
            "Epoch 13/60\n",
            "130/130 - 0s - loss: 0.6237 - mae: 0.5875\n",
            "Epoch 14/60\n",
            "130/130 - 0s - loss: 0.5971 - mae: 0.5744\n",
            "Epoch 15/60\n",
            "130/130 - 0s - loss: 0.6053 - mae: 0.5790\n",
            "Epoch 16/60\n",
            "130/130 - 0s - loss: 0.5969 - mae: 0.5728\n",
            "Epoch 17/60\n",
            "130/130 - 0s - loss: 0.5973 - mae: 0.5746\n",
            "Epoch 18/60\n",
            "130/130 - 0s - loss: 0.5705 - mae: 0.5638\n",
            "Epoch 19/60\n",
            "130/130 - 0s - loss: 0.5733 - mae: 0.5637\n",
            "Epoch 20/60\n",
            "130/130 - 0s - loss: 0.5770 - mae: 0.5637\n",
            "Epoch 21/60\n",
            "130/130 - 0s - loss: 0.5585 - mae: 0.5555\n",
            "Epoch 22/60\n",
            "130/130 - 0s - loss: 0.5713 - mae: 0.5627\n",
            "Epoch 23/60\n",
            "130/130 - 0s - loss: 0.5546 - mae: 0.5514\n",
            "Epoch 24/60\n",
            "130/130 - 0s - loss: 0.5628 - mae: 0.5628\n",
            "Epoch 25/60\n",
            "130/130 - 0s - loss: 0.5634 - mae: 0.5564\n",
            "Epoch 26/60\n",
            "130/130 - 0s - loss: 0.5633 - mae: 0.5579\n",
            "Epoch 27/60\n",
            "130/130 - 0s - loss: 0.5304 - mae: 0.5439\n",
            "Epoch 28/60\n",
            "130/130 - 0s - loss: 0.5462 - mae: 0.5479\n",
            "Epoch 29/60\n",
            "130/130 - 0s - loss: 0.5320 - mae: 0.5411\n",
            "Epoch 30/60\n",
            "130/130 - 0s - loss: 0.5355 - mae: 0.5423\n",
            "Epoch 31/60\n",
            "130/130 - 0s - loss: 0.5211 - mae: 0.5350\n",
            "Epoch 32/60\n",
            "130/130 - 0s - loss: 0.5275 - mae: 0.5405\n",
            "Epoch 33/60\n",
            "130/130 - 0s - loss: 0.5205 - mae: 0.5331\n",
            "Epoch 34/60\n",
            "130/130 - 0s - loss: 0.5114 - mae: 0.5316\n",
            "Epoch 35/60\n",
            "130/130 - 0s - loss: 0.5217 - mae: 0.5368\n",
            "Epoch 36/60\n",
            "130/130 - 0s - loss: 0.5018 - mae: 0.5242\n",
            "Epoch 37/60\n",
            "130/130 - 0s - loss: 0.5073 - mae: 0.5286\n",
            "Epoch 38/60\n",
            "130/130 - 0s - loss: 0.5053 - mae: 0.5269\n",
            "Epoch 39/60\n",
            "130/130 - 0s - loss: 0.4825 - mae: 0.5182\n",
            "Epoch 40/60\n",
            "130/130 - 0s - loss: 0.4817 - mae: 0.5157\n",
            "Epoch 41/60\n",
            "130/130 - 0s - loss: 0.4882 - mae: 0.5182\n",
            "Epoch 42/60\n",
            "130/130 - 0s - loss: 0.4987 - mae: 0.5215\n",
            "Epoch 43/60\n",
            "130/130 - 0s - loss: 0.4775 - mae: 0.5157\n",
            "Epoch 44/60\n",
            "130/130 - 0s - loss: 0.4932 - mae: 0.5173\n",
            "Epoch 45/60\n",
            "130/130 - 0s - loss: 0.4781 - mae: 0.5118\n",
            "Epoch 46/60\n",
            "130/130 - 0s - loss: 0.4908 - mae: 0.5173\n",
            "Epoch 47/60\n",
            "130/130 - 0s - loss: 0.4656 - mae: 0.5073\n",
            "Epoch 48/60\n",
            "130/130 - 0s - loss: 0.4717 - mae: 0.5075\n",
            "Epoch 49/60\n",
            "130/130 - 0s - loss: 0.4817 - mae: 0.5133\n",
            "Epoch 50/60\n",
            "130/130 - 0s - loss: 0.4645 - mae: 0.5068\n",
            "Epoch 51/60\n",
            "130/130 - 0s - loss: 0.4587 - mae: 0.4979\n",
            "Epoch 52/60\n",
            "130/130 - 0s - loss: 0.4588 - mae: 0.5017\n",
            "Epoch 53/60\n",
            "130/130 - 0s - loss: 0.4586 - mae: 0.5025\n",
            "Epoch 54/60\n",
            "130/130 - 0s - loss: 0.4611 - mae: 0.5019\n",
            "Epoch 55/60\n",
            "130/130 - 0s - loss: 0.4589 - mae: 0.5012\n",
            "Epoch 56/60\n",
            "130/130 - 0s - loss: 0.4538 - mae: 0.4958\n",
            "Epoch 57/60\n",
            "130/130 - 0s - loss: 0.4570 - mae: 0.4989\n",
            "Epoch 58/60\n",
            "130/130 - 0s - loss: 0.4414 - mae: 0.4937\n",
            "Epoch 59/60\n",
            "130/130 - 0s - loss: 0.4723 - mae: 0.5025\n",
            "Epoch 60/60\n",
            "130/130 - 0s - loss: 0.4518 - mae: 0.4955\n",
            "WARNING:tensorflow:10 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fa1864beae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "[2.0, 1.0, 1.0]\n",
            "[2.0, 1.0, 1.0]\n",
            "[1.208609, 1.5018479, 0.7182718]\n",
            "[1.2139034, 1.2991118, 0.68438333]\n",
            "[1.2712950706481934, 1.8056869506835938, 0.7186263203620911]\n",
            "(4167, 3, 1)\n",
            "(4167, 100, 2, 1)\n",
            "(4167, 100, 2, 1)\n",
            "Model: \"functional_1497\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_375 (InputLayer)       [(None, 100, 2, 1)]       0         \n",
            "_________________________________________________________________\n",
            "conv2d_1496 (Conv2D)         (None, 100, 2, 16)        112       \n",
            "_________________________________________________________________\n",
            "batch_normalization_1496 (Ba (None, 100, 2, 16)        64        \n",
            "_________________________________________________________________\n",
            "activation_1870 (Activation) (None, 100, 2, 16)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_374 (MaxPoolin (None, 50, 2, 16)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_1497 (Conv2D)         (None, 50, 2, 12)         1164      \n",
            "_________________________________________________________________\n",
            "batch_normalization_1497 (Ba (None, 50, 2, 12)         48        \n",
            "_________________________________________________________________\n",
            "up_sampling2d_374 (UpSamplin (None, 100, 2, 12)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1498 (Conv2D)         (None, 100, 2, 16)        1168      \n",
            "_________________________________________________________________\n",
            "batch_normalization_1498 (Ba (None, 100, 2, 16)        64        \n",
            "_________________________________________________________________\n",
            "activation_1872 (Activation) (None, 100, 2, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1499 (Conv2D)         (None, 100, 2, 1)         97        \n",
            "=================================================================\n",
            "Total params: 2,717\n",
            "Trainable params: 2,629\n",
            "Non-trainable params: 88\n",
            "_________________________________________________________________\n",
            "Epoch 1/30\n",
            "131/131 - 0s - loss: 0.0280 - mae: 0.0873\n",
            "Epoch 2/30\n",
            "131/131 - 0s - loss: 0.0065 - mae: 0.0465\n",
            "Epoch 3/30\n",
            "131/131 - 0s - loss: 0.0051 - mae: 0.0417\n",
            "Epoch 4/30\n",
            "131/131 - 0s - loss: 0.0047 - mae: 0.0411\n",
            "Epoch 5/30\n",
            "131/131 - 0s - loss: 0.0040 - mae: 0.0375\n",
            "Epoch 6/30\n",
            "131/131 - 0s - loss: 0.0040 - mae: 0.0384\n",
            "Epoch 7/30\n",
            "131/131 - 0s - loss: 0.0037 - mae: 0.0359\n",
            "Epoch 8/30\n",
            "131/131 - 0s - loss: 0.0027 - mae: 0.0315\n",
            "Epoch 9/30\n",
            "131/131 - 0s - loss: 0.0029 - mae: 0.0325\n",
            "Epoch 10/30\n",
            "131/131 - 0s - loss: 0.0029 - mae: 0.0319\n",
            "Epoch 11/30\n",
            "131/131 - 0s - loss: 0.0026 - mae: 0.0307\n",
            "Epoch 12/30\n",
            "131/131 - 0s - loss: 0.0027 - mae: 0.0305\n",
            "Epoch 13/30\n",
            "131/131 - 0s - loss: 0.0024 - mae: 0.0296\n",
            "Epoch 14/30\n",
            "131/131 - 0s - loss: 0.0026 - mae: 0.0307\n",
            "Epoch 15/30\n",
            "131/131 - 0s - loss: 0.0029 - mae: 0.0308\n",
            "Epoch 16/30\n",
            "131/131 - 0s - loss: 0.0019 - mae: 0.0261\n",
            "Epoch 17/30\n",
            "131/131 - 0s - loss: 0.0024 - mae: 0.0298\n",
            "Epoch 18/30\n",
            "131/131 - 0s - loss: 0.0023 - mae: 0.0279\n",
            "Epoch 19/30\n",
            "131/131 - 0s - loss: 0.0019 - mae: 0.0260\n",
            "Epoch 20/30\n",
            "131/131 - 0s - loss: 0.0026 - mae: 0.0288\n",
            "Epoch 21/30\n",
            "131/131 - 0s - loss: 0.0029 - mae: 0.0304\n",
            "Epoch 22/30\n",
            "131/131 - 0s - loss: 0.0020 - mae: 0.0248\n",
            "Epoch 23/30\n",
            "131/131 - 0s - loss: 0.0020 - mae: 0.0259\n",
            "Epoch 24/30\n",
            "131/131 - 0s - loss: 0.0018 - mae: 0.0253\n",
            "Epoch 25/30\n",
            "131/131 - 0s - loss: 0.0017 - mae: 0.0242\n",
            "Epoch 26/30\n",
            "131/131 - 0s - loss: 0.0016 - mae: 0.0241\n",
            "Epoch 27/30\n",
            "131/131 - 0s - loss: 0.0023 - mae: 0.0278\n",
            "Epoch 28/30\n",
            "131/131 - 0s - loss: 0.0017 - mae: 0.0241\n",
            "Epoch 29/30\n",
            "131/131 - 0s - loss: 0.0016 - mae: 0.0236\n",
            "Epoch 30/30\n",
            "131/131 - 0s - loss: 0.0015 - mae: 0.0223\n",
            "Model: \"functional_1499\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_375 (InputLayer)       [(None, 100, 2, 1)]       0         \n",
            "_________________________________________________________________\n",
            "conv2d_1496 (Conv2D)         (None, 100, 2, 16)        112       \n",
            "_________________________________________________________________\n",
            "batch_normalization_1496 (Ba (None, 100, 2, 16)        64        \n",
            "_________________________________________________________________\n",
            "activation_1870 (Activation) (None, 100, 2, 16)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_374 (MaxPoolin (None, 50, 2, 16)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_1497 (Conv2D)         (None, 50, 2, 12)         1164      \n",
            "_________________________________________________________________\n",
            "batch_normalization_1497 (Ba (None, 50, 2, 12)         48        \n",
            "_________________________________________________________________\n",
            "activation_1871 (Activation) (None, 50, 2, 12)         0         \n",
            "_________________________________________________________________\n",
            "flatten_374 (Flatten)        (None, 1200)              0         \n",
            "_________________________________________________________________\n",
            "dense_748 (Dense)            (None, 16)                19216     \n",
            "_________________________________________________________________\n",
            "batch_normalization_1499 (Ba (None, 16)                64        \n",
            "_________________________________________________________________\n",
            "activation_1873 (Activation) (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dropout_374 (Dropout)        (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_749 (Dense)            (None, 3)                 51        \n",
            "_________________________________________________________________\n",
            "activation_1874 (Activation) (None, 3)                 0         \n",
            "=================================================================\n",
            "Total params: 20,719\n",
            "Trainable params: 20,631\n",
            "Non-trainable params: 88\n",
            "_________________________________________________________________\n",
            "Epoch 1/60\n",
            "130/130 - 0s - loss: 2.1202 - mae: 1.0677\n",
            "Epoch 2/60\n",
            "130/130 - 0s - loss: 1.1978 - mae: 0.7959\n",
            "Epoch 3/60\n",
            "130/130 - 0s - loss: 0.9682 - mae: 0.7089\n",
            "Epoch 4/60\n",
            "130/130 - 0s - loss: 0.8481 - mae: 0.6682\n",
            "Epoch 5/60\n",
            "130/130 - 0s - loss: 0.7732 - mae: 0.6387\n",
            "Epoch 6/60\n",
            "130/130 - 0s - loss: 0.7196 - mae: 0.6172\n",
            "Epoch 7/60\n",
            "130/130 - 0s - loss: 0.6912 - mae: 0.6103\n",
            "Epoch 8/60\n",
            "130/130 - 0s - loss: 0.6616 - mae: 0.5989\n",
            "Epoch 9/60\n",
            "130/130 - 0s - loss: 0.6735 - mae: 0.6060\n",
            "Epoch 10/60\n",
            "130/130 - 0s - loss: 0.6614 - mae: 0.6056\n",
            "Epoch 11/60\n",
            "130/130 - 0s - loss: 0.6460 - mae: 0.6008\n",
            "Epoch 12/60\n",
            "130/130 - 0s - loss: 0.6420 - mae: 0.5988\n",
            "Epoch 13/60\n",
            "130/130 - 0s - loss: 0.6135 - mae: 0.5903\n",
            "Epoch 14/60\n",
            "130/130 - 0s - loss: 0.5962 - mae: 0.5763\n",
            "Epoch 15/60\n",
            "130/130 - 0s - loss: 0.6042 - mae: 0.5790\n",
            "Epoch 16/60\n",
            "130/130 - 0s - loss: 0.5908 - mae: 0.5747\n",
            "Epoch 17/60\n",
            "130/130 - 0s - loss: 0.5793 - mae: 0.5687\n",
            "Epoch 18/60\n",
            "130/130 - 0s - loss: 0.5954 - mae: 0.5740\n",
            "Epoch 19/60\n",
            "130/130 - 0s - loss: 0.5819 - mae: 0.5718\n",
            "Epoch 20/60\n",
            "130/130 - 0s - loss: 0.5630 - mae: 0.5608\n",
            "Epoch 21/60\n",
            "130/130 - 0s - loss: 0.5627 - mae: 0.5612\n",
            "Epoch 22/60\n",
            "130/130 - 0s - loss: 0.5504 - mae: 0.5539\n",
            "Epoch 23/60\n",
            "130/130 - 0s - loss: 0.5522 - mae: 0.5576\n",
            "Epoch 24/60\n",
            "130/130 - 0s - loss: 0.5563 - mae: 0.5569\n",
            "Epoch 25/60\n",
            "130/130 - 0s - loss: 0.5419 - mae: 0.5512\n",
            "Epoch 26/60\n",
            "130/130 - 0s - loss: 0.5442 - mae: 0.5508\n",
            "Epoch 27/60\n",
            "130/130 - 0s - loss: 0.5300 - mae: 0.5433\n",
            "Epoch 28/60\n",
            "130/130 - 0s - loss: 0.5227 - mae: 0.5423\n",
            "Epoch 29/60\n",
            "130/130 - 0s - loss: 0.5215 - mae: 0.5395\n",
            "Epoch 30/60\n",
            "130/130 - 0s - loss: 0.5163 - mae: 0.5366\n",
            "Epoch 31/60\n",
            "130/130 - 0s - loss: 0.5234 - mae: 0.5393\n",
            "Epoch 32/60\n",
            "130/130 - 0s - loss: 0.5111 - mae: 0.5308\n",
            "Epoch 33/60\n",
            "130/130 - 0s - loss: 0.5076 - mae: 0.5294\n",
            "Epoch 34/60\n",
            "130/130 - 0s - loss: 0.4944 - mae: 0.5240\n",
            "Epoch 35/60\n",
            "130/130 - 0s - loss: 0.5007 - mae: 0.5257\n",
            "Epoch 36/60\n",
            "130/130 - 0s - loss: 0.5080 - mae: 0.5285\n",
            "Epoch 37/60\n",
            "130/130 - 0s - loss: 0.4929 - mae: 0.5211\n",
            "Epoch 38/60\n",
            "130/130 - 0s - loss: 0.4924 - mae: 0.5209\n",
            "Epoch 39/60\n",
            "130/130 - 0s - loss: 0.4830 - mae: 0.5180\n",
            "Epoch 40/60\n",
            "130/130 - 0s - loss: 0.4718 - mae: 0.5097\n",
            "Epoch 41/60\n",
            "130/130 - 0s - loss: 0.4931 - mae: 0.5203\n",
            "Epoch 42/60\n",
            "130/130 - 0s - loss: 0.4793 - mae: 0.5147\n",
            "Epoch 43/60\n",
            "130/130 - 0s - loss: 0.4952 - mae: 0.5204\n",
            "Epoch 44/60\n",
            "130/130 - 0s - loss: 0.4555 - mae: 0.5031\n",
            "Epoch 45/60\n",
            "130/130 - 0s - loss: 0.4597 - mae: 0.5044\n",
            "Epoch 46/60\n",
            "130/130 - 0s - loss: 0.4738 - mae: 0.5111\n",
            "Epoch 47/60\n",
            "130/130 - 0s - loss: 0.4738 - mae: 0.5065\n",
            "Epoch 48/60\n",
            "130/130 - 0s - loss: 0.4669 - mae: 0.5041\n",
            "Epoch 49/60\n",
            "130/130 - 0s - loss: 0.4575 - mae: 0.5013\n",
            "Epoch 50/60\n",
            "130/130 - 0s - loss: 0.4736 - mae: 0.5043\n",
            "Epoch 51/60\n",
            "130/130 - 0s - loss: 0.4679 - mae: 0.5066\n",
            "Epoch 52/60\n",
            "130/130 - 0s - loss: 0.4498 - mae: 0.4971\n",
            "Epoch 53/60\n",
            "130/130 - 0s - loss: 0.4770 - mae: 0.5108\n",
            "Epoch 54/60\n",
            "130/130 - 0s - loss: 0.4503 - mae: 0.4994\n",
            "Epoch 55/60\n",
            "130/130 - 0s - loss: 0.4655 - mae: 0.5050\n",
            "Epoch 56/60\n",
            "130/130 - 0s - loss: 0.4506 - mae: 0.4988\n",
            "Epoch 57/60\n",
            "130/130 - 0s - loss: 0.4668 - mae: 0.5044\n",
            "Epoch 58/60\n",
            "130/130 - 0s - loss: 0.4411 - mae: 0.4946\n",
            "Epoch 59/60\n",
            "130/130 - 0s - loss: 0.4391 - mae: 0.4908\n",
            "Epoch 60/60\n",
            "130/130 - 0s - loss: 0.4595 - mae: 0.5030\n",
            "WARNING:tensorflow:11 out of the last 12 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fa186aef0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "[5.0, 5.0, 2.0]\n",
            "[5.0, 5.0, 2.0]\n",
            "[3.430902, 4.1157107, 1.2187195]\n",
            "[3.430902, 4.1157107, 1.2187195]\n",
            "[3.663179337978363, 4.342041492462158, 1.254612773656845]\n",
            "(4167, 3, 1)\n",
            "(4167, 100, 2, 1)\n",
            "(4167, 100, 2, 1)\n",
            "Model: \"functional_1501\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_376 (InputLayer)       [(None, 100, 2, 1)]       0         \n",
            "_________________________________________________________________\n",
            "conv2d_1500 (Conv2D)         (None, 100, 2, 16)        112       \n",
            "_________________________________________________________________\n",
            "batch_normalization_1500 (Ba (None, 100, 2, 16)        64        \n",
            "_________________________________________________________________\n",
            "activation_1875 (Activation) (None, 100, 2, 16)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_375 (MaxPoolin (None, 50, 2, 16)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_1501 (Conv2D)         (None, 50, 2, 12)         1164      \n",
            "_________________________________________________________________\n",
            "batch_normalization_1501 (Ba (None, 50, 2, 12)         48        \n",
            "_________________________________________________________________\n",
            "up_sampling2d_375 (UpSamplin (None, 100, 2, 12)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1502 (Conv2D)         (None, 100, 2, 16)        1168      \n",
            "_________________________________________________________________\n",
            "batch_normalization_1502 (Ba (None, 100, 2, 16)        64        \n",
            "_________________________________________________________________\n",
            "activation_1877 (Activation) (None, 100, 2, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1503 (Conv2D)         (None, 100, 2, 1)         97        \n",
            "=================================================================\n",
            "Total params: 2,717\n",
            "Trainable params: 2,629\n",
            "Non-trainable params: 88\n",
            "_________________________________________________________________\n",
            "Epoch 1/30\n",
            "131/131 - 0s - loss: 0.0278 - mae: 0.0874\n",
            "Epoch 2/30\n",
            "131/131 - 0s - loss: 0.0066 - mae: 0.0476\n",
            "Epoch 3/30\n",
            "131/131 - 0s - loss: 0.0048 - mae: 0.0410\n",
            "Epoch 4/30\n",
            "131/131 - 0s - loss: 0.0047 - mae: 0.0417\n",
            "Epoch 5/30\n",
            "131/131 - 0s - loss: 0.0041 - mae: 0.0385\n",
            "Epoch 6/30\n",
            "131/131 - 0s - loss: 0.0037 - mae: 0.0383\n",
            "Epoch 7/30\n",
            "131/131 - 0s - loss: 0.0036 - mae: 0.0362\n",
            "Epoch 8/30\n",
            "131/131 - 0s - loss: 0.0030 - mae: 0.0332\n",
            "Epoch 9/30\n",
            "131/131 - 0s - loss: 0.0031 - mae: 0.0333\n",
            "Epoch 10/30\n",
            "131/131 - 0s - loss: 0.0031 - mae: 0.0325\n",
            "Epoch 11/30\n",
            "131/131 - 0s - loss: 0.0027 - mae: 0.0313\n",
            "Epoch 12/30\n",
            "131/131 - 0s - loss: 0.0026 - mae: 0.0301\n",
            "Epoch 13/30\n",
            "131/131 - 0s - loss: 0.0024 - mae: 0.0294\n",
            "Epoch 14/30\n",
            "131/131 - 0s - loss: 0.0026 - mae: 0.0306\n",
            "Epoch 15/30\n",
            "131/131 - 0s - loss: 0.0030 - mae: 0.0304\n",
            "Epoch 16/30\n",
            "131/131 - 0s - loss: 0.0020 - mae: 0.0263\n",
            "Epoch 17/30\n",
            "131/131 - 0s - loss: 0.0024 - mae: 0.0291\n",
            "Epoch 18/30\n",
            "131/131 - 0s - loss: 0.0022 - mae: 0.0268\n",
            "Epoch 19/30\n",
            "131/131 - 0s - loss: 0.0017 - mae: 0.0246\n",
            "Epoch 20/30\n",
            "131/131 - 0s - loss: 0.0025 - mae: 0.0278\n",
            "Epoch 21/30\n",
            "131/131 - 0s - loss: 0.0027 - mae: 0.0297\n",
            "Epoch 22/30\n",
            "131/131 - 0s - loss: 0.0020 - mae: 0.0248\n",
            "Epoch 23/30\n",
            "131/131 - 0s - loss: 0.0020 - mae: 0.0259\n",
            "Epoch 24/30\n",
            "131/131 - 0s - loss: 0.0018 - mae: 0.0255\n",
            "Epoch 25/30\n",
            "131/131 - 0s - loss: 0.0018 - mae: 0.0242\n",
            "Epoch 26/30\n",
            "131/131 - 0s - loss: 0.0017 - mae: 0.0247\n",
            "Epoch 27/30\n",
            "131/131 - 0s - loss: 0.0023 - mae: 0.0272\n",
            "Epoch 28/30\n",
            "131/131 - 0s - loss: 0.0019 - mae: 0.0243\n",
            "Epoch 29/30\n",
            "131/131 - 0s - loss: 0.0018 - mae: 0.0246\n",
            "Epoch 30/30\n",
            "131/131 - 0s - loss: 0.0015 - mae: 0.0224\n",
            "Model: \"functional_1503\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_376 (InputLayer)       [(None, 100, 2, 1)]       0         \n",
            "_________________________________________________________________\n",
            "conv2d_1500 (Conv2D)         (None, 100, 2, 16)        112       \n",
            "_________________________________________________________________\n",
            "batch_normalization_1500 (Ba (None, 100, 2, 16)        64        \n",
            "_________________________________________________________________\n",
            "activation_1875 (Activation) (None, 100, 2, 16)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_375 (MaxPoolin (None, 50, 2, 16)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_1501 (Conv2D)         (None, 50, 2, 12)         1164      \n",
            "_________________________________________________________________\n",
            "batch_normalization_1501 (Ba (None, 50, 2, 12)         48        \n",
            "_________________________________________________________________\n",
            "activation_1876 (Activation) (None, 50, 2, 12)         0         \n",
            "_________________________________________________________________\n",
            "flatten_375 (Flatten)        (None, 1200)              0         \n",
            "_________________________________________________________________\n",
            "dense_750 (Dense)            (None, 16)                19216     \n",
            "_________________________________________________________________\n",
            "batch_normalization_1503 (Ba (None, 16)                64        \n",
            "_________________________________________________________________\n",
            "activation_1878 (Activation) (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dropout_375 (Dropout)        (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_751 (Dense)            (None, 3)                 51        \n",
            "_________________________________________________________________\n",
            "activation_1879 (Activation) (None, 3)                 0         \n",
            "=================================================================\n",
            "Total params: 20,719\n",
            "Trainable params: 20,631\n",
            "Non-trainable params: 88\n",
            "_________________________________________________________________\n",
            "Epoch 1/60\n",
            "130/130 - 0s - loss: 2.0983 - mae: 1.0579\n",
            "Epoch 2/60\n",
            "130/130 - 0s - loss: 1.1580 - mae: 0.7861\n",
            "Epoch 3/60\n",
            "130/130 - 0s - loss: 0.9374 - mae: 0.6983\n",
            "Epoch 4/60\n",
            "130/130 - 0s - loss: 0.8326 - mae: 0.6590\n",
            "Epoch 5/60\n",
            "130/130 - 0s - loss: 0.7715 - mae: 0.6410\n",
            "Epoch 6/60\n",
            "130/130 - 0s - loss: 0.7158 - mae: 0.6204\n",
            "Epoch 7/60\n",
            "130/130 - 0s - loss: 0.6954 - mae: 0.6107\n",
            "Epoch 8/60\n",
            "130/130 - 0s - loss: 0.6641 - mae: 0.5970\n",
            "Epoch 9/60\n",
            "130/130 - 0s - loss: 0.6724 - mae: 0.6051\n",
            "Epoch 10/60\n",
            "130/130 - 0s - loss: 0.6553 - mae: 0.6007\n",
            "Epoch 11/60\n",
            "130/130 - 0s - loss: 0.6434 - mae: 0.5960\n",
            "Epoch 12/60\n",
            "130/130 - 0s - loss: 0.6346 - mae: 0.5919\n",
            "Epoch 13/60\n",
            "130/130 - 0s - loss: 0.6003 - mae: 0.5811\n",
            "Epoch 14/60\n",
            "130/130 - 0s - loss: 0.5948 - mae: 0.5756\n",
            "Epoch 15/60\n",
            "130/130 - 0s - loss: 0.6021 - mae: 0.5790\n",
            "Epoch 16/60\n",
            "130/130 - 0s - loss: 0.5823 - mae: 0.5692\n",
            "Epoch 17/60\n",
            "130/130 - 0s - loss: 0.5720 - mae: 0.5614\n",
            "Epoch 18/60\n",
            "130/130 - 0s - loss: 0.5869 - mae: 0.5699\n",
            "Epoch 19/60\n",
            "130/130 - 0s - loss: 0.5806 - mae: 0.5675\n",
            "Epoch 20/60\n",
            "130/130 - 0s - loss: 0.5660 - mae: 0.5593\n",
            "Epoch 21/60\n",
            "130/130 - 0s - loss: 0.5634 - mae: 0.5588\n",
            "Epoch 22/60\n",
            "130/130 - 0s - loss: 0.5572 - mae: 0.5565\n",
            "Epoch 23/60\n",
            "130/130 - 0s - loss: 0.5479 - mae: 0.5534\n",
            "Epoch 24/60\n",
            "130/130 - 0s - loss: 0.5515 - mae: 0.5510\n",
            "Epoch 25/60\n",
            "130/130 - 0s - loss: 0.5538 - mae: 0.5541\n",
            "Epoch 26/60\n",
            "130/130 - 0s - loss: 0.5548 - mae: 0.5525\n",
            "Epoch 27/60\n",
            "130/130 - 0s - loss: 0.5299 - mae: 0.5439\n",
            "Epoch 28/60\n",
            "130/130 - 0s - loss: 0.5355 - mae: 0.5473\n",
            "Epoch 29/60\n",
            "130/130 - 0s - loss: 0.5320 - mae: 0.5440\n",
            "Epoch 30/60\n",
            "130/130 - 0s - loss: 0.5187 - mae: 0.5390\n",
            "Epoch 31/60\n",
            "130/130 - 0s - loss: 0.5335 - mae: 0.5427\n",
            "Epoch 32/60\n",
            "130/130 - 0s - loss: 0.5048 - mae: 0.5306\n",
            "Epoch 33/60\n",
            "130/130 - 0s - loss: 0.5246 - mae: 0.5404\n",
            "Epoch 34/60\n",
            "130/130 - 0s - loss: 0.5064 - mae: 0.5311\n",
            "Epoch 35/60\n",
            "130/130 - 0s - loss: 0.5023 - mae: 0.5253\n",
            "Epoch 36/60\n",
            "130/130 - 0s - loss: 0.5147 - mae: 0.5365\n",
            "Epoch 37/60\n",
            "130/130 - 0s - loss: 0.5024 - mae: 0.5261\n",
            "Epoch 38/60\n",
            "130/130 - 0s - loss: 0.5001 - mae: 0.5258\n",
            "Epoch 39/60\n",
            "130/130 - 0s - loss: 0.4899 - mae: 0.5232\n",
            "Epoch 40/60\n",
            "130/130 - 0s - loss: 0.4823 - mae: 0.5162\n",
            "Epoch 41/60\n",
            "130/130 - 0s - loss: 0.5090 - mae: 0.5272\n",
            "Epoch 42/60\n",
            "130/130 - 0s - loss: 0.4844 - mae: 0.5188\n",
            "Epoch 43/60\n",
            "130/130 - 0s - loss: 0.5110 - mae: 0.5284\n",
            "Epoch 44/60\n",
            "130/130 - 0s - loss: 0.4696 - mae: 0.5113\n",
            "Epoch 45/60\n",
            "130/130 - 0s - loss: 0.4822 - mae: 0.5171\n",
            "Epoch 46/60\n",
            "130/130 - 0s - loss: 0.4825 - mae: 0.5182\n",
            "Epoch 47/60\n",
            "130/130 - 0s - loss: 0.4757 - mae: 0.5099\n",
            "Epoch 48/60\n",
            "130/130 - 0s - loss: 0.4738 - mae: 0.5111\n",
            "Epoch 49/60\n",
            "130/130 - 0s - loss: 0.4803 - mae: 0.5135\n",
            "Epoch 50/60\n",
            "130/130 - 0s - loss: 0.4749 - mae: 0.5075\n",
            "Epoch 51/60\n",
            "130/130 - 0s - loss: 0.4594 - mae: 0.5046\n",
            "Epoch 52/60\n",
            "130/130 - 0s - loss: 0.4697 - mae: 0.5068\n",
            "Epoch 53/60\n",
            "130/130 - 0s - loss: 0.4809 - mae: 0.5156\n",
            "Epoch 54/60\n",
            "130/130 - 0s - loss: 0.4577 - mae: 0.5058\n",
            "Epoch 55/60\n",
            "130/130 - 0s - loss: 0.4826 - mae: 0.5153\n",
            "Epoch 56/60\n",
            "130/130 - 0s - loss: 0.4630 - mae: 0.5059\n",
            "Epoch 57/60\n",
            "130/130 - 0s - loss: 0.4575 - mae: 0.5032\n",
            "Epoch 58/60\n",
            "130/130 - 0s - loss: 0.4469 - mae: 0.4994\n",
            "Epoch 59/60\n",
            "130/130 - 0s - loss: 0.4486 - mae: 0.4983\n",
            "Epoch 60/60\n",
            "130/130 - 0s - loss: 0.4758 - mae: 0.5119\n",
            "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fa185de2730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "[2.0, 1.0, 1.0]\n",
            "[2.0, 1.0, 1.0]\n",
            "[0.9691453, 0.9488125, 0.61013114]\n",
            "[0.9691453, 0.9488125, 0.61013114]\n",
            "[1.085534155368805, 1.1108521819114685, 0.6852405220270157]\n",
            "(4167, 3, 1)\n",
            "(4167, 100, 2, 1)\n",
            "(4167, 100, 2, 1)\n",
            "Model: \"functional_1505\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_377 (InputLayer)       [(None, 100, 2, 1)]       0         \n",
            "_________________________________________________________________\n",
            "conv2d_1504 (Conv2D)         (None, 100, 2, 16)        112       \n",
            "_________________________________________________________________\n",
            "batch_normalization_1504 (Ba (None, 100, 2, 16)        64        \n",
            "_________________________________________________________________\n",
            "activation_1880 (Activation) (None, 100, 2, 16)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_376 (MaxPoolin (None, 50, 2, 16)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_1505 (Conv2D)         (None, 50, 2, 12)         1164      \n",
            "_________________________________________________________________\n",
            "batch_normalization_1505 (Ba (None, 50, 2, 12)         48        \n",
            "_________________________________________________________________\n",
            "up_sampling2d_376 (UpSamplin (None, 100, 2, 12)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1506 (Conv2D)         (None, 100, 2, 16)        1168      \n",
            "_________________________________________________________________\n",
            "batch_normalization_1506 (Ba (None, 100, 2, 16)        64        \n",
            "_________________________________________________________________\n",
            "activation_1882 (Activation) (None, 100, 2, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1507 (Conv2D)         (None, 100, 2, 1)         97        \n",
            "=================================================================\n",
            "Total params: 2,717\n",
            "Trainable params: 2,629\n",
            "Non-trainable params: 88\n",
            "_________________________________________________________________\n",
            "Epoch 1/30\n",
            "131/131 - 0s - loss: 0.0255 - mae: 0.0827\n",
            "Epoch 2/30\n",
            "131/131 - 0s - loss: 0.0065 - mae: 0.0470\n",
            "Epoch 3/30\n",
            "131/131 - 0s - loss: 0.0048 - mae: 0.0404\n",
            "Epoch 4/30\n",
            "131/131 - 0s - loss: 0.0045 - mae: 0.0402\n",
            "Epoch 5/30\n",
            "131/131 - 0s - loss: 0.0038 - mae: 0.0379\n",
            "Epoch 6/30\n",
            "131/131 - 0s - loss: 0.0037 - mae: 0.0369\n",
            "Epoch 7/30\n",
            "131/131 - 0s - loss: 0.0035 - mae: 0.0350\n",
            "Epoch 8/30\n",
            "131/131 - 0s - loss: 0.0026 - mae: 0.0308\n",
            "Epoch 9/30\n",
            "131/131 - 0s - loss: 0.0030 - mae: 0.0333\n",
            "Epoch 10/30\n",
            "131/131 - 0s - loss: 0.0030 - mae: 0.0327\n",
            "Epoch 11/30\n",
            "131/131 - 0s - loss: 0.0027 - mae: 0.0313\n",
            "Epoch 12/30\n",
            "131/131 - 0s - loss: 0.0029 - mae: 0.0319\n",
            "Epoch 13/30\n",
            "131/131 - 0s - loss: 0.0026 - mae: 0.0312\n",
            "Epoch 14/30\n",
            "131/131 - 0s - loss: 0.0023 - mae: 0.0285\n",
            "Epoch 15/30\n",
            "131/131 - 0s - loss: 0.0023 - mae: 0.0283\n",
            "Epoch 16/30\n",
            "131/131 - 0s - loss: 0.0021 - mae: 0.0271\n",
            "Epoch 17/30\n",
            "131/131 - 0s - loss: 0.0022 - mae: 0.0279\n",
            "Epoch 18/30\n",
            "131/131 - 0s - loss: 0.0022 - mae: 0.0271\n",
            "Epoch 19/30\n",
            "131/131 - 0s - loss: 0.0019 - mae: 0.0263\n",
            "Epoch 20/30\n",
            "131/131 - 0s - loss: 0.0024 - mae: 0.0279\n",
            "Epoch 21/30\n",
            "131/131 - 0s - loss: 0.0027 - mae: 0.0294\n",
            "Epoch 22/30\n",
            "131/131 - 0s - loss: 0.0021 - mae: 0.0248\n",
            "Epoch 23/30\n",
            "131/131 - 0s - loss: 0.0027 - mae: 0.0292\n",
            "Epoch 24/30\n",
            "131/131 - 0s - loss: 0.0020 - mae: 0.0272\n",
            "Epoch 25/30\n",
            "131/131 - 0s - loss: 0.0019 - mae: 0.0259\n",
            "Epoch 26/30\n",
            "131/131 - 0s - loss: 0.0016 - mae: 0.0240\n",
            "Epoch 27/30\n",
            "131/131 - 0s - loss: 0.0025 - mae: 0.0287\n",
            "Epoch 28/30\n",
            "131/131 - 0s - loss: 0.0018 - mae: 0.0241\n",
            "Epoch 29/30\n",
            "131/131 - 0s - loss: 0.0015 - mae: 0.0235\n",
            "Epoch 30/30\n",
            "131/131 - 0s - loss: 0.0016 - mae: 0.0227\n",
            "Model: \"functional_1507\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_377 (InputLayer)       [(None, 100, 2, 1)]       0         \n",
            "_________________________________________________________________\n",
            "conv2d_1504 (Conv2D)         (None, 100, 2, 16)        112       \n",
            "_________________________________________________________________\n",
            "batch_normalization_1504 (Ba (None, 100, 2, 16)        64        \n",
            "_________________________________________________________________\n",
            "activation_1880 (Activation) (None, 100, 2, 16)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_376 (MaxPoolin (None, 50, 2, 16)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_1505 (Conv2D)         (None, 50, 2, 12)         1164      \n",
            "_________________________________________________________________\n",
            "batch_normalization_1505 (Ba (None, 50, 2, 12)         48        \n",
            "_________________________________________________________________\n",
            "activation_1881 (Activation) (None, 50, 2, 12)         0         \n",
            "_________________________________________________________________\n",
            "flatten_376 (Flatten)        (None, 1200)              0         \n",
            "_________________________________________________________________\n",
            "dense_752 (Dense)            (None, 16)                19216     \n",
            "_________________________________________________________________\n",
            "batch_normalization_1507 (Ba (None, 16)                64        \n",
            "_________________________________________________________________\n",
            "activation_1883 (Activation) (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dropout_376 (Dropout)        (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_753 (Dense)            (None, 3)                 51        \n",
            "_________________________________________________________________\n",
            "activation_1884 (Activation) (None, 3)                 0         \n",
            "=================================================================\n",
            "Total params: 20,719\n",
            "Trainable params: 20,631\n",
            "Non-trainable params: 88\n",
            "_________________________________________________________________\n",
            "Epoch 1/60\n",
            "130/130 - 0s - loss: 2.0780 - mae: 1.0571\n",
            "Epoch 2/60\n",
            "130/130 - 0s - loss: 1.1149 - mae: 0.7711\n",
            "Epoch 3/60\n",
            "130/130 - 0s - loss: 0.9345 - mae: 0.6964\n",
            "Epoch 4/60\n",
            "130/130 - 0s - loss: 0.8347 - mae: 0.6567\n",
            "Epoch 5/60\n",
            "130/130 - 0s - loss: 0.7545 - mae: 0.6320\n",
            "Epoch 6/60\n",
            "130/130 - 0s - loss: 0.7051 - mae: 0.6142\n",
            "Epoch 7/60\n",
            "130/130 - 0s - loss: 0.6887 - mae: 0.6119\n",
            "Epoch 8/60\n",
            "130/130 - 0s - loss: 0.6535 - mae: 0.5981\n",
            "Epoch 9/60\n",
            "130/130 - 0s - loss: 0.6626 - mae: 0.6039\n",
            "Epoch 10/60\n",
            "130/130 - 0s - loss: 0.6510 - mae: 0.6021\n",
            "Epoch 11/60\n",
            "130/130 - 0s - loss: 0.6388 - mae: 0.5975\n",
            "Epoch 12/60\n",
            "130/130 - 0s - loss: 0.6253 - mae: 0.5923\n",
            "Epoch 13/60\n",
            "130/130 - 0s - loss: 0.6057 - mae: 0.5880\n",
            "Epoch 14/60\n",
            "130/130 - 0s - loss: 0.5910 - mae: 0.5740\n",
            "Epoch 15/60\n",
            "130/130 - 0s - loss: 0.5936 - mae: 0.5774\n",
            "Epoch 16/60\n",
            "130/130 - 0s - loss: 0.5842 - mae: 0.5711\n",
            "Epoch 17/60\n",
            "130/130 - 0s - loss: 0.5750 - mae: 0.5687\n",
            "Epoch 18/60\n",
            "130/130 - 0s - loss: 0.5839 - mae: 0.5718\n",
            "Epoch 19/60\n",
            "130/130 - 0s - loss: 0.5830 - mae: 0.5702\n",
            "Epoch 20/60\n",
            "130/130 - 0s - loss: 0.5683 - mae: 0.5623\n",
            "Epoch 21/60\n",
            "130/130 - 0s - loss: 0.5626 - mae: 0.5620\n",
            "Epoch 22/60\n",
            "130/130 - 0s - loss: 0.5540 - mae: 0.5566\n",
            "Epoch 23/60\n",
            "130/130 - 0s - loss: 0.5639 - mae: 0.5622\n",
            "Epoch 24/60\n",
            "130/130 - 0s - loss: 0.5499 - mae: 0.5529\n",
            "Epoch 25/60\n",
            "130/130 - 0s - loss: 0.5457 - mae: 0.5526\n",
            "Epoch 26/60\n",
            "130/130 - 0s - loss: 0.5423 - mae: 0.5511\n",
            "Epoch 27/60\n",
            "130/130 - 0s - loss: 0.5263 - mae: 0.5427\n",
            "Epoch 28/60\n",
            "130/130 - 0s - loss: 0.5241 - mae: 0.5428\n",
            "Epoch 29/60\n",
            "130/130 - 0s - loss: 0.5309 - mae: 0.5445\n",
            "Epoch 30/60\n",
            "130/130 - 0s - loss: 0.5093 - mae: 0.5379\n",
            "Epoch 31/60\n",
            "130/130 - 0s - loss: 0.5212 - mae: 0.5370\n",
            "Epoch 32/60\n",
            "130/130 - 0s - loss: 0.5167 - mae: 0.5357\n",
            "Epoch 33/60\n",
            "130/130 - 0s - loss: 0.5153 - mae: 0.5353\n",
            "Epoch 34/60\n",
            "130/130 - 0s - loss: 0.4987 - mae: 0.5273\n",
            "Epoch 35/60\n",
            "130/130 - 0s - loss: 0.5049 - mae: 0.5262\n",
            "Epoch 36/60\n",
            "130/130 - 0s - loss: 0.4934 - mae: 0.5235\n",
            "Epoch 37/60\n",
            "130/130 - 0s - loss: 0.5003 - mae: 0.5233\n",
            "Epoch 38/60\n",
            "130/130 - 0s - loss: 0.4964 - mae: 0.5232\n",
            "Epoch 39/60\n",
            "130/130 - 0s - loss: 0.4987 - mae: 0.5254\n",
            "Epoch 40/60\n",
            "130/130 - 0s - loss: 0.4668 - mae: 0.5078\n",
            "Epoch 41/60\n",
            "130/130 - 0s - loss: 0.4997 - mae: 0.5222\n",
            "Epoch 42/60\n",
            "130/130 - 0s - loss: 0.4749 - mae: 0.5110\n",
            "Epoch 43/60\n",
            "130/130 - 0s - loss: 0.4961 - mae: 0.5200\n",
            "Epoch 44/60\n",
            "130/130 - 0s - loss: 0.4576 - mae: 0.5006\n",
            "Epoch 45/60\n",
            "130/130 - 0s - loss: 0.4722 - mae: 0.5091\n",
            "Epoch 46/60\n",
            "130/130 - 0s - loss: 0.4814 - mae: 0.5137\n",
            "Epoch 47/60\n",
            "130/130 - 0s - loss: 0.4685 - mae: 0.5066\n",
            "Epoch 48/60\n",
            "130/130 - 0s - loss: 0.4632 - mae: 0.5026\n",
            "Epoch 49/60\n",
            "130/130 - 0s - loss: 0.4550 - mae: 0.4997\n",
            "Epoch 50/60\n",
            "130/130 - 0s - loss: 0.4654 - mae: 0.5014\n",
            "Epoch 51/60\n",
            "130/130 - 0s - loss: 0.4624 - mae: 0.5008\n",
            "Epoch 52/60\n",
            "130/130 - 0s - loss: 0.4497 - mae: 0.4975\n",
            "Epoch 53/60\n",
            "130/130 - 0s - loss: 0.4612 - mae: 0.5047\n",
            "Epoch 54/60\n",
            "130/130 - 0s - loss: 0.4515 - mae: 0.4981\n",
            "Epoch 55/60\n",
            "130/130 - 0s - loss: 0.4701 - mae: 0.5061\n",
            "Epoch 56/60\n",
            "130/130 - 0s - loss: 0.4519 - mae: 0.5007\n",
            "Epoch 57/60\n",
            "130/130 - 0s - loss: 0.4585 - mae: 0.4985\n",
            "Epoch 58/60\n",
            "130/130 - 0s - loss: 0.4506 - mae: 0.4968\n",
            "Epoch 59/60\n",
            "130/130 - 0s - loss: 0.4404 - mae: 0.4944\n",
            "Epoch 60/60\n",
            "130/130 - 0s - loss: 0.4712 - mae: 0.5070\n",
            "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fa1867890d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "[0.0, 0.0, 1.0]\n",
            "[0.0, 0.0, 1.0]\n",
            "[0.78900605, 0.76790464, 0.60742354]\n",
            "[0.78900605, 0.76790464, 0.60742354]\n",
            "[0.9174470007419586, 0.782531276345253, 0.6350687593221664]\n",
            "(4167, 3, 1)\n",
            "(4167, 100, 2, 1)\n",
            "(4167, 100, 2, 1)\n",
            "Model: \"functional_1509\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_378 (InputLayer)       [(None, 100, 2, 1)]       0         \n",
            "_________________________________________________________________\n",
            "conv2d_1508 (Conv2D)         (None, 100, 2, 16)        112       \n",
            "_________________________________________________________________\n",
            "batch_normalization_1508 (Ba (None, 100, 2, 16)        64        \n",
            "_________________________________________________________________\n",
            "activation_1885 (Activation) (None, 100, 2, 16)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_377 (MaxPoolin (None, 50, 2, 16)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_1509 (Conv2D)         (None, 50, 2, 12)         1164      \n",
            "_________________________________________________________________\n",
            "batch_normalization_1509 (Ba (None, 50, 2, 12)         48        \n",
            "_________________________________________________________________\n",
            "up_sampling2d_377 (UpSamplin (None, 100, 2, 12)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1510 (Conv2D)         (None, 100, 2, 16)        1168      \n",
            "_________________________________________________________________\n",
            "batch_normalization_1510 (Ba (None, 100, 2, 16)        64        \n",
            "_________________________________________________________________\n",
            "activation_1887 (Activation) (None, 100, 2, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1511 (Conv2D)         (None, 100, 2, 1)         97        \n",
            "=================================================================\n",
            "Total params: 2,717\n",
            "Trainable params: 2,629\n",
            "Non-trainable params: 88\n",
            "_________________________________________________________________\n",
            "Epoch 1/30\n",
            "131/131 - 0s - loss: 0.0278 - mae: 0.0874\n",
            "Epoch 2/30\n",
            "131/131 - 0s - loss: 0.0066 - mae: 0.0476\n",
            "Epoch 3/30\n",
            "131/131 - 0s - loss: 0.0048 - mae: 0.0410\n",
            "Epoch 4/30\n",
            "131/131 - 0s - loss: 0.0046 - mae: 0.0412\n",
            "Epoch 5/30\n",
            "131/131 - 0s - loss: 0.0040 - mae: 0.0383\n",
            "Epoch 6/30\n",
            "131/131 - 0s - loss: 0.0036 - mae: 0.0375\n",
            "Epoch 7/30\n",
            "131/131 - 0s - loss: 0.0036 - mae: 0.0361\n",
            "Epoch 8/30\n",
            "131/131 - 0s - loss: 0.0030 - mae: 0.0333\n",
            "Epoch 9/30\n",
            "131/131 - 0s - loss: 0.0032 - mae: 0.0335\n",
            "Epoch 10/30\n",
            "131/131 - 0s - loss: 0.0032 - mae: 0.0327\n",
            "Epoch 11/30\n",
            "131/131 - 0s - loss: 0.0027 - mae: 0.0313\n",
            "Epoch 12/30\n",
            "131/131 - 0s - loss: 0.0027 - mae: 0.0302\n",
            "Epoch 13/30\n",
            "131/131 - 0s - loss: 0.0024 - mae: 0.0294\n",
            "Epoch 14/30\n",
            "131/131 - 0s - loss: 0.0025 - mae: 0.0301\n",
            "Epoch 15/30\n",
            "131/131 - 0s - loss: 0.0031 - mae: 0.0310\n",
            "Epoch 16/30\n",
            "131/131 - 0s - loss: 0.0020 - mae: 0.0266\n",
            "Epoch 17/30\n",
            "131/131 - 0s - loss: 0.0024 - mae: 0.0297\n",
            "Epoch 18/30\n",
            "131/131 - 0s - loss: 0.0022 - mae: 0.0273\n",
            "Epoch 19/30\n",
            "131/131 - 0s - loss: 0.0017 - mae: 0.0248\n",
            "Epoch 20/30\n",
            "131/131 - 0s - loss: 0.0024 - mae: 0.0282\n",
            "Epoch 21/30\n",
            "131/131 - 0s - loss: 0.0026 - mae: 0.0291\n",
            "Epoch 22/30\n",
            "131/131 - 0s - loss: 0.0018 - mae: 0.0241\n",
            "Epoch 23/30\n",
            "131/131 - 0s - loss: 0.0021 - mae: 0.0262\n",
            "Epoch 24/30\n",
            "131/131 - 0s - loss: 0.0019 - mae: 0.0257\n",
            "Epoch 25/30\n",
            "131/131 - 0s - loss: 0.0019 - mae: 0.0252\n",
            "Epoch 26/30\n",
            "131/131 - 0s - loss: 0.0017 - mae: 0.0249\n",
            "Epoch 27/30\n",
            "131/131 - 0s - loss: 0.0023 - mae: 0.0276\n",
            "Epoch 28/30\n",
            "131/131 - 0s - loss: 0.0018 - mae: 0.0242\n",
            "Epoch 29/30\n",
            "131/131 - 0s - loss: 0.0017 - mae: 0.0241\n",
            "Epoch 30/30\n",
            "131/131 - 0s - loss: 0.0015 - mae: 0.0222\n",
            "Model: \"functional_1511\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_378 (InputLayer)       [(None, 100, 2, 1)]       0         \n",
            "_________________________________________________________________\n",
            "conv2d_1508 (Conv2D)         (None, 100, 2, 16)        112       \n",
            "_________________________________________________________________\n",
            "batch_normalization_1508 (Ba (None, 100, 2, 16)        64        \n",
            "_________________________________________________________________\n",
            "activation_1885 (Activation) (None, 100, 2, 16)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_377 (MaxPoolin (None, 50, 2, 16)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_1509 (Conv2D)         (None, 50, 2, 12)         1164      \n",
            "_________________________________________________________________\n",
            "batch_normalization_1509 (Ba (None, 50, 2, 12)         48        \n",
            "_________________________________________________________________\n",
            "activation_1886 (Activation) (None, 50, 2, 12)         0         \n",
            "_________________________________________________________________\n",
            "flatten_377 (Flatten)        (None, 1200)              0         \n",
            "_________________________________________________________________\n",
            "dense_754 (Dense)            (None, 16)                19216     \n",
            "_________________________________________________________________\n",
            "batch_normalization_1511 (Ba (None, 16)                64        \n",
            "_________________________________________________________________\n",
            "activation_1888 (Activation) (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dropout_377 (Dropout)        (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_755 (Dense)            (None, 3)                 51        \n",
            "_________________________________________________________________\n",
            "activation_1889 (Activation) (None, 3)                 0         \n",
            "=================================================================\n",
            "Total params: 20,719\n",
            "Trainable params: 20,631\n",
            "Non-trainable params: 88\n",
            "_________________________________________________________________\n",
            "Epoch 1/60\n",
            "130/130 - 0s - loss: 2.1459 - mae: 1.0706\n",
            "Epoch 2/60\n",
            "130/130 - 0s - loss: 1.2083 - mae: 0.7952\n",
            "Epoch 3/60\n",
            "130/130 - 0s - loss: 0.9868 - mae: 0.7151\n",
            "Epoch 4/60\n",
            "130/130 - 0s - loss: 0.8528 - mae: 0.6664\n",
            "Epoch 5/60\n",
            "130/130 - 0s - loss: 0.7855 - mae: 0.6427\n",
            "Epoch 6/60\n",
            "130/130 - 0s - loss: 0.7196 - mae: 0.6189\n",
            "Epoch 7/60\n",
            "130/130 - 0s - loss: 0.6989 - mae: 0.6134\n",
            "Epoch 8/60\n",
            "130/130 - 0s - loss: 0.6652 - mae: 0.6009\n",
            "Epoch 9/60\n",
            "130/130 - 0s - loss: 0.6670 - mae: 0.6020\n",
            "Epoch 10/60\n",
            "130/130 - 0s - loss: 0.6576 - mae: 0.6029\n",
            "Epoch 11/60\n",
            "130/130 - 0s - loss: 0.6516 - mae: 0.6003\n",
            "Epoch 12/60\n",
            "130/130 - 0s - loss: 0.6314 - mae: 0.5915\n",
            "Epoch 13/60\n",
            "130/130 - 0s - loss: 0.6081 - mae: 0.5857\n",
            "Epoch 14/60\n",
            "130/130 - 0s - loss: 0.5994 - mae: 0.5800\n",
            "Epoch 15/60\n",
            "130/130 - 0s - loss: 0.6081 - mae: 0.5860\n",
            "Epoch 16/60\n",
            "130/130 - 0s - loss: 0.6015 - mae: 0.5831\n",
            "Epoch 17/60\n",
            "130/130 - 0s - loss: 0.5785 - mae: 0.5679\n",
            "Epoch 18/60\n",
            "130/130 - 0s - loss: 0.5960 - mae: 0.5756\n",
            "Epoch 19/60\n",
            "130/130 - 0s - loss: 0.5800 - mae: 0.5710\n",
            "Epoch 20/60\n",
            "130/130 - 0s - loss: 0.5692 - mae: 0.5637\n",
            "Epoch 21/60\n",
            "130/130 - 0s - loss: 0.5664 - mae: 0.5660\n",
            "Epoch 22/60\n",
            "130/130 - 0s - loss: 0.5617 - mae: 0.5635\n",
            "Epoch 23/60\n",
            "130/130 - 0s - loss: 0.5545 - mae: 0.5593\n",
            "Epoch 24/60\n",
            "130/130 - 0s - loss: 0.5571 - mae: 0.5562\n",
            "Epoch 25/60\n",
            "130/130 - 0s - loss: 0.5576 - mae: 0.5593\n",
            "Epoch 26/60\n",
            "130/130 - 0s - loss: 0.5530 - mae: 0.5563\n",
            "Epoch 27/60\n",
            "130/130 - 0s - loss: 0.5359 - mae: 0.5503\n",
            "Epoch 28/60\n",
            "130/130 - 0s - loss: 0.5353 - mae: 0.5492\n",
            "Epoch 29/60\n",
            "130/130 - 0s - loss: 0.5410 - mae: 0.5509\n",
            "Epoch 30/60\n",
            "130/130 - 0s - loss: 0.5303 - mae: 0.5479\n",
            "Epoch 31/60\n",
            "130/130 - 0s - loss: 0.5343 - mae: 0.5473\n",
            "Epoch 32/60\n",
            "130/130 - 0s - loss: 0.5233 - mae: 0.5393\n",
            "Epoch 33/60\n",
            "130/130 - 0s - loss: 0.5285 - mae: 0.5432\n",
            "Epoch 34/60\n",
            "130/130 - 0s - loss: 0.5068 - mae: 0.5350\n",
            "Epoch 35/60\n",
            "130/130 - 0s - loss: 0.5070 - mae: 0.5320\n",
            "Epoch 36/60\n",
            "130/130 - 0s - loss: 0.5138 - mae: 0.5363\n",
            "Epoch 37/60\n",
            "130/130 - 0s - loss: 0.5133 - mae: 0.5336\n",
            "Epoch 38/60\n",
            "130/130 - 0s - loss: 0.4973 - mae: 0.5278\n",
            "Epoch 39/60\n",
            "130/130 - 0s - loss: 0.5015 - mae: 0.5278\n",
            "Epoch 40/60\n",
            "130/130 - 0s - loss: 0.4837 - mae: 0.5169\n",
            "Epoch 41/60\n",
            "130/130 - 0s - loss: 0.5081 - mae: 0.5305\n",
            "Epoch 42/60\n",
            "130/130 - 0s - loss: 0.4868 - mae: 0.5215\n",
            "Epoch 43/60\n",
            "130/130 - 0s - loss: 0.5061 - mae: 0.5250\n",
            "Epoch 44/60\n",
            "130/130 - 0s - loss: 0.4727 - mae: 0.5119\n",
            "Epoch 45/60\n",
            "130/130 - 0s - loss: 0.4738 - mae: 0.5127\n",
            "Epoch 46/60\n",
            "130/130 - 0s - loss: 0.4769 - mae: 0.5135\n",
            "Epoch 47/60\n",
            "130/130 - 0s - loss: 0.4781 - mae: 0.5106\n",
            "Epoch 48/60\n",
            "130/130 - 0s - loss: 0.4800 - mae: 0.5143\n",
            "Epoch 49/60\n",
            "130/130 - 0s - loss: 0.4761 - mae: 0.5112\n",
            "Epoch 50/60\n",
            "130/130 - 0s - loss: 0.4832 - mae: 0.5097\n",
            "Epoch 51/60\n",
            "130/130 - 0s - loss: 0.4674 - mae: 0.5100\n",
            "Epoch 52/60\n",
            "130/130 - 0s - loss: 0.4638 - mae: 0.5062\n",
            "Epoch 53/60\n",
            "130/130 - 0s - loss: 0.4772 - mae: 0.5142\n",
            "Epoch 54/60\n",
            "130/130 - 0s - loss: 0.4536 - mae: 0.5023\n",
            "Epoch 55/60\n",
            "130/130 - 0s - loss: 0.4792 - mae: 0.5155\n",
            "Epoch 56/60\n",
            "130/130 - 0s - loss: 0.4681 - mae: 0.5071\n",
            "Epoch 57/60\n",
            "130/130 - 0s - loss: 0.4638 - mae: 0.5081\n",
            "Epoch 58/60\n",
            "130/130 - 0s - loss: 0.4463 - mae: 0.5012\n",
            "Epoch 59/60\n",
            "130/130 - 0s - loss: 0.4447 - mae: 0.4974\n",
            "Epoch 60/60\n",
            "130/130 - 0s - loss: 0.4744 - mae: 0.5103\n",
            "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fa187b08f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "[0.0, 0.0, 0.0]\n",
            "[0.0, 0.0, 0.0]\n",
            "[0.5030819, 0.62590575, 0.5497066]\n",
            "[0.5030819, 0.62590575, 0.5497066]\n",
            "[0.5237349420785904, 0.6776718944311142, 0.5986138135194778]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VFgl2kO1mhRM"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "23pDEMZJzcbC"
      },
      "source": [
        "PD.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hiK38H1wt2KX"
      },
      "source": [
        "np.floor(X_train.shape[0]/32).astype('int')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MwQyV8fH9Ykz"
      },
      "source": [
        "y_hat[1].mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h2e2rVyEZZqy"
      },
      "source": [
        "from tensorflow.keras.models import model_from_json,load_model\n",
        "# model= load_model('/content/gdrive/My Drive/models/best_model_11_10.h5')\n",
        "# tensorflow.keras.utils.plot_model(model,show_shapes=True)\n",
        "tensorflow.keras.models.save_model(model,\"model.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FyEmdw9pF03B"
      },
      "source": [
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='upper right')\n",
        "plt.savefig('Training.jpeg')\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f4S-bZ63F03E"
      },
      "source": [
        "mses=np.array(df3)\n",
        "mses[:,:1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dc7hnd3MKV-6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "944ce98f-e36b-4f03-b9b6-73edc411c9ff"
      },
      "source": [
        "df_tot['Subject_id'].unique()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['Leontios', 'SDE 43', 'SGR07', 'SGR16', 'SGR26', 'SGR19', 'SGR33',\n",
              "       'SGR30', 'SUK01', 'SGR06', 'SDE08', 'SGR28', 'SGR27', 'SDE09',\n",
              "       'SGR10', 'SGR04', 'SUK17', 'SUK28', 'SUK36', 'SGR05', 'SGR32',\n",
              "       'SDE39', 'SGR34', 'SUK39', 'SUK03', 'SUK13', 'SDE14'], dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I-7fd4x1KWFJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5049557f-c329-43b3-9872-bb8bf0a5806b"
      },
      "source": [
        "for each in df2['Subject_id'].unique() :\n",
        "  print(data[['ids','UPDRS_22','UPDRS_23',' UPDRS_31_E1_1_C1 ','Sex']][(data['Study Subject ID']==each)])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                  ids  UPDRS_22  UPDRS_23   UPDRS_31_E1_1_C1  Sex\n",
            "131  11a9e65564567d48         0         0                   0   m\n",
            "                  ids  UPDRS_22  UPDRS_23   UPDRS_31_E1_1_C1  Sex\n",
            "122  15963826e88041df         0         0                   0   f\n",
            "                 ids  UPDRS_22  UPDRS_23   UPDRS_31_E1_1_C1  Sex\n",
            "44  15d907c726d2fc44         3         3                   1   m\n",
            "                 ids  UPDRS_22  UPDRS_23   UPDRS_31_E1_1_C1  Sex\n",
            "54  20b6e7c74b9bdf92         1         1                   0   m\n",
            "                 ids  UPDRS_22  UPDRS_23   UPDRS_31_E1_1_C1  Sex\n",
            "64  2be1fe7ec81e1b5e         4         4                   1   m\n",
            "                 ids  UPDRS_22  UPDRS_23   UPDRS_31_E1_1_C1  Sex\n",
            "57  393ac67b9a76136f         0         0                   1   m\n",
            "                 ids  UPDRS_22  UPDRS_23   UPDRS_31_E1_1_C1  Sex\n",
            "72  589df5bb325258fc         0         0                   0   m\n",
            "73  4da7ff8898cbeef0         0         0                   0   m\n",
            "                 ids  UPDRS_22  UPDRS_23   UPDRS_31_E1_1_C1  Sex\n",
            "69  563f2d62693960d7         2         1                   1   m\n",
            "                ids  UPDRS_22  UPDRS_23   UPDRS_31_E1_1_C1  Sex\n",
            "0  6cc41389d3e9aea9         0         1                   1   f\n",
            "1  a126df5f5a22ea26         0         1                   1   f\n",
            "                 ids  UPDRS_22  UPDRS_23   UPDRS_31_E1_1_C1  Sex\n",
            "58  789dcfde8e85be2a         7         5                   2   m\n",
            "                 ids  UPDRS_22  UPDRS_23   UPDRS_31_E1_1_C1  Sex\n",
            "43  7e0fdab563d7c53e         2         3                   1   f\n",
            "                 ids  UPDRS_22  UPDRS_23   UPDRS_31_E1_1_C1  Sex\n",
            "83  8007485258c172ad         0         3                   2   m\n",
            "                 ids  UPDRS_22  UPDRS_23   UPDRS_31_E1_1_C1  Sex\n",
            "66  8b0c496cc31f42fa         2         1                   1   m\n",
            "                 ids  UPDRS_22  UPDRS_23   UPDRS_31_E1_1_C1  Sex\n",
            "65  8e4c0f24697212cc         2         2                   1   m\n",
            "                 ids  UPDRS_22  UPDRS_23   UPDRS_31_E1_1_C1  Sex\n",
            "84  8e6806515fadb022         0         0                   1   m\n",
            "                 ids  UPDRS_22  UPDRS_23   UPDRS_31_E1_1_C1  Sex\n",
            "47  95f9756b35b97753         4         4                   1   m\n",
            "48  7131688875f694d4         4         4                   1   m\n",
            "                 ids  UPDRS_22  UPDRS_23   UPDRS_31_E1_1_C1  Sex\n",
            "41  a0433fa95436cefb         0         0                   0   m\n",
            "                 ids  UPDRS_22  UPDRS_23   UPDRS_31_E1_1_C1  Sex\n",
            "20  a83230cbb772f842         2         1                   1   f\n",
            "                 ids  UPDRS_22  UPDRS_23   UPDRS_31_E1_1_C1  Sex\n",
            "28  bcb2a210329fee87         0         0                   0   f\n",
            "29  aee08aa3c44d0d42         0         0                   0   f\n",
            "                 ids  UPDRS_22  UPDRS_23   UPDRS_31_E1_1_C1  Sex\n",
            "33  bc4b9313325e92c5         0         0                   0   f\n",
            "                 ids  UPDRS_22  UPDRS_23   UPDRS_31_E1_1_C1  Sex\n",
            "42  c088efdfa6a17943         0         0                   0   m\n",
            "                 ids  UPDRS_22  UPDRS_23   UPDRS_31_E1_1_C1  Sex\n",
            "71  d36c1ba7102c7c7d         5         5                   2   m\n",
            "                  ids  UPDRS_22  UPDRS_23   UPDRS_31_E1_1_C1  Sex\n",
            "117  d9b435ae999d5003         0         0                   0   f\n",
            "                 ids  UPDRS_22  UPDRS_23   UPDRS_31_E1_1_C1  Sex\n",
            "74  ed526ce9125e22b4         0         0                   0   m\n",
            "                 ids  UPDRS_22  UPDRS_23   UPDRS_31_E1_1_C1  Sex\n",
            "37  ef4837696485ad24         1         1                   1   m\n",
            "                ids  UPDRS_22  UPDRS_23   UPDRS_31_E1_1_C1  Sex\n",
            "6  f1b42729314b6f9b         3         1                   1   m\n",
            "                 ids  UPDRS_22  UPDRS_23   UPDRS_31_E1_1_C1  Sex\n",
            "16  f96144638dcc1018         1         0                   0   m\n",
            "                 ids  UPDRS_22  UPDRS_23   UPDRS_31_E1_1_C1  Sex\n",
            "91  fa1ca92b6d69c880         0         0                   1   m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R_YZh3DvY4gG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        },
        "outputId": "b8841db0-7920-4e14-ad32-a922a81e725f"
      },
      "source": [
        "data[data['ids']=='789dcfde8e85be2a']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Study Subject ID</th>\n",
              "      <th>Protocol ID</th>\n",
              "      <th>ids</th>\n",
              "      <th>Subject Status</th>\n",
              "      <th>Sex</th>\n",
              "      <th>UPDRS_22_UEXTR_RIGHT_E1_1_C1</th>\n",
              "      <th>UPDRS_22_UEXTR_LEFT_E1_1_C1</th>\n",
              "      <th>UPDRS_23_RIGHT_E1_1_C1</th>\n",
              "      <th>UPDRS_23_LEFT_E1_1_C1</th>\n",
              "      <th>UPDRS_31_E1_1_C1</th>\n",
              "      <th>SUM_PART_3_E1_1_C1</th>\n",
              "      <th>UPDRS_22</th>\n",
              "      <th>UPDRS_23</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>58</th>\n",
              "      <td>SGR20</td>\n",
              "      <td>iprognosis-sdata-study - 401/31.01.2018</td>\n",
              "      <td>789dcfde8e85be2a</td>\n",
              "      <td>available</td>\n",
              "      <td>m</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>62</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Study Subject ID  ... UPDRS_23\n",
              "58            SGR20  ...        5\n",
              "\n",
              "[1 rows x 13 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 215
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3tU1rc3CF03G"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "%matplotlib inline\n",
        "# bins =np.arange(0, 6, 0.25)\n",
        "# ax = sns.distplot(scores)\n",
        "\n",
        "# plt.style.use('ggplot')\n",
        "# plt.hist(scores, bins,Density=True)  # `density=False` would make counts\n",
        "# plt.ylabel('')\n",
        "# plt.xlabel('MSE');\n",
        "# plt.xticks(range(0, 7))\n",
        "# plt.tight_layout()\n",
        "# plt.show()\n",
        "# plt.xticks(range(0, 7))\n",
        "# ax=sns.distplot(mses[:,2:3], hist=True, kde=True, \n",
        "#              bins=bins, \n",
        "#              hist_kws={'edgecolor':'black'},\n",
        "#              kde_kws={'linewidth': 4,\"label\": \"KDE\"})\n",
        "# ax.set_xlabel('MSE UPDRS31', size = 15)\n",
        "\n",
        "# figure = ax.get_figure()    \n",
        "# figure.savefig('UPDRS31cnn16.png', dpi=400)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ap3_p8ZAF03K"
      },
      "source": [
        "ax=sns.distplot(mses[:,0:1], hist=True, kde=False, \n",
        "             bins=bins, \n",
        "             hist_kws={'edgecolor':'black'},\n",
        "             kde_kws={'linewidth': 4})\n",
        "ax.set_ylabel('Subjects', size =15)\n",
        "ax.set_xlabel('MSE UPDRS22', size = 15)\n",
        "figure = ax.get_figure()    \n",
        "figure.savefig('MSE_UPDRS22_cnn16_no_pdf.png', dpi=400)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iQ2jZNU6F03N"
      },
      "source": [
        "import csv\n",
        "temp=['UPDRS22_Predicted','UPDRS23_Predicted','UPDRS31_Predicted']\n",
        "with open('/content/gdrive/My Drive/montelo1.csv', 'w') as csv_file:\n",
        "    csv_writer = csv.writer(csv_file, delimiter=',')\n",
        "    csv_writer.writerow(temp )\n",
        "    for i in range (0,27):\n",
        "        csv_writer.writerow(scores4[i])\n",
        "        \n",
        "    \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Wu5iawmF03R"
      },
      "source": [
        "import csv\n",
        "temp=['UPDRS22_Predicted','UPDRS23_Predicted','UPDRS31_Predicted']\n",
        "with open('/content/gdrive/My Drive/montelo10_3.csv', 'w') as csv_file:\n",
        "    csv_writer = csv.writer(csv_file, delimiter=',')\n",
        "    csv_writer.writerow(temp )\n",
        "    for i in range (0,27):\n",
        "        csv_writer.writerow(scores3[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hHi4db6QF03U"
      },
      "source": [
        "temp=['UPDRS22_Predicted','UPDRS23_Predicted','UPDRS31_Predicted']\n",
        "with open('/content/gdrive/My Drive/montelo1.csv', 'w') as csv_file:\n",
        "    csv_writer = csv.writer(csv_file, delimiter=',')\n",
        "    csv_writer.writerow(temp )\n",
        "    for i in range (0,27):\n",
        "        csv_writer.writerow(scores2[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FiBoVEsjF03W"
      },
      "source": [
        "temp=['UPDRS22_True','UPDRS23_True','UPDRS31_True']\n",
        "with open('/content/gdrive/My Drive/montelo10_1.csv', 'w') as csv_file:\n",
        "    csv_writer = csv.writer(csv_file, delimiter=',')\n",
        "    csv_writer.writerow(temp )\n",
        "    for i in range (0,27):\n",
        "        csv_writer.writerow(scores1[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sgQKdrE_5pP9"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FYx1Pq45a78O"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kpNX_gYNiOH1"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lnq-p2jDF03a"
      },
      "source": [
        "df7=pd.read_csv('/content/gdrive/My Drive/montelo10_3.csv')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cf2iiq59F03c"
      },
      "source": [
        "df8=pd.read_csv('/content/gdrive/My Drive/montelo10_1.csv')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JoG32DKhF03f"
      },
      "source": [
        "df9= pd.merge(df7, df8,left_index=True, right_index=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j5xlZ9jMF03i",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 873
        },
        "outputId": "fd26d172-81cc-42d8-ebb3-b70a0366470c"
      },
      "source": [
        "\n",
        "\n",
        "# rc={'figure.figsize':(15,15)}\n",
        "\n",
        "# print(corr)\n",
        "sns.set(rc={'figure.figsize':(13,13)})\n",
        "\n",
        "corrMatrix = df9.corr()\n",
        "ax=sns.heatmap(corrMatrix, annot=True,vmin=0, vmax=1, center=0.5,cmap= 'coolwarm')\n",
        "ax.set_title('Median')\n",
        "figure = ax.get_figure()    \n",
        "# figure.savefig('/content/gdrive/My Drive/Corr_Matrix_cnn100_median_51_DEEP.png', dpi=400)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 936x936 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9sXUi8YcV1dv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 77
        },
        "outputId": "9714d738-45ae-4b88-9974-c90e5cca2197"
      },
      "source": [
        "df9.corr().iloc[[3,4,5],[0,1,2]].iloc[[0],[0]]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>UPDRS22_Predicted</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>UPDRS22_True</th>\n",
              "      <td>0.422946</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              UPDRS22_Predicted\n",
              "UPDRS22_True           0.422946"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 144
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-D7pxwSTZ2k3"
      },
      "source": [
        "df10=pd.DataFrame()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yed7GiVGbiRZ"
      },
      "source": [
        "df7=pd.read_csv('/content/gdrive/My Drive/montelo10_3.csv')\n",
        "df8=pd.read_csv('/content/gdrive/My Drive/montelo10_1.csv')\n",
        "df9= pd.merge(df7, df8,left_index=True, right_index=True)\n",
        "df10=df10.append([df9.corr().iloc[[3,4,5],[0,1,2]].iloc[[0],[0]],df9.corr().iloc[[3,4,5],[0,1,2]].iloc[[1],[1]],df9.corr().iloc[[3,4,5],[0,1,2]].iloc[[2],[2]]],ignore_index=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pr1kBikkb3GU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "44487df0-fbd4-4cc9-c242-1c6f2c1c3407"
      },
      "source": [
        "df10.dropna()\n",
        "df10.std()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "UPDRS22_Predicted    0.081331\n",
              "UPDRS23_Predicted    0.044674\n",
              "UPDRS31_Predicted    0.036827\n",
              "dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 194
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ySJygWXFXJ1h"
      },
      "source": [
        "\n",
        "\n",
        "df7=pd.read_csv('/content/gdrive/My Drive/montelo10_3.csv')\n",
        "df8=pd.read_csv('/content/gdrive/My Drive/montelo10_1.csv')\n",
        "df9= pd.merge(df7, df8,left_index=True, right_index=True)\n",
        "sum1=sum1+df9.corr().iloc[[3,4,5],[0,1,2]].iloc[[0],[0]]\n",
        "sum2=sum2+df9.corr().iloc[[3,4,5],[0,1,2]].iloc[[1],[1]]\n",
        "sum3=sum3+df9.corr().iloc[[3,4,5],[0,1,2]].iloc[[2],[2]]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "do_JV5YmaQEV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "d2078b20-5c67-4ce5-e5cb-6f1492329255"
      },
      "source": [
        "print(sum3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              UPDRS31_Predicted\n",
            "UPDRS31_True           4.932839\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WE9qGKRHZG0U",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        },
        "outputId": "2de6c03e-47b6-4194-8b31-f6c312c0c83e"
      },
      "source": [
        "sum1=sum1/10.0\n",
        "sum2=sum2/10.0\n",
        "sum3=sum3/10.0\n",
        "\n",
        "print(sum1)\n",
        "print(sum2)\n",
        "print(sum3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              UPDRS22_Predicted\n",
            "UPDRS22_True           0.475234\n",
            "              UPDRS23_Predicted\n",
            "UPDRS23_True           0.759305\n",
            "              UPDRS31_Predicted\n",
            "UPDRS31_True           0.493284\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CjnSLG-fF03l",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 790
        },
        "outputId": "a9b67cc0-e04d-434e-ba3c-f32f2f4f278f"
      },
      "source": [
        "df9.sort_values(by=['UPDRS23_True'],inplace=True)\n",
        "x=np.arange(1,28, 1)\n",
        "ax = plt.gca()\n",
        "ax.scatter(x,np.array(df9['UPDRS23_Predicted']),color='b',marker=\"o\",label='Predicted')\n",
        "ax.scatter(x,np.array(df9['UPDRS23_True']),x,color='r',marker=\"o\",label='True')\n",
        "ax.set_xlabel('Subject')\n",
        "ax.set_title('Median')\n",
        "ax.set_ylabel('UPDRS23 score')\n",
        "plt.legend(loc='upper left');\n",
        "figure = ax.get_figure()    \n",
        "# figure.savefig('/content/gdrive/My Drive/Scatter_plot_Median_UPDRS31_cnn100_51_DEEP.png', dpi=400)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 936x936 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z_gtK_x-F03n"
      },
      "source": [
        "\n",
        "data_dem[\"ID\"].unique().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eog0lf0YF03p",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a0c55d6e-5e63-411b-ce75-3dd6ee3d6b3c"
      },
      "source": [
        "import math \n",
        "import glob\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "data_full=pd.DataFrame()\n",
        "data2=pd.DataFrame()\n",
        "data3=pd.DataFrame()\n",
        "data4=pd.DataFrame()\n",
        "data5=pd.DataFrame()\n",
        "b=[]\n",
        "# get all y.txt files from all subdirectories\n",
        "ID=[]\n",
        "\n",
        "all_files = glob.glob('/content/gdrive/My Drive/thesis/Data/S*/*.txt')\n",
        "\n",
        "dimension=100\n",
        "\n",
        "# widths=widths\n",
        "i=0\n",
        "for file in all_files :\n",
        "     print(file)\n",
        "     print(int(file[-12:-10]))\n",
        "          \n",
        "     temp = pd.read_csv(file,sep=' |,', engine='python', names = ['Press','Value1','Release','Value2','Np'],skiprows=1) \n",
        "#     print(temp)\n",
        "#     temp['Release'] = temp['Release'].map(lambda x: x.lstrip('+-').rstrip('aAbBcC'))\n",
        "#     print(temp)\n",
        "#     print(temp['Value1'])\n",
        "     X_flight= np.diff(sorted(temp['Value1']/1000.0))\n",
        "    \n",
        "#     print(X_flight)\n",
        "     print(X_flight)\n",
        "\n",
        "    # X_flight = np.where(~np.isnan(X_flight), X_flight, 0)\n",
        "#     print(X_flight)\n",
        "\n",
        "     X_flight=np.where(X_flight< 3, X_flight , 0)\n",
        "\n",
        "\n",
        "     print(X_flight)\n",
        "#     print(X_flight)\n",
        "     X_hold= (temp['Value2'] - temp['Value1'])/1000.0\n",
        "   \n",
        "#     X_hold= np.where(~np.isnan(X_hold), X_hold,float(0) )\n",
        "\n",
        "     X_hold = np.where(X_hold < 0.7 , X_hold , np.median(X_hold) )\n",
        "#     print(X_hold)\n",
        "\n",
        "#     print(type(X_hold))\n",
        "\n",
        "\n",
        "     if len(X_flight) < dimension:\n",
        "\n",
        "          X_flight = np.pad(X_flight, (math.floor((dimension - len(X_flight)) / 2),math.ceil ((dimension + 1 - len(X_flight)) / 2)), 'constant', constant_values=(0,0))\n",
        "          X_hold = np.pad(X_hold,(math.floor((dimension - len(X_hold)) / 2), math.ceil((dimension + 1 - len(X_hold)) / 2)), 'constant', constant_values= (0,0))\n",
        "\n",
        "     else:\n",
        "         X_flight=X_flight[:dimension]\n",
        "         X_hold=X_hold[:dimension]\n",
        "     X_flight=X_flight[:100]\n",
        "     print(X_flight)\n",
        "     print(X_hold)\n",
        "     X_hold=X_hold[:100]\n",
        "     print(X_flight.shape)\n",
        "     a=(int(file[-12:-10])) \n",
        "     print(type(a))    \n",
        "     df33 = pd.DataFrame({ \"ID\" : a },index=[i])\n",
        "\n",
        "     df11 = pd.DataFrame({\"X_hold\": [X_hold]},index=[i])\n",
        "\n",
        "     data3=pd.concat([data3,df11],axis=0,ignore_index=False)\n",
        "     df22 = pd.DataFrame({\"X_flight\": [X_flight]},index=[i])\n",
        "\n",
        "    \n",
        "     data4=pd.concat([data4,df22],axis=0,ignore_index=False)\n",
        "     data5=pd.concat([data5,df33],axis=0,ignore_index=False) \n",
        "     i=i+1\n",
        "     print(i)\n",
        "data2=pd.concat([data4,data3],axis=1)\n",
        "data_full=pd.concat([data2,data5],axis=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            " 1.877 0.707 0.991 1.646 0.687 2.182 0.42  0.951 1.019 0.241 2.176 0.723\n",
            " 1.982 0.42  0.657 0.776 1.02  0.723 1.016]\n",
            "[0.906 1.018 1.003 1.828 0.346 0.648 0.557 0.    0.541 0.737 0.465 0.576\n",
            " 1.055 1.547 2.202 0.553 0.505 1.108 0.762 1.57  0.71  0.722 0.925 2.886\n",
            " 1.379 0.638 0.538 0.498 0.551 2.502 0.623 2.312 1.105 0.829 0.59  0.583\n",
            " 1.877 0.707 0.991 1.646 0.687 2.182 0.42  0.951 1.019 0.241 2.176 0.723\n",
            " 1.982 0.42  0.657 0.776 1.02  0.723 1.016]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.906 1.018\n",
            " 1.003 1.828 0.346 0.648 0.557 0.    0.541 0.737 0.465 0.576 1.055 1.547\n",
            " 2.202 0.553 0.505 1.108 0.762 1.57  0.71  0.722 0.925 2.886 1.379 0.638\n",
            " 0.538 0.498 0.551 2.502 0.623 2.312 1.105 0.829 0.59  0.583 1.877 0.707\n",
            " 0.991 1.646 0.687 2.182 0.42  0.951 1.019 0.241 2.176 0.723 1.982 0.42\n",
            " 0.657 0.776 1.02  0.723 1.016 0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.   ]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.129 0.121\n",
            " 0.132 0.095 0.128 0.156 0.107 0.072 0.097 0.12  0.107 0.124 0.114 0.122\n",
            " 0.12  0.121 0.106 0.117 0.131 0.134 0.11  0.156 0.141 0.097 0.18  0.129\n",
            " 0.155 0.14  0.133 0.137 0.147 0.139 0.147 0.139 0.141 0.107 0.131 0.116\n",
            " 0.123 0.105 0.139 0.175 0.112 0.115 0.12  0.099 0.108 0.186 0.133 0.179\n",
            " 0.167 0.091 0.138 0.148 0.099 0.097 0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.   ]\n",
            "(100,)\n",
            "<class 'int'>\n",
            "165\n",
            "/content/gdrive/My Drive/thesis/Data/S15/S15_TEX06.txt\n",
            "15\n",
            "[0.712 0.541 1.124 0.598 1.181 2.329 0.579 0.555 0.6   1.414 2.394 1.655\n",
            " 0.596 1.123 0.396 1.307 0.688 2.514 1.244 0.859 0.511 0.948 1.171 0.291\n",
            " 0.932 0.498 0.907 0.78  1.117 0.511 1.012 0.675 0.691 1.05  1.271 0.781\n",
            " 2.131 0.746 1.709 0.902 0.537 2.545 1.494 0.801 0.529 2.058 0.62  0.68\n",
            " 0.466 0.893 0.604 0.787 1.189 1.207 2.146 0.553 0.692]\n",
            "[0.712 0.541 1.124 0.598 1.181 2.329 0.579 0.555 0.6   1.414 2.394 1.655\n",
            " 0.596 1.123 0.396 1.307 0.688 2.514 1.244 0.859 0.511 0.948 1.171 0.291\n",
            " 0.932 0.498 0.907 0.78  1.117 0.511 1.012 0.675 0.691 1.05  1.271 0.781\n",
            " 2.131 0.746 1.709 0.902 0.537 2.545 1.494 0.801 0.529 2.058 0.62  0.68\n",
            " 0.466 0.893 0.604 0.787 1.189 1.207 2.146 0.553 0.692]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.712 0.541 1.124\n",
            " 0.598 1.181 2.329 0.579 0.555 0.6   1.414 2.394 1.655 0.596 1.123 0.396\n",
            " 1.307 0.688 2.514 1.244 0.859 0.511 0.948 1.171 0.291 0.932 0.498 0.907\n",
            " 0.78  1.117 0.511 1.012 0.675 0.691 1.05  1.271 0.781 2.131 0.746 1.709\n",
            " 0.902 0.537 2.545 1.494 0.801 0.529 2.058 0.62  0.68  0.466 0.893 0.604\n",
            " 0.787 1.189 1.207 2.146 0.553 0.692 0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.   ]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.163 0.125 0.14\n",
            " 0.139 0.13  0.104 0.137 0.131 0.142 0.133 0.138 0.162 0.096 0.123 0.113\n",
            " 0.125 0.122 0.123 0.152 0.149 0.102 0.123 0.106 0.116 0.25  0.131 0.149\n",
            " 0.114 0.149 0.145 0.124 0.143 0.15  0.141 0.139 0.156 0.139 0.137 0.155\n",
            " 0.149 0.128 0.165 0.122 0.106 0.153 0.166 0.128 0.148 0.157 0.19  0.154\n",
            " 0.162 0.158 0.157 0.123 0.113 0.133 0.131 0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.   ]\n",
            "(100,)\n",
            "<class 'int'>\n",
            "166\n",
            "/content/gdrive/My Drive/thesis/Data/S15/S15_TEX10.txt\n",
            "15\n",
            "[0.802 0.731 1.158 0.695 0.582 0.574 0.533 2.436 0.322 1.017 0.428 0.727\n",
            " 0.972 1.082 0.822 0.832 0.445 0.574 2.257 0.619 2.17  0.798 0.706 0.632\n",
            " 0.601 0.748 0.414 0.741 0.447 0.3   0.424 2.178 0.598 0.931 0.417 0.999\n",
            " 0.571 0.658 0.512 0.904 0.711 0.614 2.29  0.511 0.35  3.565 1.284 0.661\n",
            " 1.491 1.371 0.679 0.548 2.515 0.313 1.858 0.421 1.123 0.62  2.657 1.461\n",
            " 0.523]\n",
            "[0.802 0.731 1.158 0.695 0.582 0.574 0.533 2.436 0.322 1.017 0.428 0.727\n",
            " 0.972 1.082 0.822 0.832 0.445 0.574 2.257 0.619 2.17  0.798 0.706 0.632\n",
            " 0.601 0.748 0.414 0.741 0.447 0.3   0.424 2.178 0.598 0.931 0.417 0.999\n",
            " 0.571 0.658 0.512 0.904 0.711 0.614 2.29  0.511 0.35  0.    1.284 0.661\n",
            " 1.491 1.371 0.679 0.548 2.515 0.313 1.858 0.421 1.123 0.62  2.657 1.461\n",
            " 0.523]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.802 0.731 1.158 0.695 0.582\n",
            " 0.574 0.533 2.436 0.322 1.017 0.428 0.727 0.972 1.082 0.822 0.832 0.445\n",
            " 0.574 2.257 0.619 2.17  0.798 0.706 0.632 0.601 0.748 0.414 0.741 0.447\n",
            " 0.3   0.424 2.178 0.598 0.931 0.417 0.999 0.571 0.658 0.512 0.904 0.711\n",
            " 0.614 2.29  0.511 0.35  0.    1.284 0.661 1.491 1.371 0.679 0.548 2.515\n",
            " 0.313 1.858 0.421 1.123 0.62  2.657 1.461 0.523 0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.   ]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.161 0.116 0.141 0.087 0.116\n",
            " 0.099 0.124 0.123 0.114 0.098 0.12  0.183 0.104 0.114 0.12  0.146 0.137\n",
            " 0.157 0.147 0.16  0.149 0.182 0.14  0.133 0.133 0.147 0.112 0.124 0.106\n",
            " 0.149 0.115 0.131 0.14  0.148 0.14  0.197 0.161 0.165 0.162 0.166 0.119\n",
            " 0.149 0.149 0.152 0.165 0.156 0.191 0.137 0.241 0.17  0.172 0.15  0.141\n",
            " 0.137 0.132 0.128 0.139 0.146 0.166 0.128 0.081 0.114 0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.   ]\n",
            "(100,)\n",
            "<class 'int'>\n",
            "167\n",
            "/content/gdrive/My Drive/thesis/Data/S15/S15_TEX09.txt\n",
            "15\n",
            "[0.771 0.456 0.473 0.76  0.531 0.704 0.349 0.807 0.563 2.122 0.563 0.282\n",
            " 1.176 0.295 1.13  0.54  2.979 0.577 0.683 1.28  0.463 0.558 0.414 1.648\n",
            " 0.916 1.184 0.434 3.748 0.556 0.558 0.866 0.463 2.662 0.784 0.84  0.34\n",
            " 0.409 1.083 0.521 0.849 1.148 0.669 0.689 0.625 0.573 0.959 1.102 2.112\n",
            " 0.72  0.747 0.366 0.936 0.603 5.794 1.169 3.427 0.836 0.597 0.582 0.865\n",
            " 0.334 0.873 0.67  0.818 0.321 1.997 1.14  2.475 0.56  1.001 0.954 1.813\n",
            " 1.538 0.462 0.833 0.579 0.382]\n",
            "[0.771 0.456 0.473 0.76  0.531 0.704 0.349 0.807 0.563 2.122 0.563 0.282\n",
            " 1.176 0.295 1.13  0.54  2.979 0.577 0.683 1.28  0.463 0.558 0.414 1.648\n",
            " 0.916 1.184 0.434 0.    0.556 0.558 0.866 0.463 2.662 0.784 0.84  0.34\n",
            " 0.409 1.083 0.521 0.849 1.148 0.669 0.689 0.625 0.573 0.959 1.102 2.112\n",
            " 0.72  0.747 0.366 0.936 0.603 0.    1.169 0.    0.836 0.597 0.582 0.865\n",
            " 0.334 0.873 0.67  0.818 0.321 1.997 1.14  2.475 0.56  1.001 0.954 1.813\n",
            " 1.538 0.462 0.833 0.579 0.382]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.771\n",
            " 0.456 0.473 0.76  0.531 0.704 0.349 0.807 0.563 2.122 0.563 0.282 1.176\n",
            " 0.295 1.13  0.54  2.979 0.577 0.683 1.28  0.463 0.558 0.414 1.648 0.916\n",
            " 1.184 0.434 0.    0.556 0.558 0.866 0.463 2.662 0.784 0.84  0.34  0.409\n",
            " 1.083 0.521 0.849 1.148 0.669 0.689 0.625 0.573 0.959 1.102 2.112 0.72\n",
            " 0.747 0.366 0.936 0.603 0.    1.169 0.    0.836 0.597 0.582 0.865 0.334\n",
            " 0.873 0.67  0.818 0.321 1.997 1.14  2.475 0.56  1.001 0.954 1.813 1.538\n",
            " 0.462 0.833 0.579 0.382 0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.   ]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.155\n",
            " 0.123 0.149 0.173 0.111 0.138 0.158 0.181 0.123 0.198 0.13  0.157 0.19\n",
            " 0.129 0.124 0.124 0.166 0.127 0.149 0.13  0.121 0.125 0.115 0.15  0.147\n",
            " 0.12  0.142 0.155 0.122 0.141 0.147 0.129 0.146 0.127 0.124 0.149 0.208\n",
            " 0.139 0.113 0.081 0.146 0.136 0.149 0.166 0.198 0.148 0.188 0.138 0.137\n",
            " 0.133 0.125 0.115 0.153 0.114 0.154 0.172 0.161 0.106 0.141 0.14  0.116\n",
            " 0.105 0.147 0.116 0.12  0.141 0.153 0.109 0.103 0.158 0.129 0.122 0.195\n",
            " 0.12  0.139 0.146 0.139 0.106 0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.   ]\n",
            "(100,)\n",
            "<class 'int'>\n",
            "168\n",
            "/content/gdrive/My Drive/thesis/Data/S15/S15_TEX03.txt\n",
            "15\n",
            "[1.033 1.338 0.689 0.662 0.466 0.676 2.086 0.605 0.331 2.066 1.138 1.952\n",
            " 0.518 1.477 0.538 0.926 0.863 0.979 0.669 0.594 0.931 0.738 1.691 0.318\n",
            " 0.832 1.981 1.155 1.003 0.798 1.35  0.488 1.582 0.595 0.565 0.549 2.422\n",
            " 0.412 0.666 0.49  3.059 1.08  0.517 1.136 0.792 0.44  0.983 0.593 1.453\n",
            " 0.66  0.756 2.535 0.624 1.439 0.747 0.613 1.281 1.05  3.171 0.359 0.541\n",
            " 0.512 0.634 3.684 0.736 0.665 1.489 0.43  1.076 0.579 0.78  0.624 0.649\n",
            " 1.763 1.187 0.429 0.626 0.948 0.38  0.843 1.02  0.515 0.68  0.708 4.09\n",
            " 0.522 0.623 1.19  0.547 0.714 3.012 0.687 0.266 1.092 0.997 0.504 0.81\n",
            " 0.864 0.504 3.371 0.718 0.615 0.609 0.848 0.554 0.758 1.324 0.378 1.001\n",
            " 1.372 0.528 0.299]\n",
            "[1.033 1.338 0.689 0.662 0.466 0.676 2.086 0.605 0.331 2.066 1.138 1.952\n",
            " 0.518 1.477 0.538 0.926 0.863 0.979 0.669 0.594 0.931 0.738 1.691 0.318\n",
            " 0.832 1.981 1.155 1.003 0.798 1.35  0.488 1.582 0.595 0.565 0.549 2.422\n",
            " 0.412 0.666 0.49  0.    1.08  0.517 1.136 0.792 0.44  0.983 0.593 1.453\n",
            " 0.66  0.756 2.535 0.624 1.439 0.747 0.613 1.281 1.05  0.    0.359 0.541\n",
            " 0.512 0.634 0.    0.736 0.665 1.489 0.43  1.076 0.579 0.78  0.624 0.649\n",
            " 1.763 1.187 0.429 0.626 0.948 0.38  0.843 1.02  0.515 0.68  0.708 0.\n",
            " 0.522 0.623 1.19  0.547 0.714 0.    0.687 0.266 1.092 0.997 0.504 0.81\n",
            " 0.864 0.504 0.    0.718 0.615 0.609 0.848 0.554 0.758 1.324 0.378 1.001\n",
            " 1.372 0.528 0.299]\n",
            "[1.033 1.338 0.689 0.662 0.466 0.676 2.086 0.605 0.331 2.066 1.138 1.952\n",
            " 0.518 1.477 0.538 0.926 0.863 0.979 0.669 0.594 0.931 0.738 1.691 0.318\n",
            " 0.832 1.981 1.155 1.003 0.798 1.35  0.488 1.582 0.595 0.565 0.549 2.422\n",
            " 0.412 0.666 0.49  0.    1.08  0.517 1.136 0.792 0.44  0.983 0.593 1.453\n",
            " 0.66  0.756 2.535 0.624 1.439 0.747 0.613 1.281 1.05  0.    0.359 0.541\n",
            " 0.512 0.634 0.    0.736 0.665 1.489 0.43  1.076 0.579 0.78  0.624 0.649\n",
            " 1.763 1.187 0.429 0.626 0.948 0.38  0.843 1.02  0.515 0.68  0.708 0.\n",
            " 0.522 0.623 1.19  0.547 0.714 0.    0.687 0.266 1.092 0.997 0.504 0.81\n",
            " 0.864 0.504 0.    0.718]\n",
            "[0.088 0.061 0.07  0.089 0.091 0.09  0.137 0.078 0.106 0.082 0.11  0.094\n",
            " 0.136 0.118 0.111 0.147 0.127 0.119 0.136 0.108 0.121 0.104 0.147 0.159\n",
            " 0.157 0.125 0.112 0.112 0.149 0.148 0.12  0.139 0.127 0.157 0.174 0.157\n",
            " 0.22  0.191 0.115 0.149 0.147 0.09  0.113 0.133 0.147 0.139 0.161 0.175\n",
            " 0.119 0.14  0.133 0.157 0.215 0.138 0.148 0.158 0.133 0.113 0.16  0.147\n",
            " 0.121 0.133 0.172 0.153 0.14  0.113 0.188 0.182 0.129 0.147 0.224 0.149\n",
            " 0.122 0.179 0.205 0.217 0.197 0.172 0.191 0.171 0.115 0.165 0.167 0.148\n",
            " 0.147 0.156 0.174 0.164 0.198 0.149 0.156 0.166 0.174 0.239 0.146 0.182\n",
            " 0.17  0.079 0.139 0.153]\n",
            "(100,)\n",
            "<class 'int'>\n",
            "169\n",
            "/content/gdrive/My Drive/thesis/Data/S15/S15_TEX04.txt\n",
            "15\n",
            "[0.664 0.598 1.047 0.257 0.8   0.456 0.231 2.033 1.103 0.931 0.486 0.811\n",
            " 0.438 3.083 0.755 0.614 1.153 0.627 1.687 0.974 1.043 0.519 3.252 0.644\n",
            " 0.91  0.702 3.494 0.337 1.513 0.788 0.613 5.046 1.368 0.573 0.67  1.468\n",
            " 0.727 0.69  1.547 0.732 0.606 0.664 1.507 0.522 1.213 0.59  1.615 0.539\n",
            " 0.418 0.322 1.078 0.683]\n",
            "[0.664 0.598 1.047 0.257 0.8   0.456 0.231 2.033 1.103 0.931 0.486 0.811\n",
            " 0.438 0.    0.755 0.614 1.153 0.627 1.687 0.974 1.043 0.519 0.    0.644\n",
            " 0.91  0.702 0.    0.337 1.513 0.788 0.613 0.    1.368 0.573 0.67  1.468\n",
            " 0.727 0.69  1.547 0.732 0.606 0.664 1.507 0.522 1.213 0.59  1.615 0.539\n",
            " 0.418 0.322 1.078 0.683]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.664 0.598 1.047 0.257 0.8   0.456 0.231 2.033 1.103 0.931 0.486 0.811\n",
            " 0.438 0.    0.755 0.614 1.153 0.627 1.687 0.974 1.043 0.519 0.    0.644\n",
            " 0.91  0.702 0.    0.337 1.513 0.788 0.613 0.    1.368 0.573 0.67  1.468\n",
            " 0.727 0.69  1.547 0.732 0.606 0.664 1.507 0.522 1.213 0.59  1.615 0.539\n",
            " 0.418 0.322 1.078 0.683 0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.   ]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.098\n",
            " 0.123 0.098 0.115 0.157 0.114 0.089 0.125 0.101 0.12  0.103 0.125 0.07\n",
            " 0.098 0.131 0.123 0.116 0.077 0.116 0.158 0.122 0.135 0.122 0.119 0.182\n",
            " 0.153 0.156 0.137 0.114 0.138 0.133 0.143 0.124 0.136 0.121 0.165 0.136\n",
            " 0.139 0.155 0.089 0.123 0.115 0.14  0.138 0.14  0.166 0.157 0.148 0.14\n",
            " 0.162 0.131 0.125 0.092 0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.   ]\n",
            "(100,)\n",
            "<class 'int'>\n",
            "170\n",
            "/content/gdrive/My Drive/thesis/Data/S15/S15_TEX07.txt\n",
            "15\n",
            "[0.578 0.59  0.414 0.5   0.799 0.471 0.617 1.157 1.219 0.515 1.597 0.647\n",
            " 0.574 1.316 3.406 0.791 0.427 0.724 0.574 0.651 0.404 0.683 2.246 0.441\n",
            " 0.404 0.885 0.621 0.474 2.652 1.149 0.48  0.792 1.072 0.68  0.75  1.273\n",
            " 0.68  1.157 0.687 1.176 0.695 0.656 0.399 0.758 0.966 0.72  1.235 0.52\n",
            " 0.382 1.02  0.785 0.574 0.524 0.257]\n",
            "[0.578 0.59  0.414 0.5   0.799 0.471 0.617 1.157 1.219 0.515 1.597 0.647\n",
            " 0.574 1.316 0.    0.791 0.427 0.724 0.574 0.651 0.404 0.683 2.246 0.441\n",
            " 0.404 0.885 0.621 0.474 2.652 1.149 0.48  0.792 1.072 0.68  0.75  1.273\n",
            " 0.68  1.157 0.687 1.176 0.695 0.656 0.399 0.758 0.966 0.72  1.235 0.52\n",
            " 0.382 1.02  0.785 0.574 0.524 0.257]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.578\n",
            " 0.59  0.414 0.5   0.799 0.471 0.617 1.157 1.219 0.515 1.597 0.647 0.574\n",
            " 1.316 0.    0.791 0.427 0.724 0.574 0.651 0.404 0.683 2.246 0.441 0.404\n",
            " 0.885 0.621 0.474 2.652 1.149 0.48  0.792 1.072 0.68  0.75  1.273 0.68\n",
            " 1.157 0.687 1.176 0.695 0.656 0.399 0.758 0.966 0.72  1.235 0.52  0.382\n",
            " 1.02  0.785 0.574 0.524 0.257 0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.   ]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.111 0.133\n",
            " 0.106 0.125 0.107 0.114 0.133 0.137 0.154 0.09  0.139 0.163 0.114 0.124\n",
            " 0.12  0.138 0.11  0.134 0.107 0.081 0.121 0.142 0.115 0.114 0.13  0.207\n",
            " 0.113 0.157 0.124 0.181 0.139 0.133 0.122 0.173 0.14  0.104 0.154 0.148\n",
            " 0.122 0.175 0.103 0.124 0.133 0.133 0.123 0.13  0.133 0.129 0.174 0.192\n",
            " 0.144 0.133 0.15  0.14  0.126 0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.   ]\n",
            "(100,)\n",
            "<class 'int'>\n",
            "171\n",
            "/content/gdrive/My Drive/thesis/Data/S14/S14_TEX08.txt\n",
            "14\n",
            "[0.618 1.417 0.587 1.021 0.873 0.788 0.891 1.217 1.862 0.988 0.931 0.844\n",
            " 0.804 1.01  0.757 0.725 0.802 2.036 0.607 0.958 0.77  0.998 3.559 0.796\n",
            " 1.08  0.924 0.822 0.913 1.176 1.654 0.652 0.909 1.148 0.794 0.788 1.161\n",
            " 0.918 0.45  0.975 1.472 1.86  0.638 0.864 0.606 0.859 0.965 0.136 1.181\n",
            " 1.96  1.89  1.09  2.126 1.078 0.724 0.763 1.047 0.649 1.231 0.78  1.042\n",
            " 1.646 1.429 1.045 1.095 1.002 1.712 0.887 1.217 1.245 0.94  1.041]\n",
            "[0.618 1.417 0.587 1.021 0.873 0.788 0.891 1.217 1.862 0.988 0.931 0.844\n",
            " 0.804 1.01  0.757 0.725 0.802 2.036 0.607 0.958 0.77  0.998 0.    0.796\n",
            " 1.08  0.924 0.822 0.913 1.176 1.654 0.652 0.909 1.148 0.794 0.788 1.161\n",
            " 0.918 0.45  0.975 1.472 1.86  0.638 0.864 0.606 0.859 0.965 0.136 1.181\n",
            " 1.96  1.89  1.09  2.126 1.078 0.724 0.763 1.047 0.649 1.231 0.78  1.042\n",
            " 1.646 1.429 1.045 1.095 1.002 1.712 0.887 1.217 1.245 0.94  1.041]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.618 1.417 0.587 1.021 0.873 0.788 0.891 1.217 1.862 0.988\n",
            " 0.931 0.844 0.804 1.01  0.757 0.725 0.802 2.036 0.607 0.958 0.77  0.998\n",
            " 0.    0.796 1.08  0.924 0.822 0.913 1.176 1.654 0.652 0.909 1.148 0.794\n",
            " 0.788 1.161 0.918 0.45  0.975 1.472 1.86  0.638 0.864 0.606 0.859 0.965\n",
            " 0.136 1.181 1.96  1.89  1.09  2.126 1.078 0.724 0.763 1.047 0.649 1.231\n",
            " 0.78  1.042 1.646 1.429 1.045 1.095 1.002 1.712 0.887 1.217 1.245 0.94\n",
            " 1.041 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.   ]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.251 0.207 0.211 0.263 0.257 0.224 0.209 0.256 0.212 0.227\n",
            " 0.253 0.205 0.274 0.243 0.244 0.23  0.211 0.357 0.199 0.206 0.24  0.224\n",
            " 0.264 0.254 0.273 0.248 0.304 0.234 0.265 0.277 0.252 0.249 0.254 0.229\n",
            " 0.24  0.217 0.311 0.191 0.207 0.244 0.319 0.237 0.247 0.198 0.198 0.253\n",
            " 0.086 0.1   0.307 0.241 0.288 0.328 0.295 0.265 0.234 0.24  0.223 0.281\n",
            " 0.191 0.188 0.312 0.203 0.27  0.29  0.276 0.321 0.247 0.265 0.287 0.235\n",
            " 0.28  0.236 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.   ]\n",
            "(100,)\n",
            "<class 'int'>\n",
            "172\n",
            "/content/gdrive/My Drive/thesis/Data/S14/S14_TEX05.txt\n",
            "14\n",
            "[0.863 0.691 0.735 0.819 4.443 1.011 0.851 1.065 0.877 0.864 0.756 0.602\n",
            " 0.955 1.03  0.75  0.772 0.898 0.623 1.068 0.938 0.677 1.152 0.92  1.675\n",
            " 0.955 0.646 0.922 0.799 0.714 1.014 2.457 1.453 1.985 0.984 0.803 1.738\n",
            " 1.296 0.923 0.847 0.699 1.076 0.886 1.346 1.847 1.32  0.917 0.914 1.012\n",
            " 1.035 4.835 0.897 0.432 0.893 0.89  2.108 1.117 1.269 0.808 0.997 0.827\n",
            " 0.558 1.218 0.86  1.666]\n",
            "[0.863 0.691 0.735 0.819 0.    1.011 0.851 1.065 0.877 0.864 0.756 0.602\n",
            " 0.955 1.03  0.75  0.772 0.898 0.623 1.068 0.938 0.677 1.152 0.92  1.675\n",
            " 0.955 0.646 0.922 0.799 0.714 1.014 2.457 1.453 1.985 0.984 0.803 1.738\n",
            " 1.296 0.923 0.847 0.699 1.076 0.886 1.346 1.847 1.32  0.917 0.914 1.012\n",
            " 1.035 0.    0.897 0.432 0.893 0.89  2.108 1.117 1.269 0.808 0.997 0.827\n",
            " 0.558 1.218 0.86  1.666]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.863 0.691 0.735 0.819 0.    1.011\n",
            " 0.851 1.065 0.877 0.864 0.756 0.602 0.955 1.03  0.75  0.772 0.898 0.623\n",
            " 1.068 0.938 0.677 1.152 0.92  1.675 0.955 0.646 0.922 0.799 0.714 1.014\n",
            " 2.457 1.453 1.985 0.984 0.803 1.738 1.296 0.923 0.847 0.699 1.076 0.886\n",
            " 1.346 1.847 1.32  0.917 0.914 1.012 1.035 0.    0.897 0.432 0.893 0.89\n",
            " 2.108 1.117 1.269 0.808 0.997 0.827 0.558 1.218 0.86  1.666 0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.   ]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.239 0.215 0.19  0.269 0.199 0.212 0.182\n",
            " 0.195 0.244 0.232 0.224 0.219 0.206 0.272 0.241 0.255 0.306 0.282 0.257\n",
            " 0.236 0.238 0.252 0.272 0.349 0.22  0.188 0.265 0.275 0.214 0.266 0.308\n",
            " 0.198 0.379 0.281 0.261 0.335 0.29  0.282 0.248 0.224 0.224 0.236 0.24\n",
            " 0.315 0.238 0.203 0.238 0.28  0.273 0.22  0.231 0.157 0.19  0.22  0.26\n",
            " 0.272 0.302 0.105 0.202 0.179 0.258 0.265 0.248 0.273 0.278 0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.   ]\n",
            "(100,)\n",
            "<class 'int'>\n",
            "173\n",
            "/content/gdrive/My Drive/thesis/Data/S14/S14_TEX09.txt\n",
            "14\n",
            "[0.807 0.573 0.781 0.632 0.64  0.599 1.063 0.607 0.934 0.93  0.769 0.934\n",
            " 1.419 1.179 1.007 0.764 0.624 1.175 0.911 1.093 0.902 1.176 0.901 0.942\n",
            " 1.056 1.058 1.684 0.989 0.905 1.053 0.875 0.712 1.074 1.053 0.716 2.376\n",
            " 1.321 1.078 1.112 0.738 1.269 0.657 0.806 0.632 0.843 0.694 0.648 3.651]\n",
            "[0.807 0.573 0.781 0.632 0.64  0.599 1.063 0.607 0.934 0.93  0.769 0.934\n",
            " 1.419 1.179 1.007 0.764 0.624 1.175 0.911 1.093 0.902 1.176 0.901 0.942\n",
            " 1.056 1.058 1.684 0.989 0.905 1.053 0.875 0.712 1.074 1.053 0.716 2.376\n",
            " 1.321 1.078 1.112 0.738 1.269 0.657 0.806 0.632 0.843 0.694 0.648 0.   ]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.807 0.573 0.781 0.632 0.64  0.599 1.063 0.607 0.934 0.93\n",
            " 0.769 0.934 1.419 1.179 1.007 0.764 0.624 1.175 0.911 1.093 0.902 1.176\n",
            " 0.901 0.942 1.056 1.058 1.684 0.989 0.905 1.053 0.875 0.712 1.074 1.053\n",
            " 0.716 2.376 1.321 1.078 1.112 0.738 1.269 0.657 0.806 0.632 0.843 0.694\n",
            " 0.648 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.   ]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.249 0.189 0.198 0.233 0.223 0.248 0.247 0.232 0.214 0.203 0.205\n",
            " 0.274 0.324 0.264 0.274 0.264 0.241 0.281 0.245 0.232 0.245 0.242 0.269\n",
            " 0.242 0.37  0.353 0.35  0.204 0.213 0.23  0.299 0.238 0.232 0.263 0.274\n",
            " 0.281 0.257 0.257 0.252 0.186 0.253 0.082 0.297 0.239 0.205 0.177 0.198\n",
            " 0.222 0.171 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.   ]\n",
            "(100,)\n",
            "<class 'int'>\n",
            "174\n",
            "/content/gdrive/My Drive/thesis/Data/S14/S14_TEX03.txt\n",
            "14\n",
            "[1.022 1.041 2.849 0.797 1.208 0.73  1.333 0.844 0.925 0.899 1.113 1.097\n",
            " 0.93  1.311 0.824 0.917 0.819 0.848 1.06  1.392 0.95  1.301 1.016 0.983\n",
            " 0.777 0.757 1.006 1.131 2.442 1.372 0.847 1.039 0.991 1.519 0.941 0.923\n",
            " 0.948 0.76  0.977 1.213 0.931 0.861 0.814 0.773 0.814 0.611 1.276]\n",
            "[1.022 1.041 2.849 0.797 1.208 0.73  1.333 0.844 0.925 0.899 1.113 1.097\n",
            " 0.93  1.311 0.824 0.917 0.819 0.848 1.06  1.392 0.95  1.301 1.016 0.983\n",
            " 0.777 0.757 1.006 1.131 2.442 1.372 0.847 1.039 0.991 1.519 0.941 0.923\n",
            " 0.948 0.76  0.977 1.213 0.931 0.861 0.814 0.773 0.814 0.611 1.276]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    1.022 1.041 2.849 0.797 1.208 0.73  1.333 0.844 0.925 0.899\n",
            " 1.113 1.097 0.93  1.311 0.824 0.917 0.819 0.848 1.06  1.392 0.95  1.301\n",
            " 1.016 0.983 0.777 0.757 1.006 1.131 2.442 1.372 0.847 1.039 0.991 1.519\n",
            " 0.941 0.923 0.948 0.76  0.977 1.213 0.931 0.861 0.814 0.773 0.814 0.611\n",
            " 1.276 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.   ]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.231 0.182 0.138 0.248 0.208 0.198 0.174 0.245 0.216 0.213\n",
            " 0.221 0.279 0.262 0.263 0.261 0.251 0.262 0.233 0.291 0.278 0.223 0.27\n",
            " 0.266 0.265 0.204 0.248 0.265 0.216 0.298 0.216 0.273 0.277 0.282 0.271\n",
            " 0.223 0.205 0.204 0.228 0.183 0.203 0.237 0.215 0.232 0.236 0.258 0.224\n",
            " 0.169 0.223 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.   ]\n",
            "(100,)\n",
            "<class 'int'>\n",
            "175\n",
            "/content/gdrive/My Drive/thesis/Data/S14/S14_TEX02.txt\n",
            "14\n",
            "[0.695 0.615 0.957 0.975 1.542 1.043 0.503 1.055 1.042 1.07  0.951 1.212\n",
            " 1.091 2.091 2.803 1.188 1.05  1.078 1.103 1.023 1.355 1.083 1.826 0.939\n",
            " 0.933 0.94  1.495 0.957 2.066 1.409 1.005 1.154 0.763 0.864 1.249 0.947\n",
            " 1.098 1.26  0.815 0.831 0.991 0.945 1.169 1.335 0.922 1.953 1.983 0.859\n",
            " 1.191 1.081 0.829 0.981 1.325 0.986 0.856 1.409 3.176 2.13  1.197 1.436\n",
            " 0.562 1.414 1.012 0.924 1.789 2.694 0.794 1.447 0.817 2.229 1.197 1.269\n",
            " 2.627 2.142 1.143 1.104 1.133 0.861 1.656 1.254 0.904 1.165 1.215 1.187\n",
            " 1.097 0.96 ]\n",
            "[0.695 0.615 0.957 0.975 1.542 1.043 0.503 1.055 1.042 1.07  0.951 1.212\n",
            " 1.091 2.091 2.803 1.188 1.05  1.078 1.103 1.023 1.355 1.083 1.826 0.939\n",
            " 0.933 0.94  1.495 0.957 2.066 1.409 1.005 1.154 0.763 0.864 1.249 0.947\n",
            " 1.098 1.26  0.815 0.831 0.991 0.945 1.169 1.335 0.922 1.953 1.983 0.859\n",
            " 1.191 1.081 0.829 0.981 1.325 0.986 0.856 1.409 0.    2.13  1.197 1.436\n",
            " 0.562 1.414 1.012 0.924 1.789 2.694 0.794 1.447 0.817 2.229 1.197 1.269\n",
            " 2.627 2.142 1.143 1.104 1.133 0.861 1.656 1.254 0.904 1.165 1.215 1.187\n",
            " 1.097 0.96 ]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.695 0.615 0.957 0.975 1.542\n",
            " 1.043 0.503 1.055 1.042 1.07  0.951 1.212 1.091 2.091 2.803 1.188 1.05\n",
            " 1.078 1.103 1.023 1.355 1.083 1.826 0.939 0.933 0.94  1.495 0.957 2.066\n",
            " 1.409 1.005 1.154 0.763 0.864 1.249 0.947 1.098 1.26  0.815 0.831 0.991\n",
            " 0.945 1.169 1.335 0.922 1.953 1.983 0.859 1.191 1.081 0.829 0.981 1.325\n",
            " 0.986 0.856 1.409 0.    2.13  1.197 1.436 0.562 1.414 1.012 0.924 1.789\n",
            " 2.694 0.794 1.447 0.817 2.229 1.197 1.269 2.627 2.142 1.143 1.104 1.133\n",
            " 0.861 1.656 1.254 0.904 1.165 1.215 1.187 1.097 0.96  0.    0.    0.\n",
            " 0.    0.    0.    0.   ]\n",
            "[0.    0.    0.    0.    0.    0.    0.179 0.224 0.249 0.223 0.211 0.215\n",
            " 0.187 0.207 0.175 0.262 0.24  0.278 0.213 0.295 0.281 0.287 0.263 0.243\n",
            " 0.296 0.265 0.241 0.227 0.229 0.307 0.184 0.207 0.246 0.246 0.304 0.307\n",
            " 0.269 0.303 0.23  0.29  0.298 0.213 0.239 0.18  0.257 0.182 0.249 0.329\n",
            " 0.315 0.294 0.305 0.348 0.273 0.226 0.231 0.237 0.28  0.333 0.273 0.137\n",
            " 0.215 0.236 0.315 0.237 0.21  0.243 0.195 0.198 0.28  0.282 0.289 0.362\n",
            " 0.22  0.284 0.324 0.321 0.253 0.262 0.356 0.265 0.243 0.304 0.315 0.262\n",
            " 0.265 0.287 0.247 0.29  0.289 0.236 0.353 0.32  0.316 0.    0.    0.\n",
            " 0.    0.    0.    0.   ]\n",
            "(100,)\n",
            "<class 'int'>\n",
            "176\n",
            "/content/gdrive/My Drive/thesis/Data/S14/S14_TEX04.txt\n",
            "14\n",
            "[1.573 0.861 0.931 0.979 0.833 2.354 0.866 0.62  0.84  1.142 0.871 1.049\n",
            " 1.221 1.987 1.871 0.906 1.129 0.763 1.022 1.812 0.999 0.812 0.707 1.271\n",
            " 0.725 1.889 1.122 0.696 0.557 1.2   0.983 1.943 1.942 1.038 1.463 0.807\n",
            " 0.939 0.641 0.868 0.944 0.991 2.563 0.845 0.855 0.865 1.675 0.861 0.83 ]\n",
            "[1.573 0.861 0.931 0.979 0.833 2.354 0.866 0.62  0.84  1.142 0.871 1.049\n",
            " 1.221 1.987 1.871 0.906 1.129 0.763 1.022 1.812 0.999 0.812 0.707 1.271\n",
            " 0.725 1.889 1.122 0.696 0.557 1.2   0.983 1.943 1.942 1.038 1.463 0.807\n",
            " 0.939 0.641 0.868 0.944 0.991 2.563 0.845 0.855 0.865 1.675 0.861 0.83 ]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    1.573 0.861 0.931 0.979 0.833 2.354 0.866 0.62  0.84  1.142\n",
            " 0.871 1.049 1.221 1.987 1.871 0.906 1.129 0.763 1.022 1.812 0.999 0.812\n",
            " 0.707 1.271 0.725 1.889 1.122 0.696 0.557 1.2   0.983 1.943 1.942 1.038\n",
            " 1.463 0.807 0.939 0.641 0.868 0.944 0.991 2.563 0.845 0.855 0.865 1.675\n",
            " 0.861 0.83  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.   ]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.238 0.204 0.232 0.223 0.233 0.359 0.188 0.203 0.215 0.281 0.252\n",
            " 0.262 0.221 0.296 0.22  0.212 0.24  0.288 0.315 0.282 0.247 0.204 0.199\n",
            " 0.216 0.226 0.323 0.245 0.237 0.231 0.314 0.254 0.21  0.362 0.308 0.307\n",
            " 0.233 0.24  0.188 0.207 0.245 0.165 0.255 0.279 0.249 0.265 0.341 0.311\n",
            " 0.273 0.324 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.   ]\n",
            "(100,)\n",
            "<class 'int'>\n",
            "177\n",
            "/content/gdrive/My Drive/thesis/Data/S14/S14_TEX10.txt\n",
            "14\n",
            "[0.764 5.685 0.798 0.69  0.831 1.371 0.897 1.076 1.221 0.983 1.122 0.876\n",
            " 1.191 4.378 0.624 1.183 0.612 1.014 0.923 0.922 0.775 1.089 0.798 1.201\n",
            " 0.835 0.773 1.965 1.306 2.996 2.804 1.069 0.973 1.124 0.696 0.61  0.884\n",
            " 0.612 1.284 1.03  0.807 0.814 0.907 1.724 1.444 1.083 1.812 1.309 0.975\n",
            " 0.675 1.333 1.249 0.587 1.218 1.061 1.067 0.918]\n",
            "[0.764 0.    0.798 0.69  0.831 1.371 0.897 1.076 1.221 0.983 1.122 0.876\n",
            " 1.191 0.    0.624 1.183 0.612 1.014 0.923 0.922 0.775 1.089 0.798 1.201\n",
            " 0.835 0.773 1.965 1.306 2.996 2.804 1.069 0.973 1.124 0.696 0.61  0.884\n",
            " 0.612 1.284 1.03  0.807 0.814 0.907 1.724 1.444 1.083 1.812 1.309 0.975\n",
            " 0.675 1.333 1.249 0.587 1.218 1.061 1.067 0.918]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.764 0.\n",
            " 0.798 0.69  0.831 1.371 0.897 1.076 1.221 0.983 1.122 0.876 1.191 0.\n",
            " 0.624 1.183 0.612 1.014 0.923 0.922 0.775 1.089 0.798 1.201 0.835 0.773\n",
            " 1.965 1.306 2.996 2.804 1.069 0.973 1.124 0.696 0.61  0.884 0.612 1.284\n",
            " 1.03  0.807 0.814 0.907 1.724 1.444 1.083 1.812 1.309 0.975 0.675 1.333\n",
            " 1.249 0.587 1.218 1.061 1.067 0.918 0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.   ]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.115 0.066 0.24\n",
            " 0.256 0.282 0.248 0.273 0.233 0.239 0.222 0.294 0.253 0.224 0.263 0.158\n",
            " 0.244 0.262 0.29  0.332 0.332 0.285 0.364 0.257 0.257 0.229 0.215 0.227\n",
            " 0.312 0.372 0.363 0.228 0.289 0.256 0.287 0.173 0.211 0.255 0.203 0.315\n",
            " 0.241 0.252 0.233 0.356 0.27  0.222 0.346 0.261 0.29  0.289 0.308 0.278\n",
            " 0.181 0.308 0.305 0.281 0.303 0.266 0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.   ]\n",
            "(100,)\n",
            "<class 'int'>\n",
            "178\n",
            "/content/gdrive/My Drive/thesis/Data/S14/S14_TEX11.txt\n",
            "14\n",
            "[0.611 0.951 0.873 1.108 1.079 0.629 1.296 1.05  0.553 0.575 2.244 0.959\n",
            " 1.529 1.063 0.713 1.126 1.485 1.045 0.582 1.666 1.073 0.694 0.74  1.2\n",
            " 1.729 0.896 0.606 2.039 0.413 1.064 0.781 0.773 1.016 0.826 0.662 1.148\n",
            " 1.647 1.264 1.063 0.812 2.124 1.129 1.598 0.864 0.86  1.168 1.17  1.181\n",
            " 1.147 1.073]\n",
            "[0.611 0.951 0.873 1.108 1.079 0.629 1.296 1.05  0.553 0.575 2.244 0.959\n",
            " 1.529 1.063 0.713 1.126 1.485 1.045 0.582 1.666 1.073 0.694 0.74  1.2\n",
            " 1.729 0.896 0.606 2.039 0.413 1.064 0.781 0.773 1.016 0.826 0.662 1.148\n",
            " 1.647 1.264 1.063 0.812 2.124 1.129 1.598 0.864 0.86  1.168 1.17  1.181\n",
            " 1.147 1.073]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.611 0.951 0.873 1.108 1.079 0.629 1.296 1.05  0.553 0.575 2.244\n",
            " 0.959 1.529 1.063 0.713 1.126 1.485 1.045 0.582 1.666 1.073 0.694 0.74\n",
            " 1.2   1.729 0.896 0.606 2.039 0.413 1.064 0.781 0.773 1.016 0.826 0.662\n",
            " 1.148 1.647 1.264 1.063 0.812 2.124 1.129 1.598 0.864 0.86  1.168 1.17\n",
            " 1.181 1.147 1.073 0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.   ]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.22  0.284 0.212 0.204 0.243 0.219 0.222 0.284 0.212 0.233 0.256 0.19\n",
            " 0.294 0.247 0.262 0.232 0.327 0.297 0.199 0.191 0.32  0.286 0.2   0.291\n",
            " 0.304 0.213 0.215 0.3   0.172 0.191 0.274 0.241 0.233 0.247 0.237 0.24\n",
            " 0.255 0.288 0.27  0.263 0.241 0.27  0.271 0.243 0.269 0.257 0.294 0.246\n",
            " 0.321 0.288 0.22  0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.   ]\n",
            "(100,)\n",
            "<class 'int'>\n",
            "179\n",
            "/content/gdrive/My Drive/thesis/Data/S14/S14_TEX07.txt\n",
            "14\n",
            "[0.731 0.666 0.817 0.987 0.564 0.94  0.989 0.813 0.499 1.005 1.048 2.231\n",
            " 0.844 0.858 0.847 1.215 0.692 0.212 2.11  2.53  0.803 1.012 1.941 1.373\n",
            " 0.669 0.698 0.844 0.971 0.704 0.682 0.855 1.066 1.149 0.995 1.014 2.727\n",
            " 1.221 0.68  0.847 0.782 1.157 1.02  1.833 0.938 0.829 0.689 1.224 0.871\n",
            " 0.952 0.702 1.175 0.996 0.73  2.627 0.805 0.86  0.795 0.867 0.948 4.327\n",
            " 1.579 1.158 1.418 1.001 0.736 1.263 1.042 1.237 1.075 6.332 2.752 0.943\n",
            " 0.841 0.88  1.38  1.089 0.798 1.232 0.991 0.973 0.922 0.744 1.575 0.961\n",
            " 0.706 0.893 1.039 1.279 0.854 0.967 0.879 0.875 1.04  1.026 0.583 0.772\n",
            " 0.615]\n",
            "[0.731 0.666 0.817 0.987 0.564 0.94  0.989 0.813 0.499 1.005 1.048 2.231\n",
            " 0.844 0.858 0.847 1.215 0.692 0.212 2.11  2.53  0.803 1.012 1.941 1.373\n",
            " 0.669 0.698 0.844 0.971 0.704 0.682 0.855 1.066 1.149 0.995 1.014 2.727\n",
            " 1.221 0.68  0.847 0.782 1.157 1.02  1.833 0.938 0.829 0.689 1.224 0.871\n",
            " 0.952 0.702 1.175 0.996 0.73  2.627 0.805 0.86  0.795 0.867 0.948 0.\n",
            " 1.579 1.158 1.418 1.001 0.736 1.263 1.042 1.237 1.075 0.    2.752 0.943\n",
            " 0.841 0.88  1.38  1.089 0.798 1.232 0.991 0.973 0.922 0.744 1.575 0.961\n",
            " 0.706 0.893 1.039 1.279 0.854 0.967 0.879 0.875 1.04  1.026 0.583 0.772\n",
            " 0.615]\n",
            "[0.    0.731 0.666 0.817 0.987 0.564 0.94  0.989 0.813 0.499 1.005 1.048\n",
            " 2.231 0.844 0.858 0.847 1.215 0.692 0.212 2.11  2.53  0.803 1.012 1.941\n",
            " 1.373 0.669 0.698 0.844 0.971 0.704 0.682 0.855 1.066 1.149 0.995 1.014\n",
            " 2.727 1.221 0.68  0.847 0.782 1.157 1.02  1.833 0.938 0.829 0.689 1.224\n",
            " 0.871 0.952 0.702 1.175 0.996 0.73  2.627 0.805 0.86  0.795 0.867 0.948\n",
            " 0.    1.579 1.158 1.418 1.001 0.736 1.263 1.042 1.237 1.075 0.    2.752\n",
            " 0.943 0.841 0.88  1.38  1.089 0.798 1.232 0.991 0.973 0.922 0.744 1.575\n",
            " 0.961 0.706 0.893 1.039 1.279 0.854 0.967 0.879 0.875 1.04  1.026 0.583\n",
            " 0.772 0.615 0.    0.   ]\n",
            "[0.    0.207 0.208 0.173 0.237 0.181 0.214 0.215 0.164 0.182 0.206 0.265\n",
            " 0.323 0.253 0.191 0.265 0.273 0.298 0.145 0.04  0.275 0.27  0.288 0.269\n",
            " 0.304 0.161 0.174 0.223 0.211 0.237 0.233 0.273 0.198 0.196 0.194 0.255\n",
            " 0.313 0.27  0.242 0.181 0.233 0.232 0.23  0.265 0.236 0.237 0.197 0.256\n",
            " 0.205 0.249 0.253 0.199 0.246 0.222 0.265 0.222 0.182 0.195 0.231 0.254\n",
            " 0.229 0.232 0.264 0.32  0.249 0.219 0.248 0.273 0.229 0.272 0.344 0.245\n",
            " 0.279 0.219 0.236 0.296 0.274 0.224 0.265 0.287 0.261 0.211 0.245 0.249\n",
            " 0.295 0.23  0.241 0.254 0.302 0.171 0.223 0.195 0.255 0.237 0.195 0.207\n",
            " 0.172 0.209 0.215 0.    0.   ]\n",
            "(100,)\n",
            "<class 'int'>\n",
            "180\n",
            "/content/gdrive/My Drive/thesis/Data/S14/S14_TEX06.txt\n",
            "14\n",
            "[0.629 1.304 0.573 0.798 1.297 0.756 0.773 0.924 1.316 4.905 2.835 0.861\n",
            " 1.026 1.07  0.943 1.629 1.018 0.674 0.924 0.772 1.216 0.927 0.899 0.839\n",
            " 1.87  0.865 0.828 0.792 0.832 0.764 0.867 1.15  0.677 0.856 1.16  2.057\n",
            " 1.01  0.428 2.337 1.34  0.824 1.159 0.708 0.764 2.029 0.565 0.875 0.804\n",
            " 0.856 0.923 1.068 0.653 0.814 2.465 1.154 0.938 0.814 0.761]\n",
            "[0.629 1.304 0.573 0.798 1.297 0.756 0.773 0.924 1.316 0.    2.835 0.861\n",
            " 1.026 1.07  0.943 1.629 1.018 0.674 0.924 0.772 1.216 0.927 0.899 0.839\n",
            " 1.87  0.865 0.828 0.792 0.832 0.764 0.867 1.15  0.677 0.856 1.16  2.057\n",
            " 1.01  0.428 2.337 1.34  0.824 1.159 0.708 0.764 2.029 0.565 0.875 0.804\n",
            " 0.856 0.923 1.068 0.653 0.814 2.465 1.154 0.938 0.814 0.761]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.629 1.304 0.573\n",
            " 0.798 1.297 0.756 0.773 0.924 1.316 0.    2.835 0.861 1.026 1.07  0.943\n",
            " 1.629 1.018 0.674 0.924 0.772 1.216 0.927 0.899 0.839 1.87  0.865 0.828\n",
            " 0.792 0.832 0.764 0.867 1.15  0.677 0.856 1.16  2.057 1.01  0.428 2.337\n",
            " 1.34  0.824 1.159 0.708 0.764 2.029 0.565 0.875 0.804 0.856 0.923 1.068\n",
            " 0.653 0.814 2.465 1.154 0.938 0.814 0.761 0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.   ]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.229 0.173 0.215 0.191\n",
            " 0.192 0.182 0.207 0.274 0.273 0.03  0.253 0.211 0.232 0.236 0.247 0.269\n",
            " 0.21  0.224 0.232 0.249 0.281 0.254 0.282 0.266 0.307 0.215 0.148 0.21\n",
            " 0.316 0.258 0.249 0.288 0.236 0.249 0.249 0.335 0.173 0.203 0.232 0.398\n",
            " 0.238 0.26  0.283 0.29  0.365 0.181 0.207 0.214 0.216 0.267 0.249 0.203\n",
            " 0.224 0.158 0.394 0.288 0.284 0.325 0.258 0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.   ]\n",
            "(100,)\n",
            "<class 'int'>\n",
            "181\n",
            "/content/gdrive/My Drive/thesis/Data/S08/S08_TEX08.txt\n",
            "8\n",
            "[0.678 0.516 0.65  1.147 0.647 0.482 0.507 0.608 0.499 0.714 0.983 0.706\n",
            " 0.722 0.492 0.859 0.47  0.558 0.523 0.599 0.315 2.81  0.714 0.748 0.492\n",
            " 0.788 0.523 0.518 0.773 0.365 3.185 0.856 0.713 0.49  0.374 0.515 0.936\n",
            " 0.454 0.482 0.657 0.332 0.533 2.921 0.673 0.278 0.625 0.424 0.756 0.731\n",
            " 0.591 0.549 0.557 0.648 0.776 0.678 2.686 0.7   0.672 0.722 0.692 0.523\n",
            " 0.549 0.992 2.185 3.323]\n",
            "[0.678 0.516 0.65  1.147 0.647 0.482 0.507 0.608 0.499 0.714 0.983 0.706\n",
            " 0.722 0.492 0.859 0.47  0.558 0.523 0.599 0.315 2.81  0.714 0.748 0.492\n",
            " 0.788 0.523 0.518 0.773 0.365 0.    0.856 0.713 0.49  0.374 0.515 0.936\n",
            " 0.454 0.482 0.657 0.332 0.533 2.921 0.673 0.278 0.625 0.424 0.756 0.731\n",
            " 0.591 0.549 0.557 0.648 0.776 0.678 2.686 0.7   0.672 0.722 0.692 0.523\n",
            " 0.549 0.992 2.185 0.   ]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.678 0.516 0.65  1.147 0.647 0.482\n",
            " 0.507 0.608 0.499 0.714 0.983 0.706 0.722 0.492 0.859 0.47  0.558 0.523\n",
            " 0.599 0.315 2.81  0.714 0.748 0.492 0.788 0.523 0.518 0.773 0.365 0.\n",
            " 0.856 0.713 0.49  0.374 0.515 0.936 0.454 0.482 0.657 0.332 0.533 2.921\n",
            " 0.673 0.278 0.625 0.424 0.756 0.731 0.591 0.549 0.557 0.648 0.776 0.678\n",
            " 2.686 0.7   0.672 0.722 0.692 0.523 0.549 0.992 2.185 0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.   ]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.154 0.149 0.164 0.138 0.156 0.157 0.124\n",
            " 0.166 0.157 0.14  0.149 0.147 0.156 0.174 0.131 0.177 0.116 0.164 0.123\n",
            " 0.157 0.157 0.14  0.165 0.157 0.189 0.182 0.158 0.106 0.122 0.173 0.154\n",
            " 0.171 0.156 0.148 0.149 0.2   0.195 0.208 0.132 0.157 0.199 0.173 0.178\n",
            " 0.111 0.15  0.165 0.124 0.192 0.124 0.132 0.148 0.148 0.173 0.138 0.158\n",
            " 0.14  0.105 0.182 0.165 0.139 0.115 0.156 0.169 0.188 0.14  0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.   ]\n",
            "(100,)\n",
            "<class 'int'>\n",
            "182\n",
            "/content/gdrive/My Drive/thesis/Data/S08/S08_TEX05.txt\n",
            "8\n",
            "[0.627 0.648 0.682 0.616 0.673 0.397 0.558 0.567 0.554 1.06  0.847 1.145\n",
            " 0.517 0.668 0.516 0.591 1.074 0.587 0.44  0.646 0.742 1.242 0.888 0.796\n",
            " 0.982 1.763 0.705 0.504 0.44  0.526 0.629 0.673 0.664 0.742 1.364 0.728\n",
            " 0.572 0.525 0.524 2.022 1.065 0.588 1.515 1.609 0.582 0.565 0.616 2.368\n",
            " 1.006 0.731 0.79  1.105 0.707 0.449 0.665 0.698 0.842 0.87  0.639]\n",
            "[0.627 0.648 0.682 0.616 0.673 0.397 0.558 0.567 0.554 1.06  0.847 1.145\n",
            " 0.517 0.668 0.516 0.591 1.074 0.587 0.44  0.646 0.742 1.242 0.888 0.796\n",
            " 0.982 1.763 0.705 0.504 0.44  0.526 0.629 0.673 0.664 0.742 1.364 0.728\n",
            " 0.572 0.525 0.524 2.022 1.065 0.588 1.515 1.609 0.582 0.565 0.616 2.368\n",
            " 1.006 0.731 0.79  1.105 0.707 0.449 0.665 0.698 0.842 0.87  0.639]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.627 0.648 0.682 0.616\n",
            " 0.673 0.397 0.558 0.567 0.554 1.06  0.847 1.145 0.517 0.668 0.516 0.591\n",
            " 1.074 0.587 0.44  0.646 0.742 1.242 0.888 0.796 0.982 1.763 0.705 0.504\n",
            " 0.44  0.526 0.629 0.673 0.664 0.742 1.364 0.728 0.572 0.525 0.524 2.022\n",
            " 1.065 0.588 1.515 1.609 0.582 0.565 0.616 2.368 1.006 0.731 0.79  1.105\n",
            " 0.707 0.449 0.665 0.698 0.842 0.87  0.639 0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.   ]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.145 0.106 0.174 0.114\n",
            " 0.122 0.123 0.141 0.149 0.104 0.141 0.119 0.195 0.148 0.137 0.175 0.132\n",
            " 0.173 0.163 0.139 0.166 0.193 0.122 0.036 0.146 0.115 0.339 0.153 0.171\n",
            " 0.189 0.132 0.146 0.115 0.175 0.149 0.122 0.146 0.14  0.158 0.14  0.156\n",
            " 0.18  0.172 0.182 0.146 0.199 0.125 0.141 0.182 0.182 0.181 0.149 0.14\n",
            " 0.19  0.14  0.156 0.148 0.156 0.237 0.132 0.148 0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.   ]\n",
            "(100,)\n",
            "<class 'int'>\n",
            "183\n",
            "/content/gdrive/My Drive/thesis/Data/S08/S08_TEX06.txt\n",
            "8\n",
            "[0.795 0.733 0.548 0.693 0.321 0.858 0.498 0.689 0.573 0.308 0.799 0.655\n",
            " 1.595 0.707 0.517 0.582 0.813 0.617 0.65  0.941 0.714 0.587 0.65  1.339\n",
            " 0.744 0.433 0.973 2.809 0.605 0.491 0.366 0.53  0.589 0.559 0.691 0.843\n",
            " 0.616 0.7   0.734 0.695 0.689 0.557 0.756 0.5   0.756 0.498 0.74  1.806\n",
            " 0.852 0.555 0.894 0.985 0.466 0.664 0.449 0.582 1.171 1.915 0.645 0.631\n",
            " 0.749 0.629 0.643 0.588 0.447 0.568 0.691 0.796 0.59  0.466 0.566]\n",
            "[0.795 0.733 0.548 0.693 0.321 0.858 0.498 0.689 0.573 0.308 0.799 0.655\n",
            " 1.595 0.707 0.517 0.582 0.813 0.617 0.65  0.941 0.714 0.587 0.65  1.339\n",
            " 0.744 0.433 0.973 2.809 0.605 0.491 0.366 0.53  0.589 0.559 0.691 0.843\n",
            " 0.616 0.7   0.734 0.695 0.689 0.557 0.756 0.5   0.756 0.498 0.74  1.806\n",
            " 0.852 0.555 0.894 0.985 0.466 0.664 0.449 0.582 1.171 1.915 0.645 0.631\n",
            " 0.749 0.629 0.643 0.588 0.447 0.568 0.691 0.796 0.59  0.466 0.566]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.795 0.733 0.548 0.693 0.321 0.858 0.498 0.689 0.573 0.308\n",
            " 0.799 0.655 1.595 0.707 0.517 0.582 0.813 0.617 0.65  0.941 0.714 0.587\n",
            " 0.65  1.339 0.744 0.433 0.973 2.809 0.605 0.491 0.366 0.53  0.589 0.559\n",
            " 0.691 0.843 0.616 0.7   0.734 0.695 0.689 0.557 0.756 0.5   0.756 0.498\n",
            " 0.74  1.806 0.852 0.555 0.894 0.985 0.466 0.664 0.449 0.582 1.171 1.915\n",
            " 0.645 0.631 0.749 0.629 0.643 0.588 0.447 0.568 0.691 0.796 0.59  0.466\n",
            " 0.566 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.   ]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.148 0.174 0.149 0.149 0.163 0.198 0.131 0.148 0.148 0.149\n",
            " 0.19  0.124 0.173 0.182 0.1   0.149 0.165 0.117 0.165 0.196 0.127 0.111\n",
            " 0.139 0.138 0.194 0.158 0.123 0.148 0.155 0.174 0.148 0.157 0.166 0.183\n",
            " 0.206 0.146 0.118 0.15  0.172 0.177 0.157 0.141 0.158 0.124 0.115 0.148\n",
            " 0.133 0.14  0.535 0.374 0.199 0.187 0.158 0.125 0.15  0.132 0.149 0.199\n",
            " 0.144 0.165 0.123 0.172 0.165 0.196 0.19  0.143 0.181 0.196 0.098 0.157\n",
            " 0.123 0.164 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.   ]\n",
            "(100,)\n",
            "<class 'int'>\n",
            "184\n",
            "/content/gdrive/My Drive/thesis/Data/S08/S08_TEX07.txt\n",
            "8\n",
            "[0.505 0.609 0.983 0.939 0.931 0.795 0.835 0.753 2.998 0.715 0.694 0.442\n",
            " 1.024 0.356 0.681 0.857 0.683 0.697 1.108 3.557 0.765 0.583 0.615 0.952\n",
            " 0.794 0.333 0.506 0.541 5.332 0.701 0.33  0.733 0.712 0.523 0.467 0.934\n",
            " 1.931 0.727 0.946 0.651 2.81  0.739 0.539 0.633 1.124 0.579 0.642 0.72\n",
            " 0.441 0.466 0.306 0.411 0.694 0.724 4.383 0.774 0.361 0.731 0.699 0.891\n",
            " 0.421 0.647 0.545 0.47  0.507 3.548 0.715 0.295 0.835 0.653 0.549 0.54\n",
            " 0.766 0.517 0.43  0.299 0.96  0.578 0.531 0.408 0.575 0.481 0.648 0.692\n",
            " 0.631 0.58  0.326 0.348 0.731 0.516 0.466 5.805 1.637]\n",
            "[0.505 0.609 0.983 0.939 0.931 0.795 0.835 0.753 2.998 0.715 0.694 0.442\n",
            " 1.024 0.356 0.681 0.857 0.683 0.697 1.108 0.    0.765 0.583 0.615 0.952\n",
            " 0.794 0.333 0.506 0.541 0.    0.701 0.33  0.733 0.712 0.523 0.467 0.934\n",
            " 1.931 0.727 0.946 0.651 2.81  0.739 0.539 0.633 1.124 0.579 0.642 0.72\n",
            " 0.441 0.466 0.306 0.411 0.694 0.724 0.    0.774 0.361 0.731 0.699 0.891\n",
            " 0.421 0.647 0.545 0.47  0.507 0.    0.715 0.295 0.835 0.653 0.549 0.54\n",
            " 0.766 0.517 0.43  0.299 0.96  0.578 0.531 0.408 0.575 0.481 0.648 0.692\n",
            " 0.631 0.58  0.326 0.348 0.731 0.516 0.466 0.    1.637]\n",
            "[0.    0.    0.    0.505 0.609 0.983 0.939 0.931 0.795 0.835 0.753 2.998\n",
            " 0.715 0.694 0.442 1.024 0.356 0.681 0.857 0.683 0.697 1.108 0.    0.765\n",
            " 0.583 0.615 0.952 0.794 0.333 0.506 0.541 0.    0.701 0.33  0.733 0.712\n",
            " 0.523 0.467 0.934 1.931 0.727 0.946 0.651 2.81  0.739 0.539 0.633 1.124\n",
            " 0.579 0.642 0.72  0.441 0.466 0.306 0.411 0.694 0.724 0.    0.774 0.361\n",
            " 0.731 0.699 0.891 0.421 0.647 0.545 0.47  0.507 0.    0.715 0.295 0.835\n",
            " 0.653 0.549 0.54  0.766 0.517 0.43  0.299 0.96  0.578 0.531 0.408 0.575\n",
            " 0.481 0.648 0.692 0.631 0.58  0.326 0.348 0.731 0.516 0.466 0.    1.637\n",
            " 0.    0.    0.    0.   ]\n",
            "[0.    0.    0.    0.164 0.166 0.188 0.171 0.196 0.17  0.141 0.137 0.183\n",
            " 0.119 0.145 0.186 0.165 0.139 0.174 0.158 0.124 0.147 0.157 0.18  0.199\n",
            " 0.165 0.165 0.141 0.162 0.149 0.182 0.141 0.191 0.156 0.079 0.14  0.197\n",
            " 0.19  0.133 0.166 0.18  0.153 0.132 0.159 0.181 0.147 0.131 0.116 0.106\n",
            " 0.178 0.166 0.155 0.108 0.132 0.098 0.159 0.155 0.117 0.173 0.152 0.142\n",
            " 0.147 0.156 0.146 0.096 0.123 0.143 0.162 0.159 0.192 0.145 0.119 0.149\n",
            " 0.145 0.158 0.191 0.166 0.191 0.071 0.148 0.158 0.152 0.166 0.158 0.174\n",
            " 0.148 0.148 0.208 0.139 0.131 0.125 0.148 0.165 0.125 0.125 0.155 0.146\n",
            " 0.096 0.    0.    0.    0.   ]\n",
            "(100,)\n",
            "<class 'int'>\n",
            "185\n",
            "/content/gdrive/My Drive/thesis/Data/S08/S08_TEX04.txt\n",
            "8\n",
            "[0.38  0.681 0.928 1.634 0.555 0.627 0.545 0.341 0.844 0.505 0.315 0.624\n",
            " 1.312 1.823 0.715 0.603 0.642 0.692 0.577 0.641 0.664 0.651 0.804 0.872\n",
            " 1.449 0.679 1.306 0.566 0.729 1.092 4.842 0.555 0.674 0.908 0.629 0.566\n",
            " 0.548 0.726 0.679 0.44  0.64  0.574 0.49  0.705 0.399 0.732 0.665 0.822\n",
            " 0.649 4.829 0.655 0.699 0.631 0.582 0.665 0.664 0.576]\n",
            "[0.38  0.681 0.928 1.634 0.555 0.627 0.545 0.341 0.844 0.505 0.315 0.624\n",
            " 1.312 1.823 0.715 0.603 0.642 0.692 0.577 0.641 0.664 0.651 0.804 0.872\n",
            " 1.449 0.679 1.306 0.566 0.729 1.092 0.    0.555 0.674 0.908 0.629 0.566\n",
            " 0.548 0.726 0.679 0.44  0.64  0.574 0.49  0.705 0.399 0.732 0.665 0.822\n",
            " 0.649 0.    0.655 0.699 0.631 0.582 0.665 0.664 0.576]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.38  0.681 0.928\n",
            " 1.634 0.555 0.627 0.545 0.341 0.844 0.505 0.315 0.624 1.312 1.823 0.715\n",
            " 0.603 0.642 0.692 0.577 0.641 0.664 0.651 0.804 0.872 1.449 0.679 1.306\n",
            " 0.566 0.729 1.092 0.    0.555 0.674 0.908 0.629 0.566 0.548 0.726 0.679\n",
            " 0.44  0.64  0.574 0.49  0.705 0.399 0.732 0.665 0.822 0.649 0.    0.655\n",
            " 0.699 0.631 0.582 0.665 0.664 0.576 0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.   ]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.156 0.141 0.167\n",
            " 0.194 0.13  0.15  0.112 0.166 0.182 0.136 0.123 0.182 0.148 0.181 0.146\n",
            " 0.119 0.15  0.146 0.168 0.124 0.124 0.148 0.138 0.181 0.206 0.164 0.175\n",
            " 0.157 0.164 0.15  0.162 0.165 0.141 0.19  0.164 0.166 0.157 0.19  0.18\n",
            " 0.123 0.124 0.123 0.164 0.148 0.158 0.166 0.141 0.182 0.149 0.173 0.131\n",
            " 0.132 0.181 0.182 0.157 0.156 0.141 0.156 0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.   ]\n",
            "(100,)\n",
            "<class 'int'>\n",
            "186\n",
            "/content/gdrive/My Drive/thesis/Data/S08/S08_TEX09.txt\n",
            "8\n",
            "[0.529 0.632 0.635 0.619 0.516 0.54  0.632 0.55  0.265 0.807 1.996 0.647\n",
            " 0.399 0.592 0.481 0.406 0.375 0.548 0.29  0.443 0.305 0.309 0.615 0.549\n",
            " 1.432 0.645 0.786 2.286 1.048 1.259 0.901 0.915 1.355 0.619 0.341 0.665\n",
            " 0.433 0.64  0.549 0.481 1.018 0.786 0.582 0.65  0.564 0.64  0.843 0.834\n",
            " 0.652 0.59 ]\n",
            "[0.529 0.632 0.635 0.619 0.516 0.54  0.632 0.55  0.265 0.807 1.996 0.647\n",
            " 0.399 0.592 0.481 0.406 0.375 0.548 0.29  0.443 0.305 0.309 0.615 0.549\n",
            " 1.432 0.645 0.786 2.286 1.048 1.259 0.901 0.915 1.355 0.619 0.341 0.665\n",
            " 0.433 0.64  0.549 0.481 1.018 0.786 0.582 0.65  0.564 0.64  0.843 0.834\n",
            " 0.652 0.59 ]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.529 0.632 0.635 0.619 0.516 0.54  0.632 0.55  0.265 0.807 1.996\n",
            " 0.647 0.399 0.592 0.481 0.406 0.375 0.548 0.29  0.443 0.305 0.309 0.615\n",
            " 0.549 1.432 0.645 0.786 2.286 1.048 1.259 0.901 0.915 1.355 0.619 0.341\n",
            " 0.665 0.433 0.64  0.549 0.481 1.018 0.786 0.582 0.65  0.564 0.64  0.843\n",
            " 0.834 0.652 0.59  0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.   ]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.112 0.131 0.148 0.162 0.141 0.132 0.157 0.116 0.123 0.107 0.169 0.13\n",
            " 0.157 0.141 0.139 0.115 0.141 0.156 0.132 0.125 0.156 0.167 0.182 0.123\n",
            " 0.124 0.129 0.141 0.137 0.162 0.128 0.14  0.145 0.129 0.145 0.158 0.182\n",
            " 0.125 0.149 0.174 0.157 0.141 0.154 0.141 0.157 0.131 0.131 0.132 0.146\n",
            " 0.168 0.139 0.156 0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.   ]\n",
            "(100,)\n",
            "<class 'int'>\n",
            "187\n",
            "/content/gdrive/My Drive/thesis/Data/S08/S08_TEX03.txt\n",
            "8\n",
            "[0.615 0.665 0.541 0.473 0.789 0.9   0.696 0.374 0.283 0.74  0.415 0.968\n",
            " 3.221 0.918 0.676 0.417 0.308 0.34  0.748 0.883 0.637 0.599 0.69  0.714\n",
            " 1.94  0.404 0.844 0.668 0.765 0.639 0.449 0.574 0.706 0.491 0.702 0.427\n",
            " 0.583 0.507 0.599 1.889 0.695 0.885 0.897 0.702 0.55  0.473 1.214 2.538\n",
            " 0.789 0.562 0.417 0.774 0.739 0.608 0.582 0.473 0.383 0.474 0.449 0.739]\n",
            "[0.615 0.665 0.541 0.473 0.789 0.9   0.696 0.374 0.283 0.74  0.415 0.968\n",
            " 0.    0.918 0.676 0.417 0.308 0.34  0.748 0.883 0.637 0.599 0.69  0.714\n",
            " 1.94  0.404 0.844 0.668 0.765 0.639 0.449 0.574 0.706 0.491 0.702 0.427\n",
            " 0.583 0.507 0.599 1.889 0.695 0.885 0.897 0.702 0.55  0.473 1.214 2.538\n",
            " 0.789 0.562 0.417 0.774 0.739 0.608 0.582 0.473 0.383 0.474 0.449 0.739]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.615 0.665 0.541 0.473\n",
            " 0.789 0.9   0.696 0.374 0.283 0.74  0.415 0.968 0.    0.918 0.676 0.417\n",
            " 0.308 0.34  0.748 0.883 0.637 0.599 0.69  0.714 1.94  0.404 0.844 0.668\n",
            " 0.765 0.639 0.449 0.574 0.706 0.491 0.702 0.427 0.583 0.507 0.599 1.889\n",
            " 0.695 0.885 0.897 0.702 0.55  0.473 1.214 2.538 0.789 0.562 0.417 0.774\n",
            " 0.739 0.608 0.582 0.473 0.383 0.474 0.449 0.739 0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.   ]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.132 0.14  0.157 0.107 0.132\n",
            " 0.133 0.13  0.107 0.108 0.132 0.166 0.142 0.138 0.148 0.128 0.15  0.165\n",
            " 0.123 0.124 0.124 0.188 0.132 0.183 0.123 0.166 0.162 0.14  0.152 0.149\n",
            " 0.14  0.149 0.133 0.182 0.133 0.173 0.119 0.142 0.149 0.165 0.14  0.112\n",
            " 0.157 0.187 0.128 0.158 0.123 0.132 0.149 0.162 0.163 0.15  0.182 0.141\n",
            " 0.182 0.149 0.165 0.132 0.124 0.19  0.166 0.183 0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.   ]\n",
            "(100,)\n",
            "<class 'int'>\n",
            "188\n",
            "/content/gdrive/My Drive/thesis/Data/S08/S08_TEX02.txt\n",
            "8\n",
            "[0.296 0.913 0.735 0.529 0.524 0.682 0.416 1.115 0.539 0.465 0.625 0.631\n",
            " 0.647 0.782 1.716 0.561 0.524 0.74  0.902 0.672 0.846 0.732 0.589 0.59\n",
            " 0.481 0.441 0.74  0.557 0.706 1.247 3.66  1.223 0.569 0.499 0.607 0.608\n",
            " 0.909 0.789 0.544 1.541 0.714 0.823 2.18  0.553 0.698 0.649 0.482 0.491\n",
            " 0.78  0.507 0.882 0.5   0.398 0.599 0.715 0.539 0.566 0.608 0.489 0.416\n",
            " 0.689 1.35  1.511 1.015 0.875 0.707 0.336 0.724]\n",
            "[0.296 0.913 0.735 0.529 0.524 0.682 0.416 1.115 0.539 0.465 0.625 0.631\n",
            " 0.647 0.782 1.716 0.561 0.524 0.74  0.902 0.672 0.846 0.732 0.589 0.59\n",
            " 0.481 0.441 0.74  0.557 0.706 1.247 0.    1.223 0.569 0.499 0.607 0.608\n",
            " 0.909 0.789 0.544 1.541 0.714 0.823 2.18  0.553 0.698 0.649 0.482 0.491\n",
            " 0.78  0.507 0.882 0.5   0.398 0.599 0.715 0.539 0.566 0.608 0.489 0.416\n",
            " 0.689 1.35  1.511 1.015 0.875 0.707 0.336 0.724]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.296 0.913 0.735 0.529 0.524 0.682 0.416 1.115\n",
            " 0.539 0.465 0.625 0.631 0.647 0.782 1.716 0.561 0.524 0.74  0.902 0.672\n",
            " 0.846 0.732 0.589 0.59  0.481 0.441 0.74  0.557 0.706 1.247 0.    1.223\n",
            " 0.569 0.499 0.607 0.608 0.909 0.789 0.544 1.541 0.714 0.823 2.18  0.553\n",
            " 0.698 0.649 0.482 0.491 0.78  0.507 0.882 0.5   0.398 0.599 0.715 0.539\n",
            " 0.566 0.608 0.489 0.416 0.689 1.35  1.511 1.015 0.875 0.707 0.336 0.724\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.   ]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.121 0.182 0.158 0.163 0.173 0.14  0.132 0.157 0.122\n",
            " 0.157 0.191 0.123 0.107 0.15  0.199 0.162 0.157 0.157 0.158 0.144 0.113\n",
            " 0.147 0.146 0.181 0.165 0.149 0.124 0.181 0.182 0.141 0.173 0.146 0.153\n",
            " 0.149 0.142 0.132 0.156 0.144 0.154 0.166 0.138 0.164 0.197 0.162 0.156\n",
            " 0.166 0.157 0.198 0.198 0.141 0.233 0.157 0.148 0.107 0.207 0.148 0.174\n",
            " 0.091 0.131 0.132 0.141 0.141 0.196 0.165 0.189 0.145 0.103 0.158 0.165\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.   ]\n",
            "(100,)\n",
            "<class 'int'>\n",
            "189\n",
            "/content/gdrive/My Drive/thesis/Data/S12/S12_TEX02.txt\n",
            "12\n",
            "[0.882 1.15  0.647 1.674 2.603 2.658 1.484 2.493 1.229 1.323 3.244 0.905\n",
            " 0.979 4.435 0.964 1.821 4.457 1.946 0.806 1.006 4.449 0.727 0.732 0.968\n",
            " 3.455 0.726 1.1   2.583 1.8   1.113 0.769 1.267 0.721 0.884 3.32  1.26\n",
            " 3.812 1.509 3.259 3.161 2.331 1.626 3.239 4.34  0.829 0.746 3.704 0.611\n",
            " 3.056 2.502 1.104 0.614 3.709 0.639]\n",
            "[0.882 1.15  0.647 1.674 2.603 2.658 1.484 2.493 1.229 1.323 0.    0.905\n",
            " 0.979 0.    0.964 1.821 0.    1.946 0.806 1.006 0.    0.727 0.732 0.968\n",
            " 0.    0.726 1.1   2.583 1.8   1.113 0.769 1.267 0.721 0.884 0.    1.26\n",
            " 0.    1.509 0.    0.    2.331 1.626 0.    0.    0.829 0.746 0.    0.611\n",
            " 0.    2.502 1.104 0.614 0.    0.639]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.882\n",
            " 1.15  0.647 1.674 2.603 2.658 1.484 2.493 1.229 1.323 0.    0.905 0.979\n",
            " 0.    0.964 1.821 0.    1.946 0.806 1.006 0.    0.727 0.732 0.968 0.\n",
            " 0.726 1.1   2.583 1.8   1.113 0.769 1.267 0.721 0.884 0.    1.26  0.\n",
            " 1.509 0.    0.    2.331 1.626 0.    0.    0.829 0.746 0.    0.611 0.\n",
            " 2.502 1.104 0.614 0.    0.639 0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.   ]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.084 0.085\n",
            " 0.083 0.066 0.081 0.071 0.081 0.12  0.079 0.105 0.097 0.077 0.088 0.081\n",
            " 0.077 0.079 0.111 0.06  0.085 0.095 0.104 0.12  0.117 0.082 0.12  0.083\n",
            " 0.089 0.119 0.106 0.085 0.129 0.1   0.13  0.091 0.105 0.127 0.082 0.095\n",
            " 0.116 0.107 0.115 0.087 0.081 0.076 0.093 0.087 0.114 0.12  0.132 0.086\n",
            " 0.07  0.114 0.098 0.097 0.064 0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.   ]\n",
            "(100,)\n",
            "<class 'int'>\n",
            "190\n",
            "/content/gdrive/My Drive/thesis/Data/S12/S12_TEX09.txt\n",
            "12\n",
            "[0.659 0.961 3.509 0.996 0.791 2.389 0.744 0.687 1.408 2.312 2.316 0.515\n",
            " 2.773 0.805 0.765 1.305 2.768 0.747 1.058 2.994 0.711 1.325 3.318 0.804\n",
            " 1.131 4.08  0.565 2.163 0.867 2.683 2.679 0.913 2.708 3.061 0.648 3.144\n",
            " 1.463 0.799 2.584 1.034 0.985 3.062 0.761 1.218 0.709 1.456 3.283 1.525\n",
            " 2.077 2.945 0.547 3.806 0.882 1.01  0.744]\n",
            "[0.659 0.961 0.    0.996 0.791 2.389 0.744 0.687 1.408 2.312 2.316 0.515\n",
            " 2.773 0.805 0.765 1.305 2.768 0.747 1.058 2.994 0.711 1.325 0.    0.804\n",
            " 1.131 0.    0.565 2.163 0.867 2.683 2.679 0.913 2.708 0.    0.648 0.\n",
            " 1.463 0.799 2.584 1.034 0.985 0.    0.761 1.218 0.709 1.456 0.    1.525\n",
            " 2.077 2.945 0.547 0.    0.882 1.01  0.744]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.659 0.961\n",
            " 0.    0.996 0.791 2.389 0.744 0.687 1.408 2.312 2.316 0.515 2.773 0.805\n",
            " 0.765 1.305 2.768 0.747 1.058 2.994 0.711 1.325 0.    0.804 1.131 0.\n",
            " 0.565 2.163 0.867 2.683 2.679 0.913 2.708 0.    0.648 0.    1.463 0.799\n",
            " 2.584 1.034 0.985 0.    0.761 1.218 0.709 1.456 0.    1.525 2.077 2.945\n",
            " 0.547 0.    0.882 1.01  0.744 0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.   ]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.06  0.041\n",
            " 0.088 0.111 0.095 0.078 0.075 0.079 0.074 0.061 0.079 0.116 0.067 0.061\n",
            " 0.055 0.055 0.053 0.113 0.072 0.045 0.069 0.091 0.071 0.08  0.097 0.063\n",
            " 0.065 0.058 0.072 0.053 0.063 0.087 0.071 0.066 0.073 0.082 0.072 0.063\n",
            " 0.054 0.09  0.063 0.057 0.065 0.095 0.057 0.055 0.061 0.079 0.092 0.052\n",
            " 0.05  0.053 0.055 0.061 0.048 0.052 0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.   ]\n",
            "(100,)\n",
            "<class 'int'>\n",
            "191\n",
            "/content/gdrive/My Drive/thesis/Data/S12/S12_TEX07.txt\n",
            "12\n",
            "[0.866 1.102 2.83  0.84  2.228 0.501 2.706 1.479 2.704 1.302 0.999 0.812\n",
            " 1.06  1.17  1.001 3.526 0.643 0.733 1.433 3.654 1.376 1.004 0.872 2.255\n",
            " 3.267 2.894 1.878 2.692 1.146 1.198 3.625 0.825 1.397 6.878 0.81  0.528\n",
            " 2.725 0.831 2.992 1.54  0.906 1.379 2.964 0.699 0.978 0.88  2.657 0.705\n",
            " 3.186 0.892 0.762 2.729 1.549 2.918 0.818 2.726 0.748 0.747 4.595 0.895\n",
            " 6.306 3.762 0.662 0.976 2.625 0.858 0.771 0.764 1.076 0.726 0.918 0.513\n",
            " 0.776]\n",
            "[0.866 1.102 2.83  0.84  2.228 0.501 2.706 1.479 2.704 1.302 0.999 0.812\n",
            " 1.06  1.17  1.001 0.    0.643 0.733 1.433 0.    1.376 1.004 0.872 2.255\n",
            " 0.    2.894 1.878 2.692 1.146 1.198 0.    0.825 1.397 0.    0.81  0.528\n",
            " 2.725 0.831 2.992 1.54  0.906 1.379 2.964 0.699 0.978 0.88  2.657 0.705\n",
            " 0.    0.892 0.762 2.729 1.549 2.918 0.818 2.726 0.748 0.747 0.    0.895\n",
            " 0.    0.    0.662 0.976 2.625 0.858 0.771 0.764 1.076 0.726 0.918 0.513\n",
            " 0.776]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.866 1.102 2.83  0.84  2.228 0.501 2.706 1.479 2.704 1.302 0.999\n",
            " 0.812 1.06  1.17  1.001 0.    0.643 0.733 1.433 0.    1.376 1.004 0.872\n",
            " 2.255 0.    2.894 1.878 2.692 1.146 1.198 0.    0.825 1.397 0.    0.81\n",
            " 0.528 2.725 0.831 2.992 1.54  0.906 1.379 2.964 0.699 0.978 0.88  2.657\n",
            " 0.705 0.    0.892 0.762 2.729 1.549 2.918 0.818 2.726 0.748 0.747 0.\n",
            " 0.895 0.    0.    0.662 0.976 2.625 0.858 0.771 0.764 1.076 0.726 0.918\n",
            " 0.513 0.776 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.   ]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.078 0.086 0.107 0.078 0.053 0.102 0.068 0.072 0.089 0.088 0.099\n",
            " 0.09  0.101 0.079 0.09  0.069 0.068 0.09  0.081 0.052 0.09  0.094 0.096\n",
            " 0.047 0.062 0.054 0.06  0.094 0.079 0.107 0.054 0.088 0.103 0.035 0.067\n",
            " 0.08  0.068 0.046 0.122 0.105 0.11  0.07  0.086 0.091 0.091 0.077 0.059\n",
            " 0.097 0.065 0.071 0.052 0.056 0.06  0.099 0.066 0.105 0.105 0.122 0.047\n",
            " 0.052 0.054 0.09  0.061 0.065 0.068 0.105 0.07  0.08  0.09  0.044 0.041\n",
            " 0.079 0.065 0.052 0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.   ]\n",
            "(100,)\n",
            "<class 'int'>\n",
            "192\n",
            "/content/gdrive/My Drive/thesis/Data/S12/S12_TEX03.txt\n",
            "12\n",
            "[ 1.9    0.872  1.127  2.291  2.361  0.93   0.824  3.792  1.87   0.789\n",
            "  2.685  6.22   0.757  1.021 12.628  0.615  1.06   0.72   1.197  0.73\n",
            "  0.801  3.051  0.696  3.445  1.811  5.38   1.067  1.844  3.706  0.873\n",
            "  1.825  4.489  0.604  2.797  0.763  3.42   0.763  1.048  6.38   4.114\n",
            "  1.435  1.456  0.694  0.862  2.909  0.771  1.712  3.592  1.206  0.638\n",
            "  1.167  1.042  1.945  3.511  1.126  0.767  1.066  4.797  0.8    1.043\n",
            "  1.046  0.807  0.758  0.959  5.821  0.93   2.643  0.957  0.712  0.995\n",
            "  2.735  0.71   2.55   2.604  1.138  0.916  1.932  0.745  1.306]\n",
            "[1.9   0.872 1.127 2.291 2.361 0.93  0.824 0.    1.87  0.789 2.685 0.\n",
            " 0.757 1.021 0.    0.615 1.06  0.72  1.197 0.73  0.801 0.    0.696 0.\n",
            " 1.811 0.    1.067 1.844 0.    0.873 1.825 0.    0.604 2.797 0.763 0.\n",
            " 0.763 1.048 0.    0.    1.435 1.456 0.694 0.862 2.909 0.771 1.712 0.\n",
            " 1.206 0.638 1.167 1.042 1.945 0.    1.126 0.767 1.066 0.    0.8   1.043\n",
            " 1.046 0.807 0.758 0.959 0.    0.93  2.643 0.957 0.712 0.995 2.735 0.71\n",
            " 2.55  2.604 1.138 0.916 1.932 0.745 1.306]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    1.9   0.872\n",
            " 1.127 2.291 2.361 0.93  0.824 0.    1.87  0.789 2.685 0.    0.757 1.021\n",
            " 0.    0.615 1.06  0.72  1.197 0.73  0.801 0.    0.696 0.    1.811 0.\n",
            " 1.067 1.844 0.    0.873 1.825 0.    0.604 2.797 0.763 0.    0.763 1.048\n",
            " 0.    0.    1.435 1.456 0.694 0.862 2.909 0.771 1.712 0.    1.206 0.638\n",
            " 1.167 1.042 1.945 0.    1.126 0.767 1.066 0.    0.8   1.043 1.046 0.807\n",
            " 0.758 0.959 0.    0.93  2.643 0.957 0.712 0.995 2.735 0.71  2.55  2.604\n",
            " 1.138 0.916 1.932 0.745 1.306 0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.   ]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.099 0.102\n",
            " 0.07  0.081 0.11  0.077 0.104 0.078 0.078 0.112 0.096 0.113 0.112 0.086\n",
            " 0.147 0.106 0.132 0.12  0.098 0.098 0.083 0.12  0.095 0.098 0.077 0.086\n",
            " 0.087 0.092 0.086 0.088 0.088 0.101 0.06  0.055 0.085 0.096 0.086 0.079\n",
            " 0.104 0.077 0.099 0.094 0.032 0.1   0.069 0.094 0.096 0.114 0.089 0.072\n",
            " 0.075 0.098 0.078 0.078 0.085 0.099 0.073 0.078 0.064 0.086 0.075 0.101\n",
            " 0.059 0.057 0.087 0.07  0.078 0.097 0.104 0.091 0.092 0.068 0.107 0.075\n",
            " 0.106 0.082 0.063 0.102 0.063 0.062 0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.   ]\n",
            "(100,)\n",
            "<class 'int'>\n",
            "193\n",
            "/content/gdrive/My Drive/thesis/Data/S12/S12_TEX05.txt\n",
            "12\n",
            "[ 1.126  5.574  0.814  0.832  0.965  1.488  0.619  3.878  0.529  1.932\n",
            "  3.934  0.658  1.175  2.597  3.587  1.733  0.93   3.68   1.032  4.69\n",
            "  1.588  0.529  2.557  3.096  0.947  0.883  3.2    1.376  0.875  3.514\n",
            "  1.167  1.339  0.872  0.808  0.94   0.873  3.185  1.104  0.869  8.462\n",
            "  1.065  0.627  3.845  0.789  0.539  1.525  4.196  0.799  3.6    0.71\n",
            "  1.024  4.573  0.578  1.382  3.518  0.619  2.596  0.67   3.389  5.056\n",
            "  2.113  0.999  0.655  0.536  0.756  2.845  1.443  1.409  2.049  0.777\n",
            "  1.429  4.375  4.788 10.605  1.36   1.313  3.542  4.154  1.041  0.797\n",
            "  0.922  5.07   3.152  1.565  0.814  3.102  2.475  4.35   0.858  3.697\n",
            "  0.873  5.529  0.644  4.672  1.068  1.43   0.586  2.565  1.044  0.739\n",
            "  1.097  3.519  1.815  0.846  1.692  0.917  1.612]\n",
            "[1.126 0.    0.814 0.832 0.965 1.488 0.619 0.    0.529 1.932 0.    0.658\n",
            " 1.175 2.597 0.    1.733 0.93  0.    1.032 0.    1.588 0.529 2.557 0.\n",
            " 0.947 0.883 0.    1.376 0.875 0.    1.167 1.339 0.872 0.808 0.94  0.873\n",
            " 0.    1.104 0.869 0.    1.065 0.627 0.    0.789 0.539 1.525 0.    0.799\n",
            " 0.    0.71  1.024 0.    0.578 1.382 0.    0.619 2.596 0.67  0.    0.\n",
            " 2.113 0.999 0.655 0.536 0.756 2.845 1.443 1.409 2.049 0.777 1.429 0.\n",
            " 0.    0.    1.36  1.313 0.    0.    1.041 0.797 0.922 0.    0.    1.565\n",
            " 0.814 0.    2.475 0.    0.858 0.    0.873 0.    0.644 0.    1.068 1.43\n",
            " 0.586 2.565 1.044 0.739 1.097 0.    1.815 0.846 1.692 0.917 1.612]\n",
            "[1.126 0.    0.814 0.832 0.965 1.488 0.619 0.    0.529 1.932 0.    0.658\n",
            " 1.175 2.597 0.    1.733 0.93  0.    1.032 0.    1.588 0.529 2.557 0.\n",
            " 0.947 0.883 0.    1.376 0.875 0.    1.167 1.339 0.872 0.808 0.94  0.873\n",
            " 0.    1.104 0.869 0.    1.065 0.627 0.    0.789 0.539 1.525 0.    0.799\n",
            " 0.    0.71  1.024 0.    0.578 1.382 0.    0.619 2.596 0.67  0.    0.\n",
            " 2.113 0.999 0.655 0.536 0.756 2.845 1.443 1.409 2.049 0.777 1.429 0.\n",
            " 0.    0.    1.36  1.313 0.    0.    1.041 0.797 0.922 0.    0.    1.565\n",
            " 0.814 0.    2.475 0.    0.858 0.    0.873 0.    0.644 0.    1.068 1.43\n",
            " 0.586 2.565 1.044 0.739]\n",
            "[0.068 0.09  0.103 0.088 0.069 0.077 0.087 0.074 0.094 0.081 0.095 0.101\n",
            " 0.084 0.082 0.076 0.082 0.078 0.137 0.074 0.072 0.063 0.095 0.065 0.084\n",
            " 0.082 0.108 0.063 0.09  0.059 0.082 0.059 0.09  0.089 0.107 0.097 0.081\n",
            " 0.097 0.071 0.123 0.086 0.079 0.086 0.081 0.085 0.063 0.047 0.059 0.095\n",
            " 0.087 0.111 0.108 0.113 0.061 0.073 0.062 0.086 0.056 0.096 0.067 0.093\n",
            " 0.05  0.098 0.089 0.056 0.079 0.07  0.045 0.058 0.03  0.058 0.095 0.07\n",
            " 0.077 0.061 0.092 0.112 0.095 0.068 0.071 0.053 0.053 0.064 0.036 0.064\n",
            " 0.079 0.079 0.044 0.089 0.046 0.059 0.087 0.062 0.06  0.065 0.065 0.077\n",
            " 0.053 0.1   0.077 0.107]\n",
            "(100,)\n",
            "<class 'int'>\n",
            "194\n",
            "/content/gdrive/My Drive/thesis/Data/S12/S12_TEX08.txt\n",
            "12\n",
            "[0.596 2.136 0.671 3.846 0.756 1.462 0.656 1.171 0.632 1.181 1.982 5.744\n",
            " 2.229 1.535 1.067 2.52  1.025 1.131 0.835 0.915 0.876 0.583 2.204 1.458\n",
            " 4.007 0.752 2.24  0.939 0.579 1.808 0.746 0.55  0.723 3.036 0.761 0.801\n",
            " 2.545 1.394 0.784 0.623 0.76  1.242 1.823 0.854 5.333 1.268 3.95  2.692\n",
            " 1.834 1.004 3.881 0.702 0.835 1.221]\n",
            "[0.596 2.136 0.671 0.    0.756 1.462 0.656 1.171 0.632 1.181 1.982 0.\n",
            " 2.229 1.535 1.067 2.52  1.025 1.131 0.835 0.915 0.876 0.583 2.204 1.458\n",
            " 0.    0.752 2.24  0.939 0.579 1.808 0.746 0.55  0.723 0.    0.761 0.801\n",
            " 2.545 1.394 0.784 0.623 0.76  1.242 1.823 0.854 0.    1.268 0.    2.692\n",
            " 1.834 1.004 0.    0.702 0.835 1.221]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.596\n",
            " 2.136 0.671 0.    0.756 1.462 0.656 1.171 0.632 1.181 1.982 0.    2.229\n",
            " 1.535 1.067 2.52  1.025 1.131 0.835 0.915 0.876 0.583 2.204 1.458 0.\n",
            " 0.752 2.24  0.939 0.579 1.808 0.746 0.55  0.723 0.    0.761 0.801 2.545\n",
            " 1.394 0.784 0.623 0.76  1.242 1.823 0.854 0.    1.268 0.    2.692 1.834\n",
            " 1.004 0.    0.702 0.835 1.221 0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.   ]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.103 0.09\n",
            " 0.064 0.082 0.079 0.104 0.057 0.09  0.091 0.065 0.074 0.044 0.062 0.077\n",
            " 0.097 0.086 0.102 0.083 0.125 0.121 0.061 0.074 0.082 0.064 0.019 0.103\n",
            " 0.066 0.077 0.055 0.065 0.078 0.097 0.053 0.062 0.103 0.082 0.087 0.068\n",
            " 0.081 0.094 0.07  0.067 0.045 0.069 0.062 0.083 0.087 0.045 0.071 0.05\n",
            " 0.094 0.078 0.083 0.062 0.054 0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.   ]\n",
            "(100,)\n",
            "<class 'int'>\n",
            "195\n",
            "/content/gdrive/My Drive/thesis/Data/S12/S12_TEX10.txt\n",
            "12\n",
            "[0.787 0.807 4.049 0.679 1.14  2.598 1.437 2.421 0.619 1.507 1.695 1.\n",
            " 4.307 1.685 0.824 0.991 1.005 5.227 6.406 2.242 1.129 2.807 0.905 2.642\n",
            " 0.699 1.3   1.186 0.777 0.877 0.967 3.334 0.869 0.829 3.689 1.031 0.53\n",
            " 4.959 0.905 0.467 1.047 3.387 1.913 0.517 0.698 1.4   5.548 0.596 2.59\n",
            " 0.775 3.026 1.415 1.23  0.773 2.269 2.288 0.904 0.84  1.815 3.862 1.283\n",
            " 4.17  1.954 2.555 0.856 0.612 1.058]\n",
            "[0.787 0.807 0.    0.679 1.14  2.598 1.437 2.421 0.619 1.507 1.695 1.\n",
            " 0.    1.685 0.824 0.991 1.005 0.    0.    2.242 1.129 2.807 0.905 2.642\n",
            " 0.699 1.3   1.186 0.777 0.877 0.967 0.    0.869 0.829 0.    1.031 0.53\n",
            " 0.    0.905 0.467 1.047 0.    1.913 0.517 0.698 1.4   0.    0.596 2.59\n",
            " 0.775 0.    1.415 1.23  0.773 2.269 2.288 0.904 0.84  1.815 0.    1.283\n",
            " 0.    1.954 2.555 0.856 0.612 1.058]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.787 0.807 0.    0.679 1.14  2.598 1.437\n",
            " 2.421 0.619 1.507 1.695 1.    0.    1.685 0.824 0.991 1.005 0.    0.\n",
            " 2.242 1.129 2.807 0.905 2.642 0.699 1.3   1.186 0.777 0.877 0.967 0.\n",
            " 0.869 0.829 0.    1.031 0.53  0.    0.905 0.467 1.047 0.    1.913 0.517\n",
            " 0.698 1.4   0.    0.596 2.59  0.775 0.    1.415 1.23  0.773 2.269 2.288\n",
            " 0.904 0.84  1.815 0.    1.283 0.    1.954 2.555 0.856 0.612 1.058 0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.   ]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.068 0.071 0.07  0.054 0.067 0.082 0.044 0.036\n",
            " 0.069 0.057 0.063 0.063 0.052 0.069 0.055 0.079 0.103 0.054 0.065 0.103\n",
            " 0.064 0.065 0.053 0.061 0.073 0.079 0.095 0.065 0.086 0.048 0.053 0.062\n",
            " 0.049 0.053 0.063 0.088 0.05  0.069 0.071 0.052 0.086 0.049 0.032 0.048\n",
            " 0.047 0.035 0.058 0.06  0.074 0.062 0.062 0.052 0.045 0.069 0.087 0.077\n",
            " 0.07  0.046 0.094 0.054 0.068 0.049 0.064 0.053 0.096 0.04  0.046 0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.   ]\n",
            "(100,)\n",
            "<class 'int'>\n",
            "196\n",
            "/content/gdrive/My Drive/thesis/Data/S12/S12_TEX06.txt\n",
            "12\n",
            "[0.868 0.636 0.802 1.013 1.057 4.1   2.099 2.77  0.538 0.65  3.007 0.719\n",
            " 0.472 1.114 4.361 3.403 0.704 1.168 4.809 1.21  0.876 1.777 0.654 0.735\n",
            " 1.315 2.986 1.65  0.844 1.677 6.64  0.772 2.063 0.605 1.21  3.243 1.143\n",
            " 0.862 0.988 1.928 0.905 3.305 0.867 1.384 0.625 0.852 1.045 2.496 0.677\n",
            " 1.267 1.052 3.619 3.626 3.004 3.911 0.814 1.351 4.138 1.188 1.13  5.283\n",
            " 0.703 1.049 3.336 2.191 2.143 1.166 3.152 0.557 1.594 4.598 0.759 3.733\n",
            " 1.296]\n",
            "[0.868 0.636 0.802 1.013 1.057 0.    2.099 2.77  0.538 0.65  0.    0.719\n",
            " 0.472 1.114 0.    0.    0.704 1.168 0.    1.21  0.876 1.777 0.654 0.735\n",
            " 1.315 2.986 1.65  0.844 1.677 0.    0.772 2.063 0.605 1.21  0.    1.143\n",
            " 0.862 0.988 1.928 0.905 0.    0.867 1.384 0.625 0.852 1.045 2.496 0.677\n",
            " 1.267 1.052 0.    0.    0.    0.    0.814 1.351 0.    1.188 1.13  0.\n",
            " 0.703 1.049 0.    2.191 2.143 1.166 0.    0.557 1.594 0.    0.759 0.\n",
            " 1.296]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.868 0.636 0.802 1.013 1.057 0.    2.099 2.77  0.538 0.65  0.\n",
            " 0.719 0.472 1.114 0.    0.    0.704 1.168 0.    1.21  0.876 1.777 0.654\n",
            " 0.735 1.315 2.986 1.65  0.844 1.677 0.    0.772 2.063 0.605 1.21  0.\n",
            " 1.143 0.862 0.988 1.928 0.905 0.    0.867 1.384 0.625 0.852 1.045 2.496\n",
            " 0.677 1.267 1.052 0.    0.    0.    0.    0.814 1.351 0.    1.188 1.13\n",
            " 0.    0.703 1.049 0.    2.191 2.143 1.166 0.    0.557 1.594 0.    0.759\n",
            " 0.    1.296 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.   ]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.097 0.053 0.083 0.078 0.08  0.078 0.06  0.108 0.097 0.1   0.073\n",
            " 0.042 0.072 0.074 0.072 0.06  0.043 0.069 0.055 0.085 0.08  0.07  0.047\n",
            " 0.074 0.08  0.085 0.059 0.081 0.027 0.069 0.048 0.091 0.098 0.082 0.084\n",
            " 0.101 0.064 0.057 0.076 0.069 0.087 0.049 0.088 0.101 0.073 0.046 0.055\n",
            " 0.102 0.083 0.045 0.049 0.055 0.055 0.068 0.048 0.058 0.036 0.063 0.055\n",
            " 0.072 0.044 0.057 0.045 0.077 0.056 0.091 0.056 0.081 0.063 0.064 0.072\n",
            " 0.061 0.063 0.069 0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.   ]\n",
            "(100,)\n",
            "<class 'int'>\n",
            "197\n",
            "/content/gdrive/My Drive/thesis/Data/S12/S12_TEX04.txt\n",
            "12\n",
            "[ 1.037  0.766  1.337  0.697  2.196  0.473  1.129  2.39   0.729  0.6\n",
            "  0.719  1.631  2.784  0.949  1.133  1.822  1.055  2.852 11.374  1.826\n",
            "  0.502  4.942  0.914  4.07   0.665  0.932  2.74   1.668  1.532  1.006\n",
            "  0.713  1.002  4.315  0.822  2.896  1.455  4.137  0.799  1.661  0.714\n",
            "  0.964  6.622  1.507  3.274  3.715]\n",
            "[1.037 0.766 1.337 0.697 2.196 0.473 1.129 2.39  0.729 0.6   0.719 1.631\n",
            " 2.784 0.949 1.133 1.822 1.055 2.852 0.    1.826 0.502 0.    0.914 0.\n",
            " 0.665 0.932 2.74  1.668 1.532 1.006 0.713 1.002 0.    0.822 2.896 1.455\n",
            " 0.    0.799 1.661 0.714 0.964 0.    1.507 0.    0.   ]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    1.037 0.766 1.337 0.697 2.196 0.473 1.129 2.39  0.729\n",
            " 0.6   0.719 1.631 2.784 0.949 1.133 1.822 1.055 2.852 0.    1.826 0.502\n",
            " 0.    0.914 0.    0.665 0.932 2.74  1.668 1.532 1.006 0.713 1.002 0.\n",
            " 0.822 2.896 1.455 0.    0.799 1.661 0.714 0.964 0.    1.507 0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.   ]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.061 0.098 0.078 0.098 0.107 0.072 0.047 0.116 0.088\n",
            " 0.08  0.096 0.085 0.116 0.076 0.064 0.071 0.086 0.096 0.052 0.074 0.078\n",
            " 0.066 0.07  0.077 0.107 0.073 0.09  0.12  0.075 0.079 0.114 0.099 0.052\n",
            " 0.044 0.079 0.069 0.026 0.105 0.105 0.09  0.107 0.082 0.103 0.084 0.07\n",
            " 0.07  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.   ]\n",
            "(100,)\n",
            "<class 'int'>\n",
            "198\n",
            "/content/gdrive/My Drive/thesis/Data/S12/S12_TEX11.txt\n",
            "12\n",
            "[ 1.624  0.927  3.124  2.666  1.347  1.99   0.79   0.854  1.595  0.792\n",
            "  0.671  0.566  0.867  3.78   0.841  0.665  0.656  1.042  4.031  0.882\n",
            "  3.759  0.893  1.477  3.871  0.503  1.405  1.625  0.769  0.634  3.064\n",
            "  0.984  0.838  2.083  1.652  0.833  1.105  3.243  0.603  1.139  5.279\n",
            "  0.523  0.922  0.736  3.076  0.671  2.154  2.598  1.518  1.146  1.012\n",
            "  2.305  3.194 32.922  0.861  2.165  2.919  2.633  1.24   1.17   4.1\n",
            "  1.737  5.736  2.72   1.222  2.908]\n",
            "[1.624 0.927 0.    2.666 1.347 1.99  0.79  0.854 1.595 0.792 0.671 0.566\n",
            " 0.867 0.    0.841 0.665 0.656 1.042 0.    0.882 0.    0.893 1.477 0.\n",
            " 0.503 1.405 1.625 0.769 0.634 0.    0.984 0.838 2.083 1.652 0.833 1.105\n",
            " 0.    0.603 1.139 0.    0.523 0.922 0.736 0.    0.671 2.154 2.598 1.518\n",
            " 1.146 1.012 2.305 0.    0.    0.861 2.165 2.919 2.633 1.24  1.17  0.\n",
            " 1.737 0.    2.72  1.222 2.908]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    1.624 0.927 0.    2.666 1.347 1.99  0.79\n",
            " 0.854 1.595 0.792 0.671 0.566 0.867 0.    0.841 0.665 0.656 1.042 0.\n",
            " 0.882 0.    0.893 1.477 0.    0.503 1.405 1.625 0.769 0.634 0.    0.984\n",
            " 0.838 2.083 1.652 0.833 1.105 0.    0.603 1.139 0.    0.523 0.922 0.736\n",
            " 0.    0.671 2.154 2.598 1.518 1.146 1.012 2.305 0.    0.    0.861 2.165\n",
            " 2.919 2.633 1.24  1.17  0.    1.737 0.    2.72  1.222 2.908 0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.   ]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.094 0.075 0.086 0.064 0.074 0.074 0.071\n",
            " 0.021 0.056 0.083 0.08  0.066 0.073 0.088 0.049 0.081 0.115 0.057 0.054\n",
            " 0.039 0.062 0.079 0.074 0.029 0.088 0.068 0.019 0.064 0.093 0.107 0.068\n",
            " 0.081 0.067 0.063 0.115 0.046 0.07  0.07  0.041 0.065 0.057 0.041 0.058\n",
            " 0.047 0.063 0.073 0.088 0.028 0.056 0.056 0.05  0.055 0.062 0.115 0.068\n",
            " 0.097 0.079 0.089 0.079 0.064 0.065 0.082 0.056 0.032 0.071 0.056 0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.   ]\n",
            "(100,)\n",
            "<class 'int'>\n",
            "199\n",
            "/content/gdrive/My Drive/thesis/Data/S13/S13_TEX09.txt\n",
            "13\n",
            "[0.954 0.763 0.713 1.891 0.911 0.623 0.639 0.598 1.271 0.698 0.93  0.984\n",
            " 0.645 0.59  0.457 0.674 1.915 0.618 1.1   0.763 1.286 0.648 0.615 0.798\n",
            " 0.697 0.832 1.038 0.959 0.887 0.713 0.747 2.391 1.06  0.745 0.45  1.588\n",
            " 0.932 0.778 0.871 0.883 0.732 0.572 0.815 1.433 0.569 1.124 0.646 0.893\n",
            " 0.792 1.119 0.653 1.175 0.535 1.305 1.064 1.157 0.753 0.968 1.114 0.694\n",
            " 0.789 0.864 0.566 0.749 0.796 0.632 0.839 0.918]\n",
            "[0.954 0.763 0.713 1.891 0.911 0.623 0.639 0.598 1.271 0.698 0.93  0.984\n",
            " 0.645 0.59  0.457 0.674 1.915 0.618 1.1   0.763 1.286 0.648 0.615 0.798\n",
            " 0.697 0.832 1.038 0.959 0.887 0.713 0.747 2.391 1.06  0.745 0.45  1.588\n",
            " 0.932 0.778 0.871 0.883 0.732 0.572 0.815 1.433 0.569 1.124 0.646 0.893\n",
            " 0.792 1.119 0.653 1.175 0.535 1.305 1.064 1.157 0.753 0.968 1.114 0.694\n",
            " 0.789 0.864 0.566 0.749 0.796 0.632 0.839 0.918]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.954 0.763 0.713 1.891 0.911 0.623 0.639 0.598\n",
            " 1.271 0.698 0.93  0.984 0.645 0.59  0.457 0.674 1.915 0.618 1.1   0.763\n",
            " 1.286 0.648 0.615 0.798 0.697 0.832 1.038 0.959 0.887 0.713 0.747 2.391\n",
            " 1.06  0.745 0.45  1.588 0.932 0.778 0.871 0.883 0.732 0.572 0.815 1.433\n",
            " 0.569 1.124 0.646 0.893 0.792 1.119 0.653 1.175 0.535 1.305 1.064 1.157\n",
            " 0.753 0.968 1.114 0.694 0.789 0.864 0.566 0.749 0.796 0.632 0.839 0.918\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.   ]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.228 0.23  0.173 0.242 0.261 0.181 0.223 0.199 0.191\n",
            " 0.174 0.315 0.21  0.195 0.165 0.199 0.208 0.202 0.177 0.182 0.171 0.214\n",
            " 0.157 0.191 0.174 0.139 0.241 0.298 0.198 0.221 0.207 0.208 0.249 0.143\n",
            " 0.188 0.195 0.198 0.196 0.186 0.231 0.201 0.165 0.181 0.215 0.273 0.169\n",
            " 0.239 0.204 0.19  0.194 0.175 0.219 0.222 0.152 0.224 0.249 0.19  0.197\n",
            " 0.182 0.203 0.169 0.24  0.22  0.21  0.264 0.197 0.199 0.224 0.215 0.178\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.   ]\n",
            "(100,)\n",
            "<class 'int'>\n",
            "200\n",
            "/content/gdrive/My Drive/thesis/Data/S13/S13_TEX11.txt\n",
            "13\n",
            "[0.764 1.675 0.685 1.596 0.856 0.814 1.    0.744 1.005 0.991 1.637 0.63\n",
            " 3.764 0.873 1.025 1.118 1.166 0.862 0.608 0.939 1.57  0.935 0.421 1.574\n",
            " 0.818 0.992 2.202 0.887 0.44  0.473 0.623 0.74  0.999 1.585 0.706 0.716\n",
            " 1.254 0.398 0.582 0.707 0.64  0.746]\n",
            "[0.764 1.675 0.685 1.596 0.856 0.814 1.    0.744 1.005 0.991 1.637 0.63\n",
            " 0.    0.873 1.025 1.118 1.166 0.862 0.608 0.939 1.57  0.935 0.421 1.574\n",
            " 0.818 0.992 2.202 0.887 0.44  0.473 0.623 0.74  0.999 1.585 0.706 0.716\n",
            " 1.254 0.398 0.582 0.707 0.64  0.746]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.764 1.675 0.685 1.596 0.856 0.814 1.\n",
            " 0.744 1.005 0.991 1.637 0.63  0.    0.873 1.025 1.118 1.166 0.862 0.608\n",
            " 0.939 1.57  0.935 0.421 1.574 0.818 0.992 2.202 0.887 0.44  0.473 0.623\n",
            " 0.74  0.999 1.585 0.706 0.716 1.254 0.398 0.582 0.707 0.64  0.746 0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.   ]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.181 0.207 0.177 0.235 0.215 0.197 0.231 0.145\n",
            " 0.224 0.203 0.18  0.205 0.215 0.243 0.215 0.246 0.242 0.205 0.224 0.223\n",
            " 0.289 0.248 0.186 0.197 0.185 0.215 0.245 0.237 0.156 0.19  0.166 0.185\n",
            " 0.19  0.238 0.182 0.216 0.256 0.198 0.199 0.224 0.182 0.215 0.15  0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.   ]\n",
            "(100,)\n",
            "<class 'int'>\n",
            "201\n",
            "/content/gdrive/My Drive/thesis/Data/S13/S13_TEX05.txt\n",
            "13\n",
            "[1.059 0.879 0.879 0.896 0.773 0.942 1.111 0.866 0.737 0.671 0.708 0.755\n",
            " 0.49  0.722 1.316 0.903 0.64  0.649 0.83  0.755 0.739 0.724 0.606 0.548\n",
            " 1.364 0.816 0.87  0.722 0.723 0.431 0.533 0.348 0.49  0.941 0.997 0.771\n",
            " 0.465 0.739 0.741 0.531 0.674 0.555 0.44  0.633 0.491 0.48  0.499 2.762\n",
            " 3.357 0.662 0.631]\n",
            "[1.059 0.879 0.879 0.896 0.773 0.942 1.111 0.866 0.737 0.671 0.708 0.755\n",
            " 0.49  0.722 1.316 0.903 0.64  0.649 0.83  0.755 0.739 0.724 0.606 0.548\n",
            " 1.364 0.816 0.87  0.722 0.723 0.431 0.533 0.348 0.49  0.941 0.997 0.771\n",
            " 0.465 0.739 0.741 0.531 0.674 0.555 0.44  0.633 0.491 0.48  0.499 2.762\n",
            " 0.    0.662 0.631]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 1.059 0.879 0.879 0.896 0.773 0.942 1.111 0.866 0.737 0.671 0.708 0.755\n",
            " 0.49  0.722 1.316 0.903 0.64  0.649 0.83  0.755 0.739 0.724 0.606 0.548\n",
            " 1.364 0.816 0.87  0.722 0.723 0.431 0.533 0.348 0.49  0.941 0.997 0.771\n",
            " 0.465 0.739 0.741 0.531 0.674 0.555 0.44  0.633 0.491 0.48  0.499 2.762\n",
            " 0.    0.662 0.631 0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.   ]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.265 0.194 0.23  0.223 0.265 0.227 0.262 0.214 0.27  0.206 0.208 0.208\n",
            " 0.191 0.183 0.266 0.229 0.19  0.231 0.23  0.197 0.224 0.175 0.223 0.241\n",
            " 0.231 0.239 0.236 0.18  0.191 0.202 0.183 0.173 0.183 0.274 0.23  0.205\n",
            " 0.211 0.224 0.216 0.181 0.231 0.198 0.207 0.216 0.182 0.232 0.199 0.256\n",
            " 0.17  0.178 0.164 0.248 0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.   ]\n",
            "(100,)\n",
            "<class 'int'>\n",
            "202\n",
            "/content/gdrive/My Drive/thesis/Data/S13/S13_TEX03.txt\n",
            "13\n",
            "[0.821 0.883 0.77  1.196 0.789 0.952 0.637 2.047 0.814 0.552 0.507 0.848\n",
            " 0.94  1.036 0.457 0.649 0.773 0.975 0.603 1.381 1.07  0.754 0.633 0.892\n",
            " 0.645 0.656 1.014 1.079 0.807 0.631 0.71  0.57  1.109 0.705 0.72  0.763\n",
            " 1.198 0.924 1.062 0.822 0.54  0.782]\n",
            "[0.821 0.883 0.77  1.196 0.789 0.952 0.637 2.047 0.814 0.552 0.507 0.848\n",
            " 0.94  1.036 0.457 0.649 0.773 0.975 0.603 1.381 1.07  0.754 0.633 0.892\n",
            " 0.645 0.656 1.014 1.079 0.807 0.631 0.71  0.57  1.109 0.705 0.72  0.763\n",
            " 1.198 0.924 1.062 0.822 0.54  0.782]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.821 0.883 0.77  1.196 0.789 0.952 0.637\n",
            " 2.047 0.814 0.552 0.507 0.848 0.94  1.036 0.457 0.649 0.773 0.975 0.603\n",
            " 1.381 1.07  0.754 0.633 0.892 0.645 0.656 1.014 1.079 0.807 0.631 0.71\n",
            " 0.57  1.109 0.705 0.72  0.763 1.198 0.924 1.062 0.822 0.54  0.782 0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.   ]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.184 0.202 0.237 0.184 0.249 0.253 0.187 0.218\n",
            " 0.244 0.177 0.199 0.215 0.209 0.273 0.207 0.24  0.243 0.247 0.186 0.181\n",
            " 0.198 0.223 0.208 0.206 0.17  0.202 0.19  0.331 0.207 0.174 0.248 0.179\n",
            " 0.223 0.194 0.153 0.223 0.224 0.239 0.24  0.215 0.219 0.323 0.24  0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.   ]\n",
            "(100,)\n",
            "<class 'int'>\n",
            "203\n",
            "/content/gdrive/My Drive/thesis/Data/S13/S13_TEX10.txt\n",
            "13\n",
            "[ 0.751  0.849  0.665  0.809  0.686  0.599  0.764  2.251  1.209  1.894\n",
            "  0.828  0.755  1.218  0.706  0.461  0.74   0.848  0.777  0.677  0.59\n",
            "  0.764  0.874  0.796  2.686  3.942  0.636  1.29  18.489  1.247  0.827\n",
            "  1.449  0.615  1.073 31.934  1.497  0.636  1.683  0.62   5.327  1.559\n",
            "  0.873  0.98   0.739  2.452  0.974  0.913  0.729]\n",
            "[0.751 0.849 0.665 0.809 0.686 0.599 0.764 2.251 1.209 1.894 0.828 0.755\n",
            " 1.218 0.706 0.461 0.74  0.848 0.777 0.677 0.59  0.764 0.874 0.796 2.686\n",
            " 0.    0.636 1.29  0.    1.247 0.827 1.449 0.615 1.073 0.    1.497 0.636\n",
            " 1.683 0.62  0.    1.559 0.873 0.98  0.739 2.452 0.974 0.913 0.729]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.751 0.849 0.665 0.809 0.686 0.599 0.764 2.251 1.209 1.894\n",
            " 0.828 0.755 1.218 0.706 0.461 0.74  0.848 0.777 0.677 0.59  0.764 0.874\n",
            " 0.796 2.686 0.    0.636 1.29  0.    1.247 0.827 1.449 0.615 1.073 0.\n",
            " 1.497 0.636 1.683 0.62  0.    1.559 0.873 0.98  0.739 2.452 0.974 0.913\n",
            " 0.729 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.   ]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.181 0.216 0.182 0.222 0.162 0.223 0.19  0.185 0.257 0.187\n",
            " 0.231 0.24  0.261 0.22  0.178 0.274 0.225 0.215 0.186 0.175 0.223 0.207\n",
            " 0.239 0.273 0.205 0.169 0.264 0.262 0.288 0.215 0.203 0.225 0.247 0.278\n",
            " 0.256 0.254 0.19  0.186 0.171 0.247 0.24  0.206 0.184 0.2   0.181 0.272\n",
            " 0.171 0.225 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.   ]\n",
            "(100,)\n",
            "<class 'int'>\n",
            "204\n",
            "/content/gdrive/My Drive/thesis/Data/S13/S13_TEX08.txt\n",
            "13\n",
            "[3.569 8.519 0.786 0.891 0.671 0.623 1.391 0.863 0.852 0.732 0.822 1.772\n",
            " 0.679 0.781 0.757 0.872 0.765 0.78  0.716 1.2   0.819 0.658 0.872 0.706\n",
            " 0.822 0.666 0.706 0.923 2.111 0.681 0.623 0.923 1.39  0.955 0.581 0.563\n",
            " 0.699 0.839 0.714 3.242 1.064 0.73  0.898 3.207 2.099 0.74  0.674 0.783\n",
            " 0.837 0.705 0.772 0.674 0.857 0.905 1.071 0.934 0.546 0.83  0.991 0.771\n",
            " 0.764 0.723 0.813 0.849 1.76  1.025 0.721 0.764 0.897 0.954 0.667 0.771\n",
            " 0.74  0.723 3.583 0.721 0.656 0.746 0.724 0.758 0.488 0.656 0.649 0.54\n",
            " 0.713]\n",
            "[0.    0.    0.786 0.891 0.671 0.623 1.391 0.863 0.852 0.732 0.822 1.772\n",
            " 0.679 0.781 0.757 0.872 0.765 0.78  0.716 1.2   0.819 0.658 0.872 0.706\n",
            " 0.822 0.666 0.706 0.923 2.111 0.681 0.623 0.923 1.39  0.955 0.581 0.563\n",
            " 0.699 0.839 0.714 0.    1.064 0.73  0.898 0.    2.099 0.74  0.674 0.783\n",
            " 0.837 0.705 0.772 0.674 0.857 0.905 1.071 0.934 0.546 0.83  0.991 0.771\n",
            " 0.764 0.723 0.813 0.849 1.76  1.025 0.721 0.764 0.897 0.954 0.667 0.771\n",
            " 0.74  0.723 0.    0.721 0.656 0.746 0.724 0.758 0.488 0.656 0.649 0.54\n",
            " 0.713]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.786 0.891 0.671\n",
            " 0.623 1.391 0.863 0.852 0.732 0.822 1.772 0.679 0.781 0.757 0.872 0.765\n",
            " 0.78  0.716 1.2   0.819 0.658 0.872 0.706 0.822 0.666 0.706 0.923 2.111\n",
            " 0.681 0.623 0.923 1.39  0.955 0.581 0.563 0.699 0.839 0.714 0.    1.064\n",
            " 0.73  0.898 0.    2.099 0.74  0.674 0.783 0.837 0.705 0.772 0.674 0.857\n",
            " 0.905 1.071 0.934 0.546 0.83  0.991 0.771 0.764 0.723 0.813 0.849 1.76\n",
            " 1.025 0.721 0.764 0.897 0.954 0.667 0.771 0.74  0.723 0.    0.721 0.656\n",
            " 0.746 0.724 0.758 0.488 0.656 0.649 0.54  0.713 0.    0.    0.    0.\n",
            " 0.    0.    0.    0.   ]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.452 0.323 0.245 0.198 0.229\n",
            " 0.173 0.227 0.311 0.212 0.211 0.198 0.29  0.221 0.199 0.216 0.264 0.19\n",
            " 0.208 0.225 0.231 0.195 0.274 0.249 0.267 0.277 0.291 0.256 0.282 0.339\n",
            " 0.214 0.226 0.29  0.372 0.22  0.246 0.223 0.299 0.24  0.217 0.39  0.28\n",
            " 0.245 0.238 0.278 0.213 0.248 0.223 0.231 0.203 0.217 0.173 0.202 0.19\n",
            " 0.247 0.298 0.232 0.254 0.29  0.257 0.23  0.26  0.248 0.258 0.265 0.388\n",
            " 0.273 0.224 0.234 0.256 0.282 0.266 0.263 0.27  0.244 0.314 0.212 0.181\n",
            " 0.223 0.233 0.264 0.196 0.215 0.208 0.216 0.223 0.199 0.    0.    0.\n",
            " 0.    0.    0.    0.    0.   ]\n",
            "(100,)\n",
            "<class 'int'>\n",
            "205\n",
            "/content/gdrive/My Drive/thesis/Data/S13/S13_TEX02.txt\n",
            "13\n",
            "[0.614 0.898 1.038 0.855 0.906 0.797 0.839 0.872 0.674 0.864 1.98  0.997\n",
            " 0.678 0.623 0.864 0.914 2.414 0.728 0.959 0.769 0.848 0.741 0.491 0.593\n",
            " 3.03  0.931 0.669 0.772 0.707 0.715 0.946 1.782 1.474 0.533 0.921 1.081\n",
            " 0.825 1.029 0.836 0.907 0.915 0.669 0.731 0.831 0.733 0.697 0.715 1.712]\n",
            "[0.614 0.898 1.038 0.855 0.906 0.797 0.839 0.872 0.674 0.864 1.98  0.997\n",
            " 0.678 0.623 0.864 0.914 2.414 0.728 0.959 0.769 0.848 0.741 0.491 0.593\n",
            " 0.    0.931 0.669 0.772 0.707 0.715 0.946 1.782 1.474 0.533 0.921 1.081\n",
            " 0.825 1.029 0.836 0.907 0.915 0.669 0.731 0.831 0.733 0.697 0.715 1.712]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.614 0.898 1.038 0.855 0.906 0.797 0.839 0.872 0.674 0.864\n",
            " 1.98  0.997 0.678 0.623 0.864 0.914 2.414 0.728 0.959 0.769 0.848 0.741\n",
            " 0.491 0.593 0.    0.931 0.669 0.772 0.707 0.715 0.946 1.782 1.474 0.533\n",
            " 0.921 1.081 0.825 1.029 0.836 0.907 0.915 0.669 0.731 0.831 0.733 0.697\n",
            " 0.715 1.712 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.   ]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.265 0.256 0.297 0.24  0.257 0.261 0.265 0.3   0.258 0.275 0.256\n",
            " 0.245 0.22  0.24  0.257 0.233 0.306 0.253 0.256 0.228 0.258 0.249 0.242\n",
            " 0.231 0.252 0.287 0.22  0.257 0.241 0.216 0.303 0.299 0.21  0.182 0.247\n",
            " 0.272 0.305 0.269 0.187 0.257 0.305 0.237 0.207 0.275 0.249 0.243 0.265\n",
            " 0.223 0.215 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.   ]\n",
            "(100,)\n",
            "<class 'int'>\n",
            "206\n",
            "/content/gdrive/My Drive/thesis/Data/S13/S13_TEX04.txt\n",
            "13\n",
            "[ 1.087  0.794  1.308  0.703  0.624  0.751  0.679  0.781  2.592  0.73\n",
            "  1.705  1.473  0.861  0.673  2.212  1.216  0.759  0.691  0.68   0.798\n",
            "  1.448  0.737  0.712  0.741  0.516  0.592  0.556  0.472  0.599  0.74\n",
            "  0.622  2.937  0.678  0.657  0.632  0.734  0.619  0.624  0.805  0.747\n",
            "  0.806  2.147  0.545  0.648  0.967  0.786  0.707  0.705  0.624  1.727\n",
            "  2.239  0.66   0.698  0.715  0.9    0.981  0.704  0.742  0.544  0.765\n",
            "  0.641  0.639  0.805  1.755  1.86   0.74   0.764  0.539  0.748  1.076\n",
            "  1.211  0.888  0.523  0.672 20.437  0.79   0.885  0.979  1.637  0.569\n",
            "  0.889  0.691  1.132  0.586  0.683  0.811  0.558  0.699  0.731  0.647\n",
            "  0.831  0.831]\n",
            "[1.087 0.794 1.308 0.703 0.624 0.751 0.679 0.781 2.592 0.73  1.705 1.473\n",
            " 0.861 0.673 2.212 1.216 0.759 0.691 0.68  0.798 1.448 0.737 0.712 0.741\n",
            " 0.516 0.592 0.556 0.472 0.599 0.74  0.622 2.937 0.678 0.657 0.632 0.734\n",
            " 0.619 0.624 0.805 0.747 0.806 2.147 0.545 0.648 0.967 0.786 0.707 0.705\n",
            " 0.624 1.727 2.239 0.66  0.698 0.715 0.9   0.981 0.704 0.742 0.544 0.765\n",
            " 0.641 0.639 0.805 1.755 1.86  0.74  0.764 0.539 0.748 1.076 1.211 0.888\n",
            " 0.523 0.672 0.    0.79  0.885 0.979 1.637 0.569 0.889 0.691 1.132 0.586\n",
            " 0.683 0.811 0.558 0.699 0.731 0.647 0.831 0.831]\n",
            "[0.    0.    0.    0.    1.087 0.794 1.308 0.703 0.624 0.751 0.679 0.781\n",
            " 2.592 0.73  1.705 1.473 0.861 0.673 2.212 1.216 0.759 0.691 0.68  0.798\n",
            " 1.448 0.737 0.712 0.741 0.516 0.592 0.556 0.472 0.599 0.74  0.622 2.937\n",
            " 0.678 0.657 0.632 0.734 0.619 0.624 0.805 0.747 0.806 2.147 0.545 0.648\n",
            " 0.967 0.786 0.707 0.705 0.624 1.727 2.239 0.66  0.698 0.715 0.9   0.981\n",
            " 0.704 0.742 0.544 0.765 0.641 0.639 0.805 1.755 1.86  0.74  0.764 0.539\n",
            " 0.748 1.076 1.211 0.888 0.523 0.672 0.    0.79  0.885 0.979 1.637 0.569\n",
            " 0.889 0.691 1.132 0.586 0.683 0.811 0.558 0.699 0.731 0.647 0.831 0.831\n",
            " 0.    0.    0.    0.   ]\n",
            "[0.    0.    0.    0.227 0.22  0.34  0.179 0.228 0.232 0.237 0.259 0.255\n",
            " 0.198 0.175 0.256 0.237 0.231 0.207 0.271 0.226 0.25  0.222 0.224 0.298\n",
            " 0.212 0.199 0.226 0.191 0.198 0.188 0.189 0.207 0.275 0.19  0.199 0.187\n",
            " 0.165 0.165 0.197 0.178 0.19  0.223 0.207 0.182 0.233 0.17  0.19  0.19\n",
            " 0.17  0.165 0.206 0.198 0.209 0.215 0.194 0.191 0.211 0.256 0.221 0.229\n",
            " 0.205 0.169 0.191 0.218 0.173 0.227 0.265 0.222 0.256 0.247 0.21  0.199\n",
            " 0.273 0.302 0.164 0.23  0.255 0.148 0.224 0.21  0.178 0.286 0.127 0.26\n",
            " 0.194 0.271 0.178 0.233 0.238 0.192 0.223 0.222 0.164 0.276 0.232 0.249\n",
            " 0.    0.    0.    0.   ]\n",
            "(100,)\n",
            "<class 'int'>\n",
            "207\n",
            "/content/gdrive/My Drive/thesis/Data/S13/S13_TEX07.txt\n",
            "13\n",
            "[0.755 0.689 0.643 0.727 0.873 1.122 0.423 0.589 0.567 0.498 0.796 0.802\n",
            " 0.819 0.756 0.657 1.547 0.662 0.806 0.448 0.763 0.45  0.449 0.598 0.815\n",
            " 2.697 0.896 0.421 0.656 1.74  0.751 0.726 0.931 0.856 0.753 0.707 1.399\n",
            " 0.678 0.73  1.257 0.672 0.623 0.706 0.739 0.574 1.065 3.811 0.563 0.559\n",
            " 0.756 0.883 0.752 0.757 0.507 0.607 0.589 0.706 1.085 0.87  2.095 0.445\n",
            " 0.607 0.548 0.822 0.632 0.746 0.574 0.441 2.845 0.822 0.637 0.643 0.812\n",
            " 0.714 0.722 0.84  0.955 0.832 0.888 0.656 1.963 0.722 0.771 2.214 0.867\n",
            " 0.783 0.616 0.496 0.673 0.968 0.809 0.433 0.789 0.934 0.678 0.832 0.837\n",
            " 0.466 0.698 0.457 0.39  0.599]\n",
            "[0.755 0.689 0.643 0.727 0.873 1.122 0.423 0.589 0.567 0.498 0.796 0.802\n",
            " 0.819 0.756 0.657 1.547 0.662 0.806 0.448 0.763 0.45  0.449 0.598 0.815\n",
            " 2.697 0.896 0.421 0.656 1.74  0.751 0.726 0.931 0.856 0.753 0.707 1.399\n",
            " 0.678 0.73  1.257 0.672 0.623 0.706 0.739 0.574 1.065 0.    0.563 0.559\n",
            " 0.756 0.883 0.752 0.757 0.507 0.607 0.589 0.706 1.085 0.87  2.095 0.445\n",
            " 0.607 0.548 0.822 0.632 0.746 0.574 0.441 2.845 0.822 0.637 0.643 0.812\n",
            " 0.714 0.722 0.84  0.955 0.832 0.888 0.656 1.963 0.722 0.771 2.214 0.867\n",
            " 0.783 0.616 0.496 0.673 0.968 0.809 0.433 0.789 0.934 0.678 0.832 0.837\n",
            " 0.466 0.698 0.457 0.39  0.599]\n",
            "[0.755 0.689 0.643 0.727 0.873 1.122 0.423 0.589 0.567 0.498 0.796 0.802\n",
            " 0.819 0.756 0.657 1.547 0.662 0.806 0.448 0.763 0.45  0.449 0.598 0.815\n",
            " 2.697 0.896 0.421 0.656 1.74  0.751 0.726 0.931 0.856 0.753 0.707 1.399\n",
            " 0.678 0.73  1.257 0.672 0.623 0.706 0.739 0.574 1.065 0.    0.563 0.559\n",
            " 0.756 0.883 0.752 0.757 0.507 0.607 0.589 0.706 1.085 0.87  2.095 0.445\n",
            " 0.607 0.548 0.822 0.632 0.746 0.574 0.441 2.845 0.822 0.637 0.643 0.812\n",
            " 0.714 0.722 0.84  0.955 0.832 0.888 0.656 1.963 0.722 0.771 2.214 0.867\n",
            " 0.783 0.616 0.496 0.673 0.968 0.809 0.433 0.789 0.934 0.678 0.832 0.837\n",
            " 0.466 0.698 0.457 0.39 ]\n",
            "[0.206 0.231 0.198 0.203 0.249 0.307 0.189 0.232 0.184 0.157 0.232 0.258\n",
            " 0.22  0.277 0.199 0.249 0.212 0.207 0.191 0.216 0.184 0.174 0.181 0.199\n",
            " 0.281 0.211 0.179 0.198 0.198 0.194 0.234 0.238 0.271 0.246 0.257 0.274\n",
            " 0.22  0.198 0.241 0.217 0.234 0.222 0.29  0.235 0.222 0.295 0.214 0.249\n",
            " 0.214 0.214 0.17  0.199 0.182 0.222 0.197 0.199 0.252 0.31  0.188 0.17\n",
            " 0.206 0.173 0.232 0.174 0.235 0.192 0.191 0.183 0.227 0.187 0.198 0.237\n",
            " 0.173 0.203 0.228 0.232 0.19  0.241 0.193 0.241 0.232 0.259 0.234 0.261\n",
            " 0.207 0.181 0.18  0.249 0.273 0.195 0.186 0.231 0.249 0.22  0.232 0.239\n",
            " 0.191 0.224 0.207 0.215]\n",
            "(100,)\n",
            "<class 'int'>\n",
            "208\n",
            "/content/gdrive/My Drive/thesis/Data/S13/S13_TEX06.txt\n",
            "13\n",
            "[1.82  0.589 0.639 0.715 0.731 0.506 2.472 0.604 0.863 0.831 0.623 0.999\n",
            " 1.061 0.764 0.715 0.706 0.639 0.666 0.739 0.78  0.868 4.324 0.558 0.821\n",
            " 0.79  0.539 0.917 0.939 1.146 0.595 0.838 0.641 0.68  0.723 0.624 0.7\n",
            " 0.795 0.913 0.365 0.69  0.557 1.221 0.624 0.573 0.965 1.423 0.668 0.674\n",
            " 0.589 0.748 0.515 0.806 0.699 0.717 0.76  0.823 0.789 0.59  0.473 0.856\n",
            " 0.673 0.565]\n",
            "[1.82  0.589 0.639 0.715 0.731 0.506 2.472 0.604 0.863 0.831 0.623 0.999\n",
            " 1.061 0.764 0.715 0.706 0.639 0.666 0.739 0.78  0.868 0.    0.558 0.821\n",
            " 0.79  0.539 0.917 0.939 1.146 0.595 0.838 0.641 0.68  0.723 0.624 0.7\n",
            " 0.795 0.913 0.365 0.69  0.557 1.221 0.624 0.573 0.965 1.423 0.668 0.674\n",
            " 0.589 0.748 0.515 0.806 0.699 0.717 0.76  0.823 0.789 0.59  0.473 0.856\n",
            " 0.673 0.565]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    1.82  0.589 0.639 0.715 0.731\n",
            " 0.506 2.472 0.604 0.863 0.831 0.623 0.999 1.061 0.764 0.715 0.706 0.639\n",
            " 0.666 0.739 0.78  0.868 0.    0.558 0.821 0.79  0.539 0.917 0.939 1.146\n",
            " 0.595 0.838 0.641 0.68  0.723 0.624 0.7   0.795 0.913 0.365 0.69  0.557\n",
            " 1.221 0.624 0.573 0.965 1.423 0.668 0.674 0.589 0.748 0.515 0.806 0.699\n",
            " 0.717 0.76  0.823 0.789 0.59  0.473 0.856 0.673 0.565 0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.   ]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.206 0.214 0.174 0.216 0.223 0.19\n",
            " 0.232 0.17  0.197 0.207 0.19  0.285 0.312 0.207 0.265 0.235 0.256 0.199\n",
            " 0.229 0.206 0.265 0.211 0.116 0.199 0.166 0.21  0.207 0.216 0.245 0.197\n",
            " 0.222 0.2   0.256 0.199 0.174 0.232 0.253 0.273 0.132 0.182 0.165 0.198\n",
            " 0.173 0.139 0.207 0.297 0.161 0.182 0.149 0.191 0.166 0.208 0.223 0.231\n",
            " 0.195 0.224 0.224 0.215 0.198 0.232 0.174 0.198 0.157 0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.   ]\n",
            "(100,)\n",
            "<class 'int'>\n",
            "209\n",
            "/content/gdrive/My Drive/thesis/Data/S07/S07_TEX05.txt\n",
            "7\n",
            "[0.424 0.506 0.82  0.703 0.689 0.332 0.4   0.349 0.798 1.756 0.629 0.532\n",
            " 0.415 0.519 0.38  0.666 0.923 0.505 1.032 0.537 0.483 1.066 0.599 0.636\n",
            " 0.534 0.374 1.374 0.445 0.415 0.417 0.641 0.53  0.4   0.7   0.355 0.291\n",
            " 0.515 0.407 0.425 0.331 0.827 1.496 0.536 0.59  0.483 0.35  0.668 0.889\n",
            " 0.278 0.615 0.576 0.383 0.604 0.707 0.573 0.615 0.565 0.691 1.089 0.507]\n",
            "[0.424 0.506 0.82  0.703 0.689 0.332 0.4   0.349 0.798 1.756 0.629 0.532\n",
            " 0.415 0.519 0.38  0.666 0.923 0.505 1.032 0.537 0.483 1.066 0.599 0.636\n",
            " 0.534 0.374 1.374 0.445 0.415 0.417 0.641 0.53  0.4   0.7   0.355 0.291\n",
            " 0.515 0.407 0.425 0.331 0.827 1.496 0.536 0.59  0.483 0.35  0.668 0.889\n",
            " 0.278 0.615 0.576 0.383 0.604 0.707 0.573 0.615 0.565 0.691 1.089 0.507]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.424 0.506 0.82  0.703\n",
            " 0.689 0.332 0.4   0.349 0.798 1.756 0.629 0.532 0.415 0.519 0.38  0.666\n",
            " 0.923 0.505 1.032 0.537 0.483 1.066 0.599 0.636 0.534 0.374 1.374 0.445\n",
            " 0.415 0.417 0.641 0.53  0.4   0.7   0.355 0.291 0.515 0.407 0.425 0.331\n",
            " 0.827 1.496 0.536 0.59  0.483 0.35  0.668 0.889 0.278 0.615 0.576 0.383\n",
            " 0.604 0.707 0.573 0.615 0.565 0.691 1.089 0.507 0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.   ]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.116 0.099 0.1   0.103 0.098\n",
            " 0.133 0.108 0.073 0.098 0.115 0.121 0.064 0.099 0.108 0.095 0.098 0.106\n",
            " 0.096 0.081 0.105 0.09  0.074 0.105 0.088 0.108 0.097 0.089 0.112 0.107\n",
            " 0.091 0.098 0.09  0.083 0.088 0.104 0.098 0.09  0.105 0.098 0.088 0.107\n",
            " 0.07  0.111 0.091 0.091 0.1   0.081 0.103 0.095 0.09  0.083 0.089 0.088\n",
            " 0.09  0.09  0.099 0.082 0.099 0.09  0.09  0.073 0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.   ]\n",
            "(100,)\n",
            "<class 'int'>\n",
            "210\n",
            "/content/gdrive/My Drive/thesis/Data/S07/S07_TEX07.txt\n",
            "7\n",
            "[0.412 0.432 0.606 0.614 0.607 0.893 0.498 0.474 0.413 0.692 0.596 0.273\n",
            " 0.474 0.526 0.455 1.598 1.045 0.199 0.958 0.711 0.359 0.607 0.39  0.538\n",
            " 0.692 0.373 0.55  0.524 0.399 0.522 0.365 1.183 1.713 0.288 0.761 0.262\n",
            " 0.866 0.49  0.718 0.669 0.416 0.774 0.698 0.282 0.473 0.349 0.775 2.733\n",
            " 0.38  0.475 0.598 0.44  0.69  0.4   0.59  0.564 0.759 0.614 0.398 0.291\n",
            " 0.592 0.338]\n",
            "[0.412 0.432 0.606 0.614 0.607 0.893 0.498 0.474 0.413 0.692 0.596 0.273\n",
            " 0.474 0.526 0.455 1.598 1.045 0.199 0.958 0.711 0.359 0.607 0.39  0.538\n",
            " 0.692 0.373 0.55  0.524 0.399 0.522 0.365 1.183 1.713 0.288 0.761 0.262\n",
            " 0.866 0.49  0.718 0.669 0.416 0.774 0.698 0.282 0.473 0.349 0.775 2.733\n",
            " 0.38  0.475 0.598 0.44  0.69  0.4   0.59  0.564 0.759 0.614 0.398 0.291\n",
            " 0.592 0.338]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.412 0.432 0.606 0.614 0.607\n",
            " 0.893 0.498 0.474 0.413 0.692 0.596 0.273 0.474 0.526 0.455 1.598 1.045\n",
            " 0.199 0.958 0.711 0.359 0.607 0.39  0.538 0.692 0.373 0.55  0.524 0.399\n",
            " 0.522 0.365 1.183 1.713 0.288 0.761 0.262 0.866 0.49  0.718 0.669 0.416\n",
            " 0.774 0.698 0.282 0.473 0.349 0.775 2.733 0.38  0.475 0.598 0.44  0.69\n",
            " 0.4   0.59  0.564 0.759 0.614 0.398 0.291 0.592 0.338 0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.   ]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.086 0.09  0.098 0.116 0.09  0.107\n",
            " 0.095 0.096 0.07  0.098 0.121 0.099 0.092 0.107 0.088 0.131 0.062 0.097\n",
            " 0.098 0.113 0.107 0.107 0.09  0.081 0.109 0.09  0.099 0.09  0.107 0.089\n",
            " 0.116 0.1   0.106 0.122 0.108 0.086 0.098 0.106 0.114 0.086 0.141 0.098\n",
            " 0.106 0.098 0.115 0.099 0.107 0.105 0.089 0.091 0.091 0.099 0.092 0.108\n",
            " 0.081 0.081 0.107 0.071 0.115 0.081 0.098 0.096 0.1   0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.   ]\n",
            "(100,)\n",
            "<class 'int'>\n",
            "211\n",
            "/content/gdrive/My Drive/thesis/Data/S07/S07_TEX03.txt\n",
            "7\n",
            "[0.613 1.526 0.311 0.458 1.058 0.288 0.456 1.25  0.421 0.555 1.209 0.719\n",
            " 0.632 0.342 0.457 0.401 0.666 0.914 0.782 0.711 0.391 0.507 0.615 0.415\n",
            " 0.49  1.523 0.639 0.663 0.592 0.399 0.389 0.625 0.547 0.359 0.5   0.454\n",
            " 0.391 0.416 0.648 0.632 0.323 0.45 ]\n",
            "[0.613 1.526 0.311 0.458 1.058 0.288 0.456 1.25  0.421 0.555 1.209 0.719\n",
            " 0.632 0.342 0.457 0.401 0.666 0.914 0.782 0.711 0.391 0.507 0.615 0.415\n",
            " 0.49  1.523 0.639 0.663 0.592 0.399 0.389 0.625 0.547 0.359 0.5   0.454\n",
            " 0.391 0.416 0.648 0.632 0.323 0.45 ]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.613 1.526 0.311 0.458 1.058 0.288 0.456\n",
            " 1.25  0.421 0.555 1.209 0.719 0.632 0.342 0.457 0.401 0.666 0.914 0.782\n",
            " 0.711 0.391 0.507 0.615 0.415 0.49  1.523 0.639 0.663 0.592 0.399 0.389\n",
            " 0.625 0.547 0.359 0.5   0.454 0.391 0.416 0.648 0.632 0.323 0.45  0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.   ]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.097 0.066 0.085 0.083 0.09  0.071 0.107 0.124\n",
            " 0.095 0.098 0.084 0.114 0.084 0.142 0.091 0.099 0.105 0.096 0.095 0.129\n",
            " 0.09  0.09  0.099 0.106 0.1   0.074 0.107 0.132 0.1   0.091 0.089 0.1\n",
            " 0.107 0.091 0.097 0.088 0.1   0.09  0.073 0.091 0.073 0.092 0.081 0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.   ]\n",
            "(100,)\n",
            "<class 'int'>\n",
            "212\n",
            "/content/gdrive/My Drive/thesis/Data/S07/S07_TEX04.txt\n",
            "7\n",
            "[0.805 0.686 0.35  0.224 0.317 0.592 0.762 0.734 0.387 0.217 0.699 0.572\n",
            " 0.207 0.402 1.706 0.746 0.329 0.408 0.785 0.454 0.648 0.407 0.459 0.574\n",
            " 0.448 1.415 0.577 0.3   0.516 0.455 0.533 0.734 0.439 0.372 0.574 0.332\n",
            " 0.483 0.408 0.521 2.162 1.689 0.933 1.338 0.452 0.576 0.248 0.726 3.441\n",
            " 0.415 0.612 1.337 0.876 0.587 0.399 0.968 0.479 0.867 0.563 0.407 1.432\n",
            " 0.728 0.39  0.316 0.718 0.379 0.516 0.365 0.568 0.49  0.781 0.29  0.348\n",
            " 0.692 0.538 0.514]\n",
            "[0.805 0.686 0.35  0.224 0.317 0.592 0.762 0.734 0.387 0.217 0.699 0.572\n",
            " 0.207 0.402 1.706 0.746 0.329 0.408 0.785 0.454 0.648 0.407 0.459 0.574\n",
            " 0.448 1.415 0.577 0.3   0.516 0.455 0.533 0.734 0.439 0.372 0.574 0.332\n",
            " 0.483 0.408 0.521 2.162 1.689 0.933 1.338 0.452 0.576 0.248 0.726 0.\n",
            " 0.415 0.612 1.337 0.876 0.587 0.399 0.968 0.479 0.867 0.563 0.407 1.432\n",
            " 0.728 0.39  0.316 0.718 0.379 0.516 0.365 0.568 0.49  0.781 0.29  0.348\n",
            " 0.692 0.538 0.514]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.805 0.686 0.35  0.224 0.317 0.592 0.762 0.734 0.387 0.217 0.699 0.572\n",
            " 0.207 0.402 1.706 0.746 0.329 0.408 0.785 0.454 0.648 0.407 0.459 0.574\n",
            " 0.448 1.415 0.577 0.3   0.516 0.455 0.533 0.734 0.439 0.372 0.574 0.332\n",
            " 0.483 0.408 0.521 2.162 1.689 0.933 1.338 0.452 0.576 0.248 0.726 0.\n",
            " 0.415 0.612 1.337 0.876 0.587 0.399 0.968 0.479 0.867 0.563 0.407 1.432\n",
            " 0.728 0.39  0.316 0.718 0.379 0.516 0.365 0.568 0.49  0.781 0.29  0.348\n",
            " 0.692 0.538 0.514 0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.   ]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.128 0.105 0.092 0.081 0.075 0.073 0.105 0.089 0.079 0.083 0.082 0.089\n",
            " 0.066 0.058 0.097 0.086 0.105 0.091 0.098 0.079 0.107 0.09  0.09  0.123\n",
            " 0.081 0.08  0.07  0.099 0.091 0.099 0.075 0.091 0.122 0.09  0.099 0.108\n",
            " 0.099 0.091 0.09  0.1   0.1   0.121 0.103 0.095 0.083 0.081 0.073 0.095\n",
            " 0.096 0.121 0.081 0.116 0.088 0.114 0.082 0.104 0.099 0.096 0.089 0.114\n",
            " 0.111 0.091 0.083 0.09  0.096 0.107 0.065 0.082 0.096 0.071 0.089 0.072\n",
            " 0.09  0.095 0.14  0.083 0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.   ]\n",
            "(100,)\n",
            "<class 'int'>\n",
            "213\n",
            "/content/gdrive/My Drive/thesis/Data/S07/S07_TEX02.txt\n",
            "7\n",
            "[0.553 0.448 0.673 0.607 0.682 1.1   0.511 1.226 0.512 0.715 0.383 0.7\n",
            " 0.562 0.427 0.52  0.366 0.561 0.497 0.382 0.713 0.417 0.455 0.582 0.607\n",
            " 0.951 1.473 0.685 0.424 0.541 0.481 0.483 0.572 0.291 1.655 0.802 0.495\n",
            " 0.448 0.652 0.43  0.574 0.173 0.682 0.648 0.64  0.434 0.49  1.093 2.943\n",
            " 0.78  0.321 0.348 0.491 0.341 0.825 0.546 0.501 0.408 2.216 0.55  0.606\n",
            " 0.25  0.657 0.499 1.772 0.305 0.685 0.454 0.499 0.383 0.692 0.305 0.76\n",
            " 0.547 0.265 0.457 0.976 0.471]\n",
            "[0.553 0.448 0.673 0.607 0.682 1.1   0.511 1.226 0.512 0.715 0.383 0.7\n",
            " 0.562 0.427 0.52  0.366 0.561 0.497 0.382 0.713 0.417 0.455 0.582 0.607\n",
            " 0.951 1.473 0.685 0.424 0.541 0.481 0.483 0.572 0.291 1.655 0.802 0.495\n",
            " 0.448 0.652 0.43  0.574 0.173 0.682 0.648 0.64  0.434 0.49  1.093 2.943\n",
            " 0.78  0.321 0.348 0.491 0.341 0.825 0.546 0.501 0.408 2.216 0.55  0.606\n",
            " 0.25  0.657 0.499 1.772 0.305 0.685 0.454 0.499 0.383 0.692 0.305 0.76\n",
            " 0.547 0.265 0.457 0.976 0.471]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.553\n",
            " 0.448 0.673 0.607 0.682 1.1   0.511 1.226 0.512 0.715 0.383 0.7   0.562\n",
            " 0.427 0.52  0.366 0.561 0.497 0.382 0.713 0.417 0.455 0.582 0.607 0.951\n",
            " 1.473 0.685 0.424 0.541 0.481 0.483 0.572 0.291 1.655 0.802 0.495 0.448\n",
            " 0.652 0.43  0.574 0.173 0.682 0.648 0.64  0.434 0.49  1.093 2.943 0.78\n",
            " 0.321 0.348 0.491 0.341 0.825 0.546 0.501 0.408 2.216 0.55  0.606 0.25\n",
            " 0.657 0.499 1.772 0.305 0.685 0.454 0.499 0.383 0.692 0.305 0.76  0.547\n",
            " 0.265 0.457 0.976 0.471 0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.   ]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.095\n",
            " 0.099 0.091 0.066 0.098 0.098 0.095 0.084 0.095 0.108 0.082 0.081 0.071\n",
            " 0.082 0.079 0.083 0.074 0.078 0.063 0.08  0.083 0.097 0.074 0.09  0.099\n",
            " 0.121 0.077 0.056 0.09  0.088 0.116 0.098 0.09  0.083 0.09  0.062 0.09\n",
            " 0.091 0.096 0.098 0.08  0.083 0.117 0.09  0.108 0.073 0.091 0.111 0.093\n",
            " 0.087 0.083 0.083 0.1   0.092 0.087 0.1   0.063 0.096 0.107 0.099 0.084\n",
            " 0.085 0.083 0.107 0.089 0.1   0.079 0.1   0.083 0.116 0.112 0.082 0.097\n",
            " 0.08  0.09  0.081 0.078 0.081 0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.   ]\n",
            "(100,)\n",
            "<class 'int'>\n",
            "214\n",
            "/content/gdrive/My Drive/thesis/Data/S07/S07_TEX06.txt\n",
            "7\n",
            "[0.363 0.698 0.868 0.722 0.537 0.44  0.784 1.554 0.148 0.215 0.952 0.435\n",
            " 0.458 2.266 0.362 0.349 1.075 0.814 0.62  0.533 0.781 2.978 1.455 0.51\n",
            " 0.878 0.587 0.348 0.4   0.933 3.485 1.329 0.444 1.646 0.325 0.682 0.581\n",
            " 0.284 0.638 4.986 0.486 0.549 0.456 1.701 0.618 0.777 0.93  0.479 0.522\n",
            " 0.709 0.382 0.483 0.615 0.381 0.349 2.576 0.466 0.482]\n",
            "[0.363 0.698 0.868 0.722 0.537 0.44  0.784 1.554 0.148 0.215 0.952 0.435\n",
            " 0.458 2.266 0.362 0.349 1.075 0.814 0.62  0.533 0.781 2.978 1.455 0.51\n",
            " 0.878 0.587 0.348 0.4   0.933 0.    1.329 0.444 1.646 0.325 0.682 0.581\n",
            " 0.284 0.638 0.    0.486 0.549 0.456 1.701 0.618 0.777 0.93  0.479 0.522\n",
            " 0.709 0.382 0.483 0.615 0.381 0.349 2.576 0.466 0.482]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.363 0.698 0.868\n",
            " 0.722 0.537 0.44  0.784 1.554 0.148 0.215 0.952 0.435 0.458 2.266 0.362\n",
            " 0.349 1.075 0.814 0.62  0.533 0.781 2.978 1.455 0.51  0.878 0.587 0.348\n",
            " 0.4   0.933 0.    1.329 0.444 1.646 0.325 0.682 0.581 0.284 0.638 0.\n",
            " 0.486 0.549 0.456 1.701 0.618 0.777 0.93  0.479 0.522 0.709 0.382 0.483\n",
            " 0.615 0.381 0.349 2.576 0.466 0.482 0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.   ]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.078 0.082 0.082\n",
            " 0.102 0.086 0.082 0.099 0.088 0.087 0.114 0.075 0.069 0.091 0.075 0.119\n",
            " 0.082 0.073 0.086 0.113 0.074 0.115 0.099 0.095 0.046 0.091 0.096 0.099\n",
            " 0.1   0.106 0.113 0.101 0.07  0.083 0.107 0.09  0.065 0.099 0.081 0.084\n",
            " 0.102 0.082 0.073 0.107 0.085 0.099 0.113 0.095 0.106 0.133 0.106 0.072\n",
            " 0.089 0.107 0.09  0.091 0.057 0.082 0.073 0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.   ]\n",
            "(100,)\n",
            "<class 'int'>\n",
            "215\n",
            "/content/gdrive/My Drive/thesis/Data/S07/S07_TEX08.txt\n",
            "7\n",
            "[0.644 0.5   0.651 0.538 0.991 0.647 0.264 0.482 0.291 0.241 0.533 1.181\n",
            " 0.654 0.524 0.541 0.6   0.539 0.851 0.696 0.29  0.54  0.365 0.574 0.548\n",
            " 0.515 0.382 0.465 1.28  0.6   0.574 0.582 0.367 0.565 0.616 0.663 0.664\n",
            " 0.616 0.591 0.432 0.54  0.216 1.58  0.623 0.615 0.174 0.508 0.382 0.498\n",
            " 0.349 0.5   0.34  0.547 0.499 0.633 0.299 0.557 0.49  0.776 0.661 0.658\n",
            " 0.574 0.431 1.023 1.49  0.838 0.581 0.59  0.414 0.334]\n",
            "[0.644 0.5   0.651 0.538 0.991 0.647 0.264 0.482 0.291 0.241 0.533 1.181\n",
            " 0.654 0.524 0.541 0.6   0.539 0.851 0.696 0.29  0.54  0.365 0.574 0.548\n",
            " 0.515 0.382 0.465 1.28  0.6   0.574 0.582 0.367 0.565 0.616 0.663 0.664\n",
            " 0.616 0.591 0.432 0.54  0.216 1.58  0.623 0.615 0.174 0.508 0.382 0.498\n",
            " 0.349 0.5   0.34  0.547 0.499 0.633 0.299 0.557 0.49  0.776 0.661 0.658\n",
            " 0.574 0.431 1.023 1.49  0.838 0.581 0.59  0.414 0.334]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.644 0.5   0.651 0.538 0.991 0.647 0.264 0.482 0.291\n",
            " 0.241 0.533 1.181 0.654 0.524 0.541 0.6   0.539 0.851 0.696 0.29  0.54\n",
            " 0.365 0.574 0.548 0.515 0.382 0.465 1.28  0.6   0.574 0.582 0.367 0.565\n",
            " 0.616 0.663 0.664 0.616 0.591 0.432 0.54  0.216 1.58  0.623 0.615 0.174\n",
            " 0.508 0.382 0.498 0.349 0.5   0.34  0.547 0.499 0.633 0.299 0.557 0.49\n",
            " 0.776 0.661 0.658 0.574 0.431 1.023 1.49  0.838 0.581 0.59  0.414 0.334\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.   ]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.096 0.092 0.098 0.096 0.073 0.087 0.064 0.091 0.09\n",
            " 0.091 0.107 0.082 0.079 0.083 0.083 0.064 0.08  0.056 0.119 0.072 0.09\n",
            " 0.09  0.099 0.097 0.116 0.108 0.083 0.091 0.092 0.09  0.09  0.083 0.09\n",
            " 0.106 0.088 0.1   0.075 0.083 0.081 0.098 0.073 0.057 0.114 0.081 0.073\n",
            " 0.083 0.081 0.081 0.09  0.081 0.097 0.098 0.084 0.099 0.09  0.098 0.081\n",
            " 0.073 0.105 0.057 0.09  0.056 0.108 0.09  0.104 0.114 0.08  0.09  0.1\n",
            " 0.089 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.   ]\n",
            "(100,)\n",
            "<class 'int'>\n",
            "216\n",
            "/content/gdrive/My Drive/thesis/Data/S09/S09_TEX10.txt\n",
            "9\n",
            "[8.432 4.188 0.804 3.112 0.503 0.333 0.443 0.793 0.401 0.681 1.07  0.468\n",
            " 0.558 0.24  1.08  0.598 1.741 0.554 0.623 0.573 0.374 2.406 0.453 0.539\n",
            " 0.242 1.44  0.489 0.766 1.979 0.529 0.275 0.916 0.531 0.711 0.692 0.68\n",
            " 1.279 0.451 0.333 1.224 0.395 1.164 0.594 0.462 0.583]\n",
            "[0.    0.    0.804 0.    0.503 0.333 0.443 0.793 0.401 0.681 1.07  0.468\n",
            " 0.558 0.24  1.08  0.598 1.741 0.554 0.623 0.573 0.374 2.406 0.453 0.539\n",
            " 0.242 1.44  0.489 0.766 1.979 0.529 0.275 0.916 0.531 0.711 0.692 0.68\n",
            " 1.279 0.451 0.333 1.224 0.395 1.164 0.594 0.462 0.583]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.804 0.    0.503 0.333 0.443 0.793 0.401\n",
            " 0.681 1.07  0.468 0.558 0.24  1.08  0.598 1.741 0.554 0.623 0.573 0.374\n",
            " 2.406 0.453 0.539 0.242 1.44  0.489 0.766 1.979 0.529 0.275 0.916 0.531\n",
            " 0.711 0.692 0.68  1.279 0.451 0.333 1.224 0.395 1.164 0.594 0.462 0.583\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.   ]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.107 0.062 0.071 0.124 0.087 0.116 0.082 0.104 0.091\n",
            " 0.107 0.116 0.117 0.115 0.115 0.049 0.107 0.124 0.095 0.156 0.107 0.141\n",
            " 0.099 0.069 0.09  0.116 0.107 0.079 0.107 0.096 0.087 0.081 0.107 0.105\n",
            " 0.121 0.082 0.08  0.092 0.075 0.116 0.09  0.121 0.107 0.117 0.104 0.125\n",
            " 0.107 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.   ]\n",
            "(100,)\n",
            "<class 'int'>\n",
            "217\n",
            "/content/gdrive/My Drive/thesis/Data/S09/S09_TEX05.txt\n",
            "9\n",
            "[0.563 2.348 1.193 0.581 0.827 0.22  0.658 0.717 0.495 1.705 0.455 0.707\n",
            " 0.802 0.83  2.411 0.669 0.332 0.557 1.833 0.704 0.608 0.496 0.598 0.467\n",
            " 0.307 0.464 1.966 1.003 0.645 0.225 0.507 0.534 0.799 0.473 0.588 0.448\n",
            " 0.275 0.55  0.497 0.249 0.375 1.014 0.549]\n",
            "[0.563 2.348 1.193 0.581 0.827 0.22  0.658 0.717 0.495 1.705 0.455 0.707\n",
            " 0.802 0.83  2.411 0.669 0.332 0.557 1.833 0.704 0.608 0.496 0.598 0.467\n",
            " 0.307 0.464 1.966 1.003 0.645 0.225 0.507 0.534 0.799 0.473 0.588 0.448\n",
            " 0.275 0.55  0.497 0.249 0.375 1.014 0.549]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.563 2.348 1.193 0.581 0.827 0.22  0.658 0.717\n",
            " 0.495 1.705 0.455 0.707 0.802 0.83  2.411 0.669 0.332 0.557 1.833 0.704\n",
            " 0.608 0.496 0.598 0.467 0.307 0.464 1.966 1.003 0.645 0.225 0.507 0.534\n",
            " 0.799 0.473 0.588 0.448 0.275 0.55  0.497 0.249 0.375 1.014 0.549 0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.   ]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.071 0.133 0.111 0.124 0.149 0.112 0.083 0.08\n",
            " 0.095 0.107 0.088 0.133 0.1   0.087 0.087 0.12  0.099 0.075 0.091 0.103\n",
            " 0.106 0.13  0.108 0.1   0.062 0.107 0.116 0.076 0.121 0.108 0.083 0.124\n",
            " 0.097 0.088 0.114 0.106 0.124 0.115 0.08  0.107 0.107 0.064 0.113 0.122\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.   ]\n",
            "(100,)\n",
            "<class 'int'>\n",
            "218\n",
            "/content/gdrive/My Drive/thesis/Data/S09/S09_TEX03.txt\n",
            "9\n",
            "[0.373 0.707 0.549 0.498 0.541 0.499 1.281 2.62  1.193 0.523 0.457 0.724\n",
            " 0.639 0.673 0.3   0.54  1.498 0.588 0.666 0.723 0.909 0.766 1.02  0.597\n",
            " 0.607 0.523 1.733 0.495 0.666 0.698 1.526 1.509 0.549 0.306 0.524 0.69\n",
            " 1.222 0.657 0.606 0.399 2.822 0.288 0.935 0.413]\n",
            "[0.373 0.707 0.549 0.498 0.541 0.499 1.281 2.62  1.193 0.523 0.457 0.724\n",
            " 0.639 0.673 0.3   0.54  1.498 0.588 0.666 0.723 0.909 0.766 1.02  0.597\n",
            " 0.607 0.523 1.733 0.495 0.666 0.698 1.526 1.509 0.549 0.306 0.524 0.69\n",
            " 1.222 0.657 0.606 0.399 2.822 0.288 0.935 0.413]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.373 0.707 0.549 0.498 0.541 0.499 1.281 2.62\n",
            " 1.193 0.523 0.457 0.724 0.639 0.673 0.3   0.54  1.498 0.588 0.666 0.723\n",
            " 0.909 0.766 1.02  0.597 0.607 0.523 1.733 0.495 0.666 0.698 1.526 1.509\n",
            " 0.549 0.306 0.524 0.69  1.222 0.657 0.606 0.399 2.822 0.288 0.935 0.413\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.   ]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.098 0.141 0.099 0.115 0.149 0.098 0.09  0.122 0.094\n",
            " 0.115 0.107 0.141 0.099 0.108 0.15  0.124 0.108 0.08  0.108 0.107 0.107\n",
            " 0.088 0.086 0.113 0.124 0.123 0.091 0.105 0.116 0.108 0.099 0.102 0.115\n",
            " 0.115 0.107 0.116 0.108 0.132 0.165 0.124 0.133 0.097 0.124 0.119 0.089\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.   ]\n",
            "(100,)\n",
            "<class 'int'>\n",
            "219\n",
            "/content/gdrive/My Drive/thesis/Data/S09/S09_TEX11.txt\n",
            "9\n",
            "[0.482 0.424 0.639 0.558 0.582 0.399 0.566 0.773 1.714 0.654 0.233 0.967\n",
            " 1.799 0.878 0.265 0.389 1.926 0.519 0.584 0.666 0.422 0.566 1.5   0.595\n",
            " 1.723 0.522 0.407 1.255 1.648 0.874 0.43  0.698 0.697 0.625 0.543 0.396\n",
            " 0.524 0.658 1.79  0.906 0.304 0.674 0.391 1.105 1.325 0.687 0.672 0.768\n",
            " 0.68  1.35  0.67 ]\n",
            "[0.482 0.424 0.639 0.558 0.582 0.399 0.566 0.773 1.714 0.654 0.233 0.967\n",
            " 1.799 0.878 0.265 0.389 1.926 0.519 0.584 0.666 0.422 0.566 1.5   0.595\n",
            " 1.723 0.522 0.407 1.255 1.648 0.874 0.43  0.698 0.697 0.625 0.543 0.396\n",
            " 0.524 0.658 1.79  0.906 0.304 0.674 0.391 1.105 1.325 0.687 0.672 0.768\n",
            " 0.68  1.35  0.67 ]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.482 0.424 0.639 0.558 0.582 0.399 0.566 0.773 1.714 0.654 0.233 0.967\n",
            " 1.799 0.878 0.265 0.389 1.926 0.519 0.584 0.666 0.422 0.566 1.5   0.595\n",
            " 1.723 0.522 0.407 1.255 1.648 0.874 0.43  0.698 0.697 0.625 0.543 0.396\n",
            " 0.524 0.658 1.79  0.906 0.304 0.674 0.391 1.105 1.325 0.687 0.672 0.768\n",
            " 0.68  1.35  0.67  0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.   ]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.073 0.115 0.099 0.099 0.115 0.107 0.14  0.114 0.106 0.096 0.116 0.083\n",
            " 0.122 0.079 0.132 0.074 0.076 0.07  0.099 0.081 0.089 0.14  0.115 0.095\n",
            " 0.098 0.105 0.098 0.114 0.107 0.098 0.129 0.081 0.099 0.084 0.09  0.096\n",
            " 0.124 0.131 0.073 0.103 0.104 0.083 0.099 0.081 0.124 0.103 0.133 0.126\n",
            " 0.114 0.133 0.146 0.124 0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.   ]\n",
            "(100,)\n",
            "<class 'int'>\n",
            "220\n",
            "/content/gdrive/My Drive/thesis/Data/S09/S09_TEX04.txt\n",
            "9\n",
            "[0.529 1.267 0.711 0.614 0.283 0.599 0.542 0.481 0.474 0.498 0.601 0.472\n",
            " 0.51  0.254 1.123 0.69  0.315 0.865 0.582 0.665 2.289 0.571 0.672 0.524\n",
            " 0.283 0.607 0.649 0.275 0.382 0.78  0.675 0.713 2.622 0.578 0.623 1.64\n",
            " 0.562 0.84  0.508 0.533 1.756 1.177 0.481 0.45  0.524 0.722 0.283 0.524\n",
            " 0.482 0.374 3.047 0.244 0.86  0.381 0.297 0.541 0.648 0.86  1.121 0.619\n",
            " 0.35  0.609 4.266 1.469 0.866 0.493 0.393 0.664 2.772 0.611 0.665 0.25\n",
            " 0.482 0.507 0.774 0.773 0.415 0.506 0.674 0.3   1.89  0.762 0.522 0.467\n",
            " 1.618 0.425 0.333 0.8   1.739 0.362 1.214 0.531]\n",
            "[0.529 1.267 0.711 0.614 0.283 0.599 0.542 0.481 0.474 0.498 0.601 0.472\n",
            " 0.51  0.254 1.123 0.69  0.315 0.865 0.582 0.665 2.289 0.571 0.672 0.524\n",
            " 0.283 0.607 0.649 0.275 0.382 0.78  0.675 0.713 2.622 0.578 0.623 1.64\n",
            " 0.562 0.84  0.508 0.533 1.756 1.177 0.481 0.45  0.524 0.722 0.283 0.524\n",
            " 0.482 0.374 0.    0.244 0.86  0.381 0.297 0.541 0.648 0.86  1.121 0.619\n",
            " 0.35  0.609 0.    1.469 0.866 0.493 0.393 0.664 2.772 0.611 0.665 0.25\n",
            " 0.482 0.507 0.774 0.773 0.415 0.506 0.674 0.3   1.89  0.762 0.522 0.467\n",
            " 1.618 0.425 0.333 0.8   1.739 0.362 1.214 0.531]\n",
            "[0.    0.    0.    0.    0.529 1.267 0.711 0.614 0.283 0.599 0.542 0.481\n",
            " 0.474 0.498 0.601 0.472 0.51  0.254 1.123 0.69  0.315 0.865 0.582 0.665\n",
            " 2.289 0.571 0.672 0.524 0.283 0.607 0.649 0.275 0.382 0.78  0.675 0.713\n",
            " 2.622 0.578 0.623 1.64  0.562 0.84  0.508 0.533 1.756 1.177 0.481 0.45\n",
            " 0.524 0.722 0.283 0.524 0.482 0.374 0.    0.244 0.86  0.381 0.297 0.541\n",
            " 0.648 0.86  1.121 0.619 0.35  0.609 0.    1.469 0.866 0.493 0.393 0.664\n",
            " 2.772 0.611 0.665 0.25  0.482 0.507 0.774 0.773 0.415 0.506 0.674 0.3\n",
            " 1.89  0.762 0.522 0.467 1.618 0.425 0.333 0.8   1.739 0.362 1.214 0.531\n",
            " 0.    0.    0.    0.   ]\n",
            "[0.    0.    0.    0.095 0.106 0.078 0.099 0.117 0.091 0.116 0.105 0.099\n",
            " 0.107 0.107 0.097 0.099 0.095 0.084 0.091 0.089 0.108 0.124 0.133 0.132\n",
            " 0.12  0.139 0.116 0.115 0.124 0.091 0.098 0.113 0.064 0.116 0.114 0.099\n",
            " 0.128 0.107 0.108 0.121 0.117 0.108 0.106 0.115 0.096 0.115 0.125 0.106\n",
            " 0.123 0.099 0.098 0.107 0.091 0.099 0.094 0.09  0.087 0.089 0.124 0.115\n",
            " 0.09  0.094 0.103 0.125 0.091 0.088 0.078 0.106 0.103 0.108 0.115 0.108\n",
            " 0.095 0.107 0.116 0.098 0.14  0.116 0.123 0.113 0.09  0.115 0.14  0.089\n",
            " 0.129 0.089 0.098 0.097 0.099 0.116 0.107 0.08  0.07  0.09  0.122 0.099\n",
            " 0.    0.    0.    0.   ]\n",
            "(100,)\n",
            "<class 'int'>\n",
            "221\n",
            "/content/gdrive/My Drive/thesis/Data/S09/S09_TEX06.txt\n",
            "9\n",
            "[1.159 0.55  0.558 0.671 0.606 0.274 0.559 0.198 0.525 0.273 0.424 2.652\n",
            " 0.64  1.922 0.637 2.048 1.194 0.757 0.614 0.535 0.488 2.457 0.918 0.665\n",
            " 0.558 0.281 0.723 2.032 0.979 0.471 0.525 0.55  0.356 0.517 0.506 0.463\n",
            " 0.767 2.28  0.677 1.147 0.953 0.56  1.543 1.11  0.801 0.696 0.598 0.498\n",
            " 0.391 0.565 2.779 0.708 0.479 0.633 0.546 0.293 0.556 0.325 0.991 0.866\n",
            " 0.395]\n",
            "[1.159 0.55  0.558 0.671 0.606 0.274 0.559 0.198 0.525 0.273 0.424 2.652\n",
            " 0.64  1.922 0.637 2.048 1.194 0.757 0.614 0.535 0.488 2.457 0.918 0.665\n",
            " 0.558 0.281 0.723 2.032 0.979 0.471 0.525 0.55  0.356 0.517 0.506 0.463\n",
            " 0.767 2.28  0.677 1.147 0.953 0.56  1.543 1.11  0.801 0.696 0.598 0.498\n",
            " 0.391 0.565 2.779 0.708 0.479 0.633 0.546 0.293 0.556 0.325 0.991 0.866\n",
            " 0.395]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    1.159 0.55  0.558 0.671 0.606\n",
            " 0.274 0.559 0.198 0.525 0.273 0.424 2.652 0.64  1.922 0.637 2.048 1.194\n",
            " 0.757 0.614 0.535 0.488 2.457 0.918 0.665 0.558 0.281 0.723 2.032 0.979\n",
            " 0.471 0.525 0.55  0.356 0.517 0.506 0.463 0.767 2.28  0.677 1.147 0.953\n",
            " 0.56  1.543 1.11  0.801 0.696 0.598 0.498 0.391 0.565 2.779 0.708 0.479\n",
            " 0.633 0.546 0.293 0.556 0.325 0.991 0.866 0.395 0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.   ]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.103 0.098 0.123 0.147 0.089\n",
            " 0.1   0.1   0.098 0.074 0.105 0.107 0.107 0.071 0.098 0.113 0.082 0.096\n",
            " 0.124 0.123 0.091 0.113 0.107 0.103 0.115 0.107 0.106 0.124 0.091 0.095\n",
            " 0.088 0.141 0.107 0.113 0.115 0.114 0.122 0.141 0.124 0.112 0.109 0.125\n",
            " 0.102 0.15  0.094 0.132 0.12  0.106 0.098 0.114 0.157 0.148 0.129 0.095\n",
            " 0.123 0.139 0.092 0.089 0.116 0.156 0.087 0.102 0.124 0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.   ]\n",
            "(100,)\n",
            "<class 'int'>\n",
            "222\n",
            "/content/gdrive/My Drive/thesis/Data/S09/S09_TEX07.txt\n",
            "9\n",
            "[0.345 1.163 0.49  0.533 0.358 1.106 0.941 2.295 0.627 0.517 0.484 1.038\n",
            " 0.529 0.542 0.471 0.758 0.34  0.45  0.616 0.555 4.5   1.645 0.562 0.625\n",
            " 1.873 0.76  0.266 0.483 0.834 1.38  0.563 0.724 7.216 0.473 0.5   2.638\n",
            " 0.864 0.523 0.681 0.45  0.332 0.516 0.722 0.326 0.832 0.553 0.258 0.324\n",
            " 0.599 2.095 0.573 0.877 0.645 0.284 0.431 1.311 2.586 0.866 0.653 0.3\n",
            " 0.649 0.782 1.415 0.663 0.641 0.524 0.349 0.707 0.575 0.339 0.624 0.433\n",
            " 0.615 0.877 2.328 0.805 0.654 1.325 0.478 1.475 0.229 0.742 0.549 0.48\n",
            " 0.836 0.814 1.093 0.515 1.872 0.573 0.256 0.474 0.875 0.464 0.825 0.454\n",
            " 0.333 0.751 0.29  0.231 0.526]\n",
            "[0.345 1.163 0.49  0.533 0.358 1.106 0.941 2.295 0.627 0.517 0.484 1.038\n",
            " 0.529 0.542 0.471 0.758 0.34  0.45  0.616 0.555 0.    1.645 0.562 0.625\n",
            " 1.873 0.76  0.266 0.483 0.834 1.38  0.563 0.724 0.    0.473 0.5   2.638\n",
            " 0.864 0.523 0.681 0.45  0.332 0.516 0.722 0.326 0.832 0.553 0.258 0.324\n",
            " 0.599 2.095 0.573 0.877 0.645 0.284 0.431 1.311 2.586 0.866 0.653 0.3\n",
            " 0.649 0.782 1.415 0.663 0.641 0.524 0.349 0.707 0.575 0.339 0.624 0.433\n",
            " 0.615 0.877 2.328 0.805 0.654 1.325 0.478 1.475 0.229 0.742 0.549 0.48\n",
            " 0.836 0.814 1.093 0.515 1.872 0.573 0.256 0.474 0.875 0.464 0.825 0.454\n",
            " 0.333 0.751 0.29  0.231 0.526]\n",
            "[0.345 1.163 0.49  0.533 0.358 1.106 0.941 2.295 0.627 0.517 0.484 1.038\n",
            " 0.529 0.542 0.471 0.758 0.34  0.45  0.616 0.555 0.    1.645 0.562 0.625\n",
            " 1.873 0.76  0.266 0.483 0.834 1.38  0.563 0.724 0.    0.473 0.5   2.638\n",
            " 0.864 0.523 0.681 0.45  0.332 0.516 0.722 0.326 0.832 0.553 0.258 0.324\n",
            " 0.599 2.095 0.573 0.877 0.645 0.284 0.431 1.311 2.586 0.866 0.653 0.3\n",
            " 0.649 0.782 1.415 0.663 0.641 0.524 0.349 0.707 0.575 0.339 0.624 0.433\n",
            " 0.615 0.877 2.328 0.805 0.654 1.325 0.478 1.475 0.229 0.742 0.549 0.48\n",
            " 0.836 0.814 1.093 0.515 1.872 0.573 0.256 0.474 0.875 0.464 0.825 0.454\n",
            " 0.333 0.751 0.29  0.231]\n",
            "[0.128 0.124 0.073 0.1   0.116 0.09  0.098 0.13  0.128 0.125 0.131 0.129\n",
            " 0.105 0.15  0.105 0.167 0.123 0.09  0.123 0.097 0.114 0.127 0.112 0.124\n",
            " 0.114 0.136 0.15  0.125 0.107 0.121 0.105 0.107 0.098 0.106 0.1   0.106\n",
            " 0.112 0.106 0.115 0.09  0.131 0.123 0.123 0.116 0.097 0.095 0.123 0.091\n",
            " 0.091 0.097 0.124 0.117 0.105 0.117 0.074 0.084 0.112 0.087 0.127 0.108\n",
            " 0.107 0.132 0.14  0.105 0.116 0.107 0.123 0.107 0.098 0.147 0.124 0.133\n",
            " 0.132 0.116 0.113 0.111 0.095 0.116 0.096 0.123 0.054 0.099 0.089 0.106\n",
            " 0.116 0.102 0.077 0.116 0.099 0.114 0.14  0.108 0.108 0.098 0.124 0.079\n",
            " 0.1   0.098 0.08  0.056]\n",
            "(100,)\n",
            "<class 'int'>\n",
            "223\n",
            "/content/gdrive/My Drive/thesis/Data/S09/S09_TEX09.txt\n",
            "9\n",
            "[0.73  0.811 0.678 0.382 0.859 0.544 0.543 0.925 0.78  0.645 0.524 0.609\n",
            " 0.432 0.449 0.356 0.582 2.17  0.448 0.976 0.513 0.532 0.558 0.544 0.542\n",
            " 0.517 0.604 0.699 1.782 0.77  0.632 0.399 0.707 2.155 0.746 0.556 0.542\n",
            " 1.022 0.413 0.69  1.747 0.265 0.707 0.399 0.515 0.639 0.902 0.362 0.507\n",
            " 0.342 0.736 2.268 0.479 0.576 0.505 0.549 0.833 0.407 0.908 0.363 0.442\n",
            " 0.649 0.456 0.541 0.475]\n",
            "[0.73  0.811 0.678 0.382 0.859 0.544 0.543 0.925 0.78  0.645 0.524 0.609\n",
            " 0.432 0.449 0.356 0.582 2.17  0.448 0.976 0.513 0.532 0.558 0.544 0.542\n",
            " 0.517 0.604 0.699 1.782 0.77  0.632 0.399 0.707 2.155 0.746 0.556 0.542\n",
            " 1.022 0.413 0.69  1.747 0.265 0.707 0.399 0.515 0.639 0.902 0.362 0.507\n",
            " 0.342 0.736 2.268 0.479 0.576 0.505 0.549 0.833 0.407 0.908 0.363 0.442\n",
            " 0.649 0.456 0.541 0.475]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.73  0.811 0.678 0.382 0.859 0.544\n",
            " 0.543 0.925 0.78  0.645 0.524 0.609 0.432 0.449 0.356 0.582 2.17  0.448\n",
            " 0.976 0.513 0.532 0.558 0.544 0.542 0.517 0.604 0.699 1.782 0.77  0.632\n",
            " 0.399 0.707 2.155 0.746 0.556 0.542 1.022 0.413 0.69  1.747 0.265 0.707\n",
            " 0.399 0.515 0.639 0.902 0.362 0.507 0.342 0.736 2.268 0.479 0.576 0.505\n",
            " 0.549 0.833 0.407 0.908 0.363 0.442 0.649 0.456 0.541 0.475 0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.   ]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.123 0.083 0.119 0.09  0.065 0.095 0.109\n",
            " 0.089 0.079 0.113 0.125 0.124 0.098 0.097 0.13  0.132 0.106 0.099 0.116\n",
            " 0.095 0.122 0.098 0.113 0.126 0.125 0.097 0.092 0.091 0.112 0.098 0.107\n",
            " 0.091 0.099 0.104 0.131 0.132 0.114 0.122 0.142 0.133 0.107 0.115 0.124\n",
            " 0.098 0.116 0.108 0.137 0.108 0.108 0.073 0.078 0.129 0.132 0.112 0.139\n",
            " 0.116 0.114 0.114 0.096 0.14  0.106 0.106 0.099 0.106 0.098 0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.   ]\n",
            "(100,)\n",
            "<class 'int'>\n",
            "224\n",
            "/content/gdrive/My Drive/thesis/Data/S09/S09_TEX02.txt\n",
            "9\n",
            "[0.287 0.984 0.587 0.867 2.444 1.846 0.78  0.447 0.256 0.691 1.114 0.589\n",
            " 0.49  0.64  1.283 2.052 0.647 0.481 0.632 1.255 0.739 0.408 0.399 0.707\n",
            " 2.073 1.419 0.697 0.357 0.498 0.432 0.558 0.357 1.513 0.624 0.705 0.615\n",
            " 0.432 0.616 0.482 0.716 0.68  0.358 6.542 2.013 0.89  0.571 0.713 0.549]\n",
            "[0.287 0.984 0.587 0.867 2.444 1.846 0.78  0.447 0.256 0.691 1.114 0.589\n",
            " 0.49  0.64  1.283 2.052 0.647 0.481 0.632 1.255 0.739 0.408 0.399 0.707\n",
            " 2.073 1.419 0.697 0.357 0.498 0.432 0.558 0.357 1.513 0.624 0.705 0.615\n",
            " 0.432 0.616 0.482 0.716 0.68  0.358 0.    2.013 0.89  0.571 0.713 0.549]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.287 0.984 0.587 0.867 2.444 1.846 0.78  0.447 0.256 0.691\n",
            " 1.114 0.589 0.49  0.64  1.283 2.052 0.647 0.481 0.632 1.255 0.739 0.408\n",
            " 0.399 0.707 2.073 1.419 0.697 0.357 0.498 0.432 0.558 0.357 1.513 0.624\n",
            " 0.705 0.615 0.432 0.616 0.482 0.716 0.68  0.358 0.    2.013 0.89  0.571\n",
            " 0.713 0.549 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.   ]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.137 0.124 0.078 0.132 0.088 0.145 0.103 0.171 0.123 0.091 0.098\n",
            " 0.139 0.116 0.174 0.125 0.104 0.147 0.123 0.149 0.166 0.124 0.183 0.124\n",
            " 0.115 0.131 0.136 0.131 0.157 0.107 0.115 0.1   0.14  0.115 0.14  0.182\n",
            " 0.133 0.149 0.183 0.124 0.107 0.147 0.14  0.123 0.114 0.105 0.105 0.148\n",
            " 0.099 0.082 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.   ]\n",
            "(100,)\n",
            "<class 'int'>\n",
            "225\n",
            "/content/gdrive/My Drive/thesis/Data/S09/S09_TEX08.txt\n",
            "9\n",
            "[0.571 3.031 0.627 0.647 0.674 1.442 0.888 0.437 0.616 0.792 0.647 0.458\n",
            " 0.674 1.305 0.847 0.488 0.35  0.523 0.558 0.273 0.657 0.39  0.417 1.272\n",
            " 1.612 0.565 0.782 0.689 0.424 2.764 0.602 0.758 0.408 0.781 1.23  0.273\n",
            " 0.241 1.198 0.648 0.341 0.567 0.696 0.614 0.424 3.152 0.783 1.03  0.553\n",
            " 0.501 0.557 0.498 0.416 0.784 0.247 0.457 0.485 1.704 0.571 0.383 0.665\n",
            " 2.282 0.577 0.932 0.524 0.356 0.574 0.407 0.408 0.615 0.267 0.532 0.323\n",
            " 0.399 0.667 0.499 0.599 0.841 0.246 0.425 0.597 0.308 0.457]\n",
            "[0.571 0.    0.627 0.647 0.674 1.442 0.888 0.437 0.616 0.792 0.647 0.458\n",
            " 0.674 1.305 0.847 0.488 0.35  0.523 0.558 0.273 0.657 0.39  0.417 1.272\n",
            " 1.612 0.565 0.782 0.689 0.424 2.764 0.602 0.758 0.408 0.781 1.23  0.273\n",
            " 0.241 1.198 0.648 0.341 0.567 0.696 0.614 0.424 0.    0.783 1.03  0.553\n",
            " 0.501 0.557 0.498 0.416 0.784 0.247 0.457 0.485 1.704 0.571 0.383 0.665\n",
            " 2.282 0.577 0.932 0.524 0.356 0.574 0.407 0.408 0.615 0.267 0.532 0.323\n",
            " 0.399 0.667 0.499 0.599 0.841 0.246 0.425 0.597 0.308 0.457]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.571 0.    0.627\n",
            " 0.647 0.674 1.442 0.888 0.437 0.616 0.792 0.647 0.458 0.674 1.305 0.847\n",
            " 0.488 0.35  0.523 0.558 0.273 0.657 0.39  0.417 1.272 1.612 0.565 0.782\n",
            " 0.689 0.424 2.764 0.602 0.758 0.408 0.781 1.23  0.273 0.241 1.198 0.648\n",
            " 0.341 0.567 0.696 0.614 0.424 0.    0.783 1.03  0.553 0.501 0.557 0.498\n",
            " 0.416 0.784 0.247 0.457 0.485 1.704 0.571 0.383 0.665 2.282 0.577 0.932\n",
            " 0.524 0.356 0.574 0.407 0.408 0.615 0.267 0.532 0.323 0.399 0.667 0.499\n",
            " 0.599 0.841 0.246 0.425 0.597 0.308 0.457 0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.   ]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.105 0.109 0.128 0.115\n",
            " 0.099 0.108 0.103 0.096 0.108 0.107 0.106 0.132 0.106 0.096 0.097 0.105\n",
            " 0.108 0.091 0.108 0.147 0.082 0.098 0.115 0.174 0.105 0.132 0.124 0.097\n",
            " 0.141 0.089 0.077 0.091 0.106 0.114 0.107 0.114 0.107 0.074 0.089 0.09\n",
            " 0.099 0.112 0.099 0.1   0.091 0.081 0.113 0.081 0.099 0.131 0.106 0.107\n",
            " 0.116 0.122 0.098 0.098 0.08  0.114 0.083 0.124 0.107 0.093 0.091 0.09\n",
            " 0.089 0.124 0.097 0.108 0.09  0.091 0.057 0.074 0.05  0.066 0.107 0.099\n",
            " 0.097 0.111 0.09  0.089 0.124 0.081 0.099 0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.   ]\n",
            "(100,)\n",
            "<class 'int'>\n",
            "226\n",
            "/content/gdrive/My Drive/thesis/Data/S11/S11_TEX04.txt\n",
            "11\n",
            "[0.881 0.769 0.874 0.613 0.799 0.532 1.091 0.771 1.221 0.631 0.649 0.598\n",
            " 0.823 2.504 1.363 0.853 1.342 0.957 0.725 0.758 0.798 0.815 0.767 0.777\n",
            " 0.715 0.901 0.862 0.931 1.106 0.763 1.214 3.876 1.143 0.739 0.967 2.084\n",
            " 0.809 0.795 0.872 0.675 0.836 1.03  1.796 0.718 1.003 0.754 0.728 0.948\n",
            " 0.615 0.581 0.816 0.672]\n",
            "[0.881 0.769 0.874 0.613 0.799 0.532 1.091 0.771 1.221 0.631 0.649 0.598\n",
            " 0.823 2.504 1.363 0.853 1.342 0.957 0.725 0.758 0.798 0.815 0.767 0.777\n",
            " 0.715 0.901 0.862 0.931 1.106 0.763 1.214 0.    1.143 0.739 0.967 2.084\n",
            " 0.809 0.795 0.872 0.675 0.836 1.03  1.796 0.718 1.003 0.754 0.728 0.948\n",
            " 0.615 0.581 0.816 0.672]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.881 0.769 0.874 0.613 0.799 0.532 1.091 0.771 1.221 0.631 0.649 0.598\n",
            " 0.823 2.504 1.363 0.853 1.342 0.957 0.725 0.758 0.798 0.815 0.767 0.777\n",
            " 0.715 0.901 0.862 0.931 1.106 0.763 1.214 0.    1.143 0.739 0.967 2.084\n",
            " 0.809 0.795 0.872 0.675 0.836 1.03  1.796 0.718 1.003 0.754 0.728 0.948\n",
            " 0.615 0.581 0.816 0.672 0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.   ]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.095\n",
            " 0.137 0.132 0.156 0.116 0.124 0.123 0.079 0.123 0.107 0.149 0.157 0.116\n",
            " 0.115 0.112 0.12  0.157 0.127 0.127 0.125 0.115 0.148 0.14  0.12  0.141\n",
            " 0.099 0.12  0.097 0.114 0.113 0.073 0.089 0.12  0.115 0.066 0.113 0.099\n",
            " 0.146 0.107 0.141 0.132 0.094 0.136 0.127 0.149 0.11  0.129 0.083 0.132\n",
            " 0.099 0.115 0.114 0.09  0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.   ]\n",
            "(100,)\n",
            "<class 'int'>\n",
            "227\n",
            "/content/gdrive/My Drive/thesis/Data/S11/S11_TEX07.txt\n",
            "11\n",
            "[1.393 0.773 0.664 0.785 0.894 0.74  0.803 1.241 0.74  0.706 0.823 0.65\n",
            " 0.696 0.868 0.84  0.815 1.279 0.973 1.455 1.494 0.983 0.528 0.804 0.74\n",
            " 0.834 0.807 0.662 0.95  4.439 0.822 0.655 0.713 0.835 0.83  0.746 1.247\n",
            " 0.738 0.792 0.765 2.344 0.947 0.923 0.815 2.809 0.889 0.761 1.067 0.989\n",
            " 0.605 0.756 0.615]\n",
            "[1.393 0.773 0.664 0.785 0.894 0.74  0.803 1.241 0.74  0.706 0.823 0.65\n",
            " 0.696 0.868 0.84  0.815 1.279 0.973 1.455 1.494 0.983 0.528 0.804 0.74\n",
            " 0.834 0.807 0.662 0.95  0.    0.822 0.655 0.713 0.835 0.83  0.746 1.247\n",
            " 0.738 0.792 0.765 2.344 0.947 0.923 0.815 2.809 0.889 0.761 1.067 0.989\n",
            " 0.605 0.756 0.615]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 1.393 0.773 0.664 0.785 0.894 0.74  0.803 1.241 0.74  0.706 0.823 0.65\n",
            " 0.696 0.868 0.84  0.815 1.279 0.973 1.455 1.494 0.983 0.528 0.804 0.74\n",
            " 0.834 0.807 0.662 0.95  0.    0.822 0.655 0.713 0.835 0.83  0.746 1.247\n",
            " 0.738 0.792 0.765 2.344 0.947 0.923 0.815 2.809 0.889 0.761 1.067 0.989\n",
            " 0.605 0.756 0.615 0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.   ]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.137 0.174 0.157 0.116 0.162 0.157 0.133 0.152 0.157 0.09  0.174 0.133\n",
            " 0.106 0.1   0.13  0.104 0.12  0.138 0.095 0.095 0.097 0.111 0.115 0.134\n",
            " 0.15  0.13  0.146 0.132 0.278 0.095 0.137 0.097 0.132 0.129 0.138 0.115\n",
            " 0.106 0.125 0.105 0.097 0.088 0.138 0.129 0.128 0.12  0.121 0.091 0.104\n",
            " 0.138 0.106 0.115 0.089 0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.   ]\n",
            "(100,)\n",
            "<class 'int'>\n",
            "228\n",
            "/content/gdrive/My Drive/thesis/Data/S11/S11_TEX08.txt\n",
            "11\n",
            "[0.794 0.825 0.915 0.871 0.782 1.307 0.687 1.139 1.306 0.613 0.706 2.339\n",
            " 0.83  0.696 0.931 1.643 0.794 0.852 0.753 0.827 0.753 0.818 1.008 0.644\n",
            " 0.715 0.876 0.738 0.765 1.472 0.784 0.713 0.607 0.828 0.79  1.702 1.147\n",
            " 0.691 0.707 0.834 0.916 1.397 0.745 1.197 0.874 0.699 0.722 0.862 0.754\n",
            " 0.758 2.369 0.696 0.844 1.055 0.715]\n",
            "[0.794 0.825 0.915 0.871 0.782 1.307 0.687 1.139 1.306 0.613 0.706 2.339\n",
            " 0.83  0.696 0.931 1.643 0.794 0.852 0.753 0.827 0.753 0.818 1.008 0.644\n",
            " 0.715 0.876 0.738 0.765 1.472 0.784 0.713 0.607 0.828 0.79  1.702 1.147\n",
            " 0.691 0.707 0.834 0.916 1.397 0.745 1.197 0.874 0.699 0.722 0.862 0.754\n",
            " 0.758 2.369 0.696 0.844 1.055 0.715]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.794\n",
            " 0.825 0.915 0.871 0.782 1.307 0.687 1.139 1.306 0.613 0.706 2.339 0.83\n",
            " 0.696 0.931 1.643 0.794 0.852 0.753 0.827 0.753 0.818 1.008 0.644 0.715\n",
            " 0.876 0.738 0.765 1.472 0.784 0.713 0.607 0.828 0.79  1.702 1.147 0.691\n",
            " 0.707 0.834 0.916 1.397 0.745 1.197 0.874 0.699 0.722 0.862 0.754 0.758\n",
            " 2.369 0.696 0.844 1.055 0.715 0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.   ]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.146 0.141\n",
            " 0.14  0.105 0.107 0.105 0.112 0.098 0.14  0.105 0.107 0.133 0.112 0.097\n",
            " 0.132 0.124 0.136 0.133 0.129 0.108 0.121 0.141 0.105 0.111 0.116 0.117\n",
            " 0.097 0.157 0.141 0.107 0.13  0.108 0.134 0.138 0.104 0.115 0.141 0.099\n",
            " 0.124 0.113 0.128 0.104 0.107 0.133 0.09  0.123 0.091 0.136 0.097 0.113\n",
            " 0.123 0.109 0.114 0.107 0.108 0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.   ]\n",
            "(100,)\n",
            "<class 'int'>\n",
            "229\n",
            "/content/gdrive/My Drive/thesis/Data/S11/S11_TEX11.txt\n",
            "11\n",
            "[1.028 1.123 1.29  0.913 0.93  1.094 0.672 0.484 0.688 0.665 0.4   1.489\n",
            " 0.856 0.704 0.868 0.771 0.77  0.852 0.863 0.737 0.507 0.698 0.851 0.913\n",
            " 0.538 0.78  0.818 0.986 0.934 0.854 1.212 1.481 1.039 0.663 0.906 0.756\n",
            " 0.966 0.906 0.724 0.562 0.868 0.78  0.969 0.714 0.834 0.738 1.823 1.603\n",
            " 1.154 0.955 1.589 0.638 0.818 0.536]\n",
            "[1.028 1.123 1.29  0.913 0.93  1.094 0.672 0.484 0.688 0.665 0.4   1.489\n",
            " 0.856 0.704 0.868 0.771 0.77  0.852 0.863 0.737 0.507 0.698 0.851 0.913\n",
            " 0.538 0.78  0.818 0.986 0.934 0.854 1.212 1.481 1.039 0.663 0.906 0.756\n",
            " 0.966 0.906 0.724 0.562 0.868 0.78  0.969 0.714 0.834 0.738 1.823 1.603\n",
            " 1.154 0.955 1.589 0.638 0.818 0.536]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    1.028\n",
            " 1.123 1.29  0.913 0.93  1.094 0.672 0.484 0.688 0.665 0.4   1.489 0.856\n",
            " 0.704 0.868 0.771 0.77  0.852 0.863 0.737 0.507 0.698 0.851 0.913 0.538\n",
            " 0.78  0.818 0.986 0.934 0.854 1.212 1.481 1.039 0.663 0.906 0.756 0.966\n",
            " 0.906 0.724 0.562 0.868 0.78  0.969 0.714 0.834 0.738 1.823 1.603 1.154\n",
            " 0.955 1.589 0.638 0.818 0.536 0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.   ]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.128 0.122\n",
            " 0.121 0.169 0.103 0.095 0.123 0.166 0.147 0.083 0.091 0.098 0.122 0.121\n",
            " 0.124 0.12  0.138 0.116 0.103 0.113 0.115 0.115 0.123 0.095 0.146 0.148\n",
            " 0.124 0.12  0.106 0.111 0.105 0.09  0.055 0.13  0.107 0.098 0.124 0.121\n",
            " 0.08  0.137 0.148 0.128 0.087 0.116 0.082 0.155 0.106 0.112 0.155 0.114\n",
            " 0.198 0.105 0.106 0.136 0.107 0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.   ]\n",
            "(100,)\n",
            "<class 'int'>\n",
            "230\n",
            "/content/gdrive/My Drive/thesis/Data/S11/S11_TEX09.txt\n",
            "11\n",
            "[0.838 0.848 0.834 1.022 0.712 0.911 0.786 0.841 0.49  1.171 0.691 0.811\n",
            " 0.814 0.712 0.794 0.719 0.657 0.794 0.773 0.754 1.026 0.824 0.973 0.799\n",
            " 1.04  0.697 0.874 0.779 0.809 0.597 0.942 1.528 0.806 0.843 0.824 0.698\n",
            " 0.721 1.118 0.612 1.024 0.572 1.093 1.723 0.711 1.241 0.897 0.934 0.805\n",
            " 0.774 0.917 0.805 0.522 1.034 0.661 1.078 0.604 0.615 0.926]\n",
            "[0.838 0.848 0.834 1.022 0.712 0.911 0.786 0.841 0.49  1.171 0.691 0.811\n",
            " 0.814 0.712 0.794 0.719 0.657 0.794 0.773 0.754 1.026 0.824 0.973 0.799\n",
            " 1.04  0.697 0.874 0.779 0.809 0.597 0.942 1.528 0.806 0.843 0.824 0.698\n",
            " 0.721 1.118 0.612 1.024 0.572 1.093 1.723 0.711 1.241 0.897 0.934 0.805\n",
            " 0.774 0.917 0.805 0.522 1.034 0.661 1.078 0.604 0.615 0.926]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.838 0.848 0.834\n",
            " 1.022 0.712 0.911 0.786 0.841 0.49  1.171 0.691 0.811 0.814 0.712 0.794\n",
            " 0.719 0.657 0.794 0.773 0.754 1.026 0.824 0.973 0.799 1.04  0.697 0.874\n",
            " 0.779 0.809 0.597 0.942 1.528 0.806 0.843 0.824 0.698 0.721 1.118 0.612\n",
            " 1.024 0.572 1.093 1.723 0.711 1.241 0.897 0.934 0.805 0.774 0.917 0.805\n",
            " 0.522 1.034 0.661 1.078 0.604 0.615 0.926 0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.   ]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.128 0.139 0.156 0.104\n",
            " 0.113 0.074 0.145 0.124 0.115 0.132 0.142 0.115 0.112 0.154 0.123 0.128\n",
            " 0.149 0.108 0.129 0.121 0.116 0.113 0.087 0.137 0.086 0.111 0.097 0.129\n",
            " 0.123 0.173 0.124 0.13  0.132 0.107 0.154 0.136 0.129 0.149 0.128 0.132\n",
            " 0.113 0.116 0.12  0.152 0.091 0.089 0.157 0.145 0.106 0.106 0.137 0.13\n",
            " 0.116 0.121 0.133 0.112 0.148 0.124 0.112 0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.   ]\n",
            "(100,)\n",
            "<class 'int'>\n",
            "231\n",
            "/content/gdrive/My Drive/thesis/Data/S11/S11_TEX03.txt\n",
            "11\n",
            "[1.52  0.981 0.731 0.856 0.839 0.841 0.938 0.772 0.94  0.905 0.825 0.982\n",
            " 0.611 0.943 0.797 0.799 1.003 0.948 0.621 0.747 0.608 1.004 3.035 0.656\n",
            " 0.857 0.874 0.857 0.93  0.664 0.792 0.999 0.988 0.628 0.957 0.821 0.966\n",
            " 0.563 0.781 0.852 2.175 0.763 0.99  0.83  1.068 0.678 0.967 0.777 1.607\n",
            " 1.027 0.951 1.577 0.581 0.884 0.906 0.72  2.029 0.874 1.347 0.446 0.672\n",
            " 1.164 0.992 1.504 0.647 0.817 0.797 0.529 0.673 0.648 0.926 0.638 0.841\n",
            " 1.153 1.612 0.514 0.776 0.93  0.556 0.868 0.897 0.753 0.959 1.21  2.537\n",
            " 0.797 1.138 1.907 0.719 0.698 0.901 0.788 1.071 1.956 0.71  0.977 1.08\n",
            " 5.934 0.569 2.355 0.906 1.414 0.931 1.01  0.739 0.516]\n",
            "[1.52  0.981 0.731 0.856 0.839 0.841 0.938 0.772 0.94  0.905 0.825 0.982\n",
            " 0.611 0.943 0.797 0.799 1.003 0.948 0.621 0.747 0.608 1.004 0.    0.656\n",
            " 0.857 0.874 0.857 0.93  0.664 0.792 0.999 0.988 0.628 0.957 0.821 0.966\n",
            " 0.563 0.781 0.852 2.175 0.763 0.99  0.83  1.068 0.678 0.967 0.777 1.607\n",
            " 1.027 0.951 1.577 0.581 0.884 0.906 0.72  2.029 0.874 1.347 0.446 0.672\n",
            " 1.164 0.992 1.504 0.647 0.817 0.797 0.529 0.673 0.648 0.926 0.638 0.841\n",
            " 1.153 1.612 0.514 0.776 0.93  0.556 0.868 0.897 0.753 0.959 1.21  2.537\n",
            " 0.797 1.138 1.907 0.719 0.698 0.901 0.788 1.071 1.956 0.71  0.977 1.08\n",
            " 0.    0.569 2.355 0.906 1.414 0.931 1.01  0.739 0.516]\n",
            "[1.52  0.981 0.731 0.856 0.839 0.841 0.938 0.772 0.94  0.905 0.825 0.982\n",
            " 0.611 0.943 0.797 0.799 1.003 0.948 0.621 0.747 0.608 1.004 0.    0.656\n",
            " 0.857 0.874 0.857 0.93  0.664 0.792 0.999 0.988 0.628 0.957 0.821 0.966\n",
            " 0.563 0.781 0.852 2.175 0.763 0.99  0.83  1.068 0.678 0.967 0.777 1.607\n",
            " 1.027 0.951 1.577 0.581 0.884 0.906 0.72  2.029 0.874 1.347 0.446 0.672\n",
            " 1.164 0.992 1.504 0.647 0.817 0.797 0.529 0.673 0.648 0.926 0.638 0.841\n",
            " 1.153 1.612 0.514 0.776 0.93  0.556 0.868 0.897 0.753 0.959 1.21  2.537\n",
            " 0.797 1.138 1.907 0.719 0.698 0.901 0.788 1.071 1.956 0.71  0.977 1.08\n",
            " 0.    0.569 2.355 0.906]\n",
            "[0.123 0.14  0.148 0.156 0.156 0.158 0.098 0.132 0.116 0.081 0.09  0.113\n",
            " 0.153 0.157 0.103 0.12  0.153 0.139 0.189 0.14  0.141 0.107 0.091 0.172\n",
            " 0.173 0.123 0.13  0.12  0.139 0.123 0.137 0.177 0.103 0.157 0.164 0.141\n",
            " 0.154 0.174 0.149 0.145 0.13  0.14  0.148 0.132 0.136 0.132 0.153 0.133\n",
            " 0.121 0.099 0.087 0.148 0.14  0.129 0.121 0.157 0.131 0.096 0.112 0.123\n",
            " 0.099 0.107 0.129 0.122 0.107 0.112 0.179 0.158 0.132 0.1   0.121 0.148\n",
            " 0.105 0.14  0.148 0.141 0.146 0.181 0.148 0.129 0.104 0.166 0.103 0.132\n",
            " 0.139 0.097 0.089 0.103 0.157 0.149 0.12  0.096 0.098 0.12  0.132 0.12\n",
            " 0.086 0.144 0.124 0.163]\n",
            "(100,)\n",
            "<class 'int'>\n",
            "232\n",
            "/content/gdrive/My Drive/thesis/Data/S11/S11_TEX02.txt\n",
            "11\n",
            "[0.967 0.903 1.072 1.071 0.996 0.599 0.805 0.967 2.017 0.751 1.096 1.853\n",
            " 0.788 0.841 0.99  0.772 0.828 0.991 0.84  0.97  0.714 0.649 1.082 0.787\n",
            " 1.138 0.764 0.798 0.458 1.946 0.89  1.479 3.812 0.884 1.125 0.707 0.818\n",
            " 2.58  1.156 1.416 1.338 0.814 1.659 1.553 0.866 0.306 0.814 1.288 0.905\n",
            " 0.739 0.739 0.992 0.695 0.55  0.815]\n",
            "[0.967 0.903 1.072 1.071 0.996 0.599 0.805 0.967 2.017 0.751 1.096 1.853\n",
            " 0.788 0.841 0.99  0.772 0.828 0.991 0.84  0.97  0.714 0.649 1.082 0.787\n",
            " 1.138 0.764 0.798 0.458 1.946 0.89  1.479 0.    0.884 1.125 0.707 0.818\n",
            " 2.58  1.156 1.416 1.338 0.814 1.659 1.553 0.866 0.306 0.814 1.288 0.905\n",
            " 0.739 0.739 0.992 0.695 0.55  0.815]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.967\n",
            " 0.903 1.072 1.071 0.996 0.599 0.805 0.967 2.017 0.751 1.096 1.853 0.788\n",
            " 0.841 0.99  0.772 0.828 0.991 0.84  0.97  0.714 0.649 1.082 0.787 1.138\n",
            " 0.764 0.798 0.458 1.946 0.89  1.479 0.    0.884 1.125 0.707 0.818 2.58\n",
            " 1.156 1.416 1.338 0.814 1.659 1.553 0.866 0.306 0.814 1.288 0.905 0.739\n",
            " 0.739 0.992 0.695 0.55  0.815 0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.   ]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.098 0.119\n",
            " 0.097 0.138 0.09  0.182 0.173 0.116 0.163 0.091 0.112 0.156 0.114 0.124\n",
            " 0.088 0.137 0.146 0.116 0.13  0.179 0.106 0.148 0.131 0.162 0.123 0.149\n",
            " 0.125 0.108 0.156 0.105 0.112 0.129 0.164 0.128 0.142 0.099 0.12  0.108\n",
            " 0.132 0.095 0.119 0.111 0.114 0.132 0.147 0.19  0.149 0.14  0.165 0.141\n",
            " 0.108 0.163 0.141 0.123 0.115 0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.   ]\n",
            "(100,)\n",
            "<class 'int'>\n",
            "233\n",
            "/content/gdrive/My Drive/thesis/Data/S11/S11_TEX06.txt\n",
            "11\n",
            "[0.678 0.625 1.132 0.746 0.739 1.041 0.631 0.738 0.851 1.094 1.451 0.719\n",
            " 1.091 0.654 0.786 0.865 0.997 1.753 0.661 0.732 0.935 2.484 1.356 0.528\n",
            " 0.631 1.722 1.923 0.922 0.88  0.738 0.76  0.756 0.753 0.975 0.988 1.779\n",
            " 0.797 2.958 0.756 0.798 0.882 0.728 1.173 1.906 0.857 0.752 0.8   0.865\n",
            " 0.841 0.971 0.914 0.694 0.676 1.229 0.857 0.989 0.672]\n",
            "[0.678 0.625 1.132 0.746 0.739 1.041 0.631 0.738 0.851 1.094 1.451 0.719\n",
            " 1.091 0.654 0.786 0.865 0.997 1.753 0.661 0.732 0.935 2.484 1.356 0.528\n",
            " 0.631 1.722 1.923 0.922 0.88  0.738 0.76  0.756 0.753 0.975 0.988 1.779\n",
            " 0.797 2.958 0.756 0.798 0.882 0.728 1.173 1.906 0.857 0.752 0.8   0.865\n",
            " 0.841 0.971 0.914 0.694 0.676 1.229 0.857 0.989 0.672]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.678 0.625 1.132\n",
            " 0.746 0.739 1.041 0.631 0.738 0.851 1.094 1.451 0.719 1.091 0.654 0.786\n",
            " 0.865 0.997 1.753 0.661 0.732 0.935 2.484 1.356 0.528 0.631 1.722 1.923\n",
            " 0.922 0.88  0.738 0.76  0.756 0.753 0.975 0.988 1.779 0.797 2.958 0.756\n",
            " 0.798 0.882 0.728 1.173 1.906 0.857 0.752 0.8   0.865 0.841 0.971 0.914\n",
            " 0.694 0.676 1.229 0.857 0.989 0.672 0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.   ]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.138 0.125 0.149\n",
            " 0.146 0.115 0.124 0.114 0.123 0.142 0.056 0.117 0.161 0.116 0.13  0.124\n",
            " 0.145 0.094 0.111 0.12  0.124 0.125 0.128 0.171 0.094 0.098 0.091 0.114\n",
            " 0.095 0.096 0.122 0.106 0.111 0.111 0.131 0.112 0.113 0.105 0.122 0.097\n",
            " 0.114 0.088 0.138 0.108 0.123 0.112 0.152 0.124 0.105 0.12  0.127 0.112\n",
            " 0.121 0.133 0.097 0.123 0.13  0.139 0.107 0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.   ]\n",
            "(100,)\n",
            "<class 'int'>\n",
            "234\n",
            "/content/gdrive/My Drive/thesis/Data/S11/S11_TEX10.txt\n",
            "11\n",
            "[0.863 0.771 2.149 0.603 1.301 0.702 0.475 0.647 1.067 0.596 0.858 2.077\n",
            " 0.647 0.722 0.886 0.914 1.027 0.715 0.714 0.993 0.745 0.919 0.778 0.78\n",
            " 0.85  0.998 0.747 0.595 0.967 0.546 0.681 0.714 0.715 1.189 1.906 0.751\n",
            " 1.155 0.726 0.62  0.859 0.662 0.723 0.908 0.931 1.004 1.79  0.725 0.943\n",
            " 0.832 0.854 0.812 1.083 0.612 0.95  0.564 0.762 0.591 1.031 1.873 0.778\n",
            " 0.901 0.946 1.404 0.779 0.456 0.991 0.762 0.826 0.67  0.881 2.235 1.464\n",
            " 1.095 0.807 0.989 0.838 0.649 0.673 0.918 0.604 0.689 1.016 0.939]\n",
            "[0.863 0.771 2.149 0.603 1.301 0.702 0.475 0.647 1.067 0.596 0.858 2.077\n",
            " 0.647 0.722 0.886 0.914 1.027 0.715 0.714 0.993 0.745 0.919 0.778 0.78\n",
            " 0.85  0.998 0.747 0.595 0.967 0.546 0.681 0.714 0.715 1.189 1.906 0.751\n",
            " 1.155 0.726 0.62  0.859 0.662 0.723 0.908 0.931 1.004 1.79  0.725 0.943\n",
            " 0.832 0.854 0.812 1.083 0.612 0.95  0.564 0.762 0.591 1.031 1.873 0.778\n",
            " 0.901 0.946 1.404 0.779 0.456 0.991 0.762 0.826 0.67  0.881 2.235 1.464\n",
            " 1.095 0.807 0.989 0.838 0.649 0.673 0.918 0.604 0.689 1.016 0.939]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.863 0.771 2.149 0.603\n",
            " 1.301 0.702 0.475 0.647 1.067 0.596 0.858 2.077 0.647 0.722 0.886 0.914\n",
            " 1.027 0.715 0.714 0.993 0.745 0.919 0.778 0.78  0.85  0.998 0.747 0.595\n",
            " 0.967 0.546 0.681 0.714 0.715 1.189 1.906 0.751 1.155 0.726 0.62  0.859\n",
            " 0.662 0.723 0.908 0.931 1.004 1.79  0.725 0.943 0.832 0.854 0.812 1.083\n",
            " 0.612 0.95  0.564 0.762 0.591 1.031 1.873 0.778 0.901 0.946 1.404 0.779\n",
            " 0.456 0.991 0.762 0.826 0.67  0.881 2.235 1.464 1.095 0.807 0.989 0.838\n",
            " 0.649 0.673 0.918 0.604 0.689 1.016 0.939 0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.   ]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.102 0.147 0.131 0.111\n",
            " 0.124 0.121 0.1   0.116 0.117 0.139 0.116 0.13  0.172 0.164 0.1   0.087\n",
            " 0.119 0.115 0.132 0.098 0.103 0.116 0.093 0.147 0.114 0.077 0.11  0.111\n",
            " 0.099 0.155 0.09  0.132 0.132 0.099 0.122 0.118 0.082 0.124 0.112 0.132\n",
            " 0.146 0.082 0.123 0.062 0.121 0.087 0.11  0.117 0.121 0.111 0.121 0.108\n",
            " 0.13  0.123 0.13  0.14  0.083 0.115 0.114 0.128 0.131 0.128 0.104 0.137\n",
            " 0.155 0.148 0.113 0.116 0.096 0.098 0.115 0.115 0.08  0.133 0.107 0.123\n",
            " 0.149 0.164 0.157 0.161 0.148 0.115 0.13  0.113 0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.   ]\n",
            "(100,)\n",
            "<class 'int'>\n",
            "235\n",
            "/content/gdrive/My Drive/thesis/Data/S11/S11_TEX05.txt\n",
            "11\n",
            "[0.913 0.965 0.695 0.85  0.671 0.942 0.821 0.556 0.516 1.033 0.879 0.73\n",
            " 1.033 1.044 1.076 0.646 0.843 1.176 1.973 0.786 0.86  1.044 0.8   0.807\n",
            " 0.53  0.851 1.363 0.789 0.437 0.69  0.917 1.602 0.826 0.777 0.716 0.755\n",
            " 1.463 0.743 1.218 0.909 0.806 0.829 0.64  0.817 0.673 0.795 1.05  0.621\n",
            " 0.531 0.641]\n",
            "[0.913 0.965 0.695 0.85  0.671 0.942 0.821 0.556 0.516 1.033 0.879 0.73\n",
            " 1.033 1.044 1.076 0.646 0.843 1.176 1.973 0.786 0.86  1.044 0.8   0.807\n",
            " 0.53  0.851 1.363 0.789 0.437 0.69  0.917 1.602 0.826 0.777 0.716 0.755\n",
            " 1.463 0.743 1.218 0.909 0.806 0.829 0.64  0.817 0.673 0.795 1.05  0.621\n",
            " 0.531 0.641]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.913 0.965 0.695 0.85  0.671 0.942 0.821 0.556 0.516 1.033 0.879\n",
            " 0.73  1.033 1.044 1.076 0.646 0.843 1.176 1.973 0.786 0.86  1.044 0.8\n",
            " 0.807 0.53  0.851 1.363 0.789 0.437 0.69  0.917 1.602 0.826 0.777 0.716\n",
            " 0.755 1.463 0.743 1.218 0.909 0.806 0.829 0.64  0.817 0.673 0.795 1.05\n",
            " 0.621 0.531 0.641 0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.   ]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.097 0.139 0.154 0.107 0.13  0.131 0.121 0.148 0.115 0.131 0.103 0.114\n",
            " 0.124 0.13  0.134 0.13  0.132 0.102 0.131 0.145 0.107 0.137 0.107 0.088\n",
            " 0.146 0.147 0.161 0.102 0.12  0.099 0.141 0.095 0.131 0.136 0.157 0.123\n",
            " 0.107 0.082 0.129 0.116 0.129 0.112 0.132 0.106 0.112 0.145 0.09  0.137\n",
            " 0.131 0.141 0.106 0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.   ]\n",
            "(100,)\n",
            "<class 'int'>\n",
            "236\n",
            "/content/gdrive/My Drive/thesis/Data/S10/S10_TEX11.txt\n",
            "10\n",
            "[0.48  1.059 1.06  0.79  1.69  0.861 1.375 1.005 0.438 0.505 0.944 3.309\n",
            " 2.746 1.015 1.176 1.154 0.856 1.139 0.369 1.955 0.615 0.44  0.901 0.914\n",
            " 1.156 3.225 0.469 0.781 0.919 0.97  0.514 0.885 0.838 1.406 0.865 0.762\n",
            " 0.95  0.813 1.021 1.516 0.819 0.834 1.061 1.966 0.889 0.905]\n",
            "[0.48  1.059 1.06  0.79  1.69  0.861 1.375 1.005 0.438 0.505 0.944 0.\n",
            " 2.746 1.015 1.176 1.154 0.856 1.139 0.369 1.955 0.615 0.44  0.901 0.914\n",
            " 1.156 0.    0.469 0.781 0.919 0.97  0.514 0.885 0.838 1.406 0.865 0.762\n",
            " 0.95  0.813 1.021 1.516 0.819 0.834 1.061 1.966 0.889 0.905]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.48  1.059 1.06  0.79  1.69  0.861 1.375 1.005 0.438\n",
            " 0.505 0.944 0.    2.746 1.015 1.176 1.154 0.856 1.139 0.369 1.955 0.615\n",
            " 0.44  0.901 0.914 1.156 0.    0.469 0.781 0.919 0.97  0.514 0.885 0.838\n",
            " 1.406 0.865 0.762 0.95  0.813 1.021 1.516 0.819 0.834 1.061 1.966 0.889\n",
            " 0.905 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.   ]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.189 0.215 0.228 0.157 0.339 0.195 0.174 0.229 0.212 0.206\n",
            " 0.167 0.244 0.177 0.167 0.214 0.154 0.161 0.236 0.153 0.217 0.231 0.198\n",
            " 0.183 0.216 0.196 0.294 0.195 0.209 0.177 0.246 0.166 0.166 0.189 0.272\n",
            " 0.179 0.179 0.207 0.171 0.197 0.232 0.228 0.166 0.221 0.248 0.194 0.196\n",
            " 0.187 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.   ]\n",
            "(100,)\n",
            "<class 'int'>\n",
            "237\n",
            "/content/gdrive/My Drive/thesis/Data/S10/S10_TEX09.txt\n",
            "10\n",
            "[0.696 0.492 1.255 0.54  0.748 0.756 0.983 3.368 1.186 0.933 0.812 0.713\n",
            " 1.001 2.986 1.249 0.803 0.769 1.475 1.064 0.997 1.166 1.162 1.455 1.331\n",
            " 0.721 1.006 1.055 3.823 2.269 1.03  1.23  0.941 2.251 1.18  0.572 0.892\n",
            " 0.939 1.014 2.095 1.156 0.777 0.757 0.788 0.799 1.141 0.679 0.366 0.599]\n",
            "[0.696 0.492 1.255 0.54  0.748 0.756 0.983 0.    1.186 0.933 0.812 0.713\n",
            " 1.001 2.986 1.249 0.803 0.769 1.475 1.064 0.997 1.166 1.162 1.455 1.331\n",
            " 0.721 1.006 1.055 0.    2.269 1.03  1.23  0.941 2.251 1.18  0.572 0.892\n",
            " 0.939 1.014 2.095 1.156 0.777 0.757 0.788 0.799 1.141 0.679 0.366 0.599]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.696 0.492 1.255 0.54  0.748 0.756 0.983 0.    1.186 0.933\n",
            " 0.812 0.713 1.001 2.986 1.249 0.803 0.769 1.475 1.064 0.997 1.166 1.162\n",
            " 1.455 1.331 0.721 1.006 1.055 0.    2.269 1.03  1.23  0.941 2.251 1.18\n",
            " 0.572 0.892 0.939 1.014 2.095 1.156 0.777 0.757 0.788 0.799 1.141 0.679\n",
            " 0.366 0.599 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.   ]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.181 0.208 0.164 0.18  0.207 0.158 0.175 0.182 0.227 0.139 0.188\n",
            " 0.148 0.165 0.237 0.244 0.168 0.155 0.167 0.203 0.146 0.261 0.17  0.203\n",
            " 0.154 0.205 0.213 0.187 0.255 0.054 0.188 0.255 0.139 0.17  0.196 0.171\n",
            " 0.141 0.164 0.188 0.179 0.203 0.169 0.166 0.207 0.175 0.248 0.213 0.148\n",
            " 0.182 0.149 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.   ]\n",
            "(100,)\n",
            "<class 'int'>\n",
            "238\n",
            "/content/gdrive/My Drive/thesis/Data/S10/S10_TEX10.txt\n",
            "10\n",
            "[1.247 1.267 0.883 1.098 1.079 0.934 0.84  0.964 0.681 0.873 0.898 1.421\n",
            " 0.47  1.075 0.652 1.133 0.664 0.823 0.58  1.327 0.988 1.181 0.793 0.491\n",
            " 1.78  0.805 1.272 3.083 0.934 0.968 1.323 1.416 0.845 0.932 0.404 1.19\n",
            " 1.482 0.776 0.959 0.9   1.211 1.5   2.33  0.781 1.29  1.058 0.437 1.176\n",
            " 0.81  0.457 1.083 1.463 0.805 0.887]\n",
            "[1.247 1.267 0.883 1.098 1.079 0.934 0.84  0.964 0.681 0.873 0.898 1.421\n",
            " 0.47  1.075 0.652 1.133 0.664 0.823 0.58  1.327 0.988 1.181 0.793 0.491\n",
            " 1.78  0.805 1.272 0.    0.934 0.968 1.323 1.416 0.845 0.932 0.404 1.19\n",
            " 1.482 0.776 0.959 0.9   1.211 1.5   2.33  0.781 1.29  1.058 0.437 1.176\n",
            " 0.81  0.457 1.083 1.463 0.805 0.887]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    1.247\n",
            " 1.267 0.883 1.098 1.079 0.934 0.84  0.964 0.681 0.873 0.898 1.421 0.47\n",
            " 1.075 0.652 1.133 0.664 0.823 0.58  1.327 0.988 1.181 0.793 0.491 1.78\n",
            " 0.805 1.272 0.    0.934 0.968 1.323 1.416 0.845 0.932 0.404 1.19  1.482\n",
            " 0.776 0.959 0.9   1.211 1.5   2.33  0.781 1.29  1.058 0.437 1.176 0.81\n",
            " 0.457 1.083 1.463 0.805 0.887 0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.   ]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.213 0.237\n",
            " 0.225 0.165 0.222 0.207 0.138 0.212 0.171 0.188 0.121 0.186 0.178 0.198\n",
            " 0.17  0.208 0.189 0.208 0.132 0.226 0.252 0.211 0.237 0.183 0.183 0.201\n",
            " 0.165 0.248 0.206 0.179 0.157 0.19  0.187 0.205 0.165 0.15  0.189 0.153\n",
            " 0.208 0.147 0.243 0.172 0.278 0.227 0.175 0.207 0.179 0.19  0.162 0.199\n",
            " 0.175 0.279 0.137 0.234 0.203 0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.   ]\n",
            "(100,)\n",
            "<class 'int'>\n",
            "239\n",
            "/content/gdrive/My Drive/thesis/Data/S10/S10_TEX07.txt\n",
            "10\n",
            "[0.635 2.91  0.967 0.855 0.586 1.001 0.737 1.323 0.622 0.731 1.016 1.239\n",
            " 0.488 2.421 0.762 1.581 0.673 0.482 0.448 1.193 0.963 2.512 1.011 0.828\n",
            " 0.924 1.021 0.797 0.85  1.72  1.08  1.961 1.054 0.822 8.386 1.751 1.205\n",
            " 1.012 1.038 1.491 1.12  0.397 1.499 1.188 1.005 1.803 0.735 0.909 1.013\n",
            " 1.222 0.93  0.905 2.019 0.663 0.816 1.174 0.545 1.754 1.01  0.826 0.968\n",
            " 0.923 3.007 0.972 0.662 0.631 0.8   0.981 0.872 0.71  1.106 0.925 0.972\n",
            " 0.897 0.922 0.956 0.778 0.901 0.776 1.417 0.701 0.893 0.856 1.361 1.048\n",
            " 1.122 0.37  0.747 0.526]\n",
            "[0.635 2.91  0.967 0.855 0.586 1.001 0.737 1.323 0.622 0.731 1.016 1.239\n",
            " 0.488 2.421 0.762 1.581 0.673 0.482 0.448 1.193 0.963 2.512 1.011 0.828\n",
            " 0.924 1.021 0.797 0.85  1.72  1.08  1.961 1.054 0.822 0.    1.751 1.205\n",
            " 1.012 1.038 1.491 1.12  0.397 1.499 1.188 1.005 1.803 0.735 0.909 1.013\n",
            " 1.222 0.93  0.905 2.019 0.663 0.816 1.174 0.545 1.754 1.01  0.826 0.968\n",
            " 0.923 0.    0.972 0.662 0.631 0.8   0.981 0.872 0.71  1.106 0.925 0.972\n",
            " 0.897 0.922 0.956 0.778 0.901 0.776 1.417 0.701 0.893 0.856 1.361 1.048\n",
            " 1.122 0.37  0.747 0.526]\n",
            "[0.    0.    0.    0.    0.    0.    0.635 2.91  0.967 0.855 0.586 1.001\n",
            " 0.737 1.323 0.622 0.731 1.016 1.239 0.488 2.421 0.762 1.581 0.673 0.482\n",
            " 0.448 1.193 0.963 2.512 1.011 0.828 0.924 1.021 0.797 0.85  1.72  1.08\n",
            " 1.961 1.054 0.822 0.    1.751 1.205 1.012 1.038 1.491 1.12  0.397 1.499\n",
            " 1.188 1.005 1.803 0.735 0.909 1.013 1.222 0.93  0.905 2.019 0.663 0.816\n",
            " 1.174 0.545 1.754 1.01  0.826 0.968 0.923 0.    0.972 0.662 0.631 0.8\n",
            " 0.981 0.872 0.71  1.106 0.925 0.972 0.897 0.922 0.956 0.778 0.901 0.776\n",
            " 1.417 0.701 0.893 0.856 1.361 1.048 1.122 0.37  0.747 0.526 0.    0.\n",
            " 0.    0.    0.    0.   ]\n",
            "[0.    0.    0.    0.    0.    0.269 0.2   0.238 0.161 0.188 0.233 0.173\n",
            " 0.216 0.19  0.164 0.241 0.237 0.154 0.206 0.214 0.183 0.19  0.19  0.197\n",
            " 0.224 0.145 0.187 0.192 0.178 0.213 0.203 0.238 0.156 0.195 0.194 0.245\n",
            " 0.203 0.155 0.246 0.21  0.231 0.196 0.24  0.205 0.17  0.188 0.19  0.21\n",
            " 0.179 0.252 0.144 0.19  0.178 0.212 0.227 0.17  0.255 0.181 0.217 0.273\n",
            " 0.179 0.165 0.193 0.186 0.229 0.162 0.252 0.203 0.161 0.14  0.139 0.236\n",
            " 0.169 0.146 0.24  0.199 0.212 0.179 0.195 0.27  0.146 0.199 0.17  0.258\n",
            " 0.185 0.2   0.146 0.269 0.146 0.228 0.153 0.173 0.066 0.19  0.    0.\n",
            " 0.    0.    0.    0.   ]\n",
            "(100,)\n",
            "<class 'int'>\n",
            "240\n",
            "/content/gdrive/My Drive/thesis/Data/S10/S10_TEX03.txt\n",
            "10\n",
            "[1.536 1.729 0.873 4.313 0.628 2.42  1.148 1.551 0.79  1.174 1.081 0.898\n",
            " 0.886 2.104 1.564 0.57  1.346 0.991 0.571 0.956 0.964 2.479 1.129 3.442\n",
            " 0.996 0.913 0.854 1.068 1.319 1.611 3.817 1.26  1.025 1.18  1.854 1.076\n",
            " 1.141 2.085 0.883 0.995 1.149 1.023 0.71  0.64  3.478 0.778 0.455 1.342]\n",
            "[1.536 1.729 0.873 0.    0.628 2.42  1.148 1.551 0.79  1.174 1.081 0.898\n",
            " 0.886 2.104 1.564 0.57  1.346 0.991 0.571 0.956 0.964 2.479 1.129 0.\n",
            " 0.996 0.913 0.854 1.068 1.319 1.611 0.    1.26  1.025 1.18  1.854 1.076\n",
            " 1.141 2.085 0.883 0.995 1.149 1.023 0.71  0.64  0.    0.778 0.455 1.342]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    1.536 1.729 0.873 0.    0.628 2.42  1.148 1.551 0.79  1.174\n",
            " 1.081 0.898 0.886 2.104 1.564 0.57  1.346 0.991 0.571 0.956 0.964 2.479\n",
            " 1.129 0.    0.996 0.913 0.854 1.068 1.319 1.611 0.    1.26  1.025 1.18\n",
            " 1.854 1.076 1.141 2.085 0.883 0.995 1.149 1.023 0.71  0.64  0.    0.778\n",
            " 0.455 1.342 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.   ]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.31  0.337 0.17  0.203 0.194 0.24  0.271 0.221 0.208 0.225 0.207\n",
            " 0.179 0.204 0.214 0.213 0.195 0.232 0.234 0.195 0.199 0.149 0.198 0.245\n",
            " 0.214 0.28  0.253 0.247 0.198 0.211 0.189 0.248 0.278 0.25  0.189 0.237\n",
            " 0.203 0.149 0.196 0.188 0.169 0.247 0.186 0.162 0.191 0.199 0.204 0.173\n",
            " 0.191 0.154 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.   ]\n",
            "(100,)\n",
            "<class 'int'>\n",
            "241\n",
            "/content/gdrive/My Drive/thesis/Data/S10/S10_TEX04.txt\n",
            "10\n",
            "[1.58  1.136 0.786 1.667 1.005 0.93  3.209 1.204 1.468 1.074 1.867 0.951\n",
            " 3.297 1.614 1.701 1.115 1.115 1.71  1.099 1.229 2.7   1.18  0.923 0.702\n",
            " 1.183 1.657 1.101 0.822 0.703 0.928 1.063 1.696 0.913 1.198 2.076 1.099\n",
            " 1.113 0.971 1.374 0.401 0.952 1.818 0.899 0.971 0.438 1.218 0.752 1.324]\n",
            "[1.58  1.136 0.786 1.667 1.005 0.93  0.    1.204 1.468 1.074 1.867 0.951\n",
            " 0.    1.614 1.701 1.115 1.115 1.71  1.099 1.229 2.7   1.18  0.923 0.702\n",
            " 1.183 1.657 1.101 0.822 0.703 0.928 1.063 1.696 0.913 1.198 2.076 1.099\n",
            " 1.113 0.971 1.374 0.401 0.952 1.818 0.899 0.971 0.438 1.218 0.752 1.324]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    1.58  1.136 0.786 1.667 1.005 0.93  0.    1.204 1.468 1.074\n",
            " 1.867 0.951 0.    1.614 1.701 1.115 1.115 1.71  1.099 1.229 2.7   1.18\n",
            " 0.923 0.702 1.183 1.657 1.101 0.822 0.703 0.928 1.063 1.696 0.913 1.198\n",
            " 2.076 1.099 1.113 0.971 1.374 0.401 0.952 1.818 0.899 0.971 0.438 1.218\n",
            " 0.752 1.324 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.   ]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.302 0.352 0.222 0.207 0.212 0.178 0.195 0.252 0.212 0.189 0.187\n",
            " 0.174 0.179 0.222 0.145 0.239 0.197 0.212 0.205 0.227 0.186 0.278 0.212\n",
            " 0.186 0.241 0.22  0.226 0.205 0.164 0.209 0.253 0.196 0.231 0.232 0.236\n",
            " 0.23  0.202 0.228 0.262 0.126 0.183 0.19  0.247 0.196 0.172 0.232 0.162\n",
            " 0.158 0.241 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.   ]\n",
            "(100,)\n",
            "<class 'int'>\n",
            "242\n",
            "/content/gdrive/My Drive/thesis/Data/S10/S10_TEX02.txt\n",
            "10\n",
            "[1.489 3.468 1.985 1.538 1.055 3.7   1.504 0.503 1.633 0.821 0.982 1.351\n",
            " 1.101 1.097 1.074 6.526 1.953 1.444 1.223 1.771 1.159 1.967 1.228 1.687\n",
            " 1.252 1.217 0.906 1.097 0.679 3.056 1.66  0.813 0.823 2.046 0.697 1.933\n",
            " 0.889 1.199 2.091 0.763 0.691 0.666 3.185 1.937 0.836 0.723 0.892 1.11\n",
            " 0.822 1.118 0.777 0.715 1.124 3.483 1.063 0.989 1.685 0.706 1.639 0.495\n",
            " 0.897 1.382 0.71  1.468 1.089 0.621 2.889 2.86  0.67  1.982 0.735 1.95\n",
            " 2.364 1.325 0.81  1.175 1.036 5.511 1.164 0.987 0.855 0.837 1.449 0.905\n",
            " 0.776 1.666 0.962 0.796 1.084 0.786 1.381 0.824 0.645]\n",
            "[1.489 0.    1.985 1.538 1.055 0.    1.504 0.503 1.633 0.821 0.982 1.351\n",
            " 1.101 1.097 1.074 0.    1.953 1.444 1.223 1.771 1.159 1.967 1.228 1.687\n",
            " 1.252 1.217 0.906 1.097 0.679 0.    1.66  0.813 0.823 2.046 0.697 1.933\n",
            " 0.889 1.199 2.091 0.763 0.691 0.666 0.    1.937 0.836 0.723 0.892 1.11\n",
            " 0.822 1.118 0.777 0.715 1.124 0.    1.063 0.989 1.685 0.706 1.639 0.495\n",
            " 0.897 1.382 0.71  1.468 1.089 0.621 2.889 2.86  0.67  1.982 0.735 1.95\n",
            " 2.364 1.325 0.81  1.175 1.036 0.    1.164 0.987 0.855 0.837 1.449 0.905\n",
            " 0.776 1.666 0.962 0.796 1.084 0.786 1.381 0.824 0.645]\n",
            "[0.    0.    0.    1.489 0.    1.985 1.538 1.055 0.    1.504 0.503 1.633\n",
            " 0.821 0.982 1.351 1.101 1.097 1.074 0.    1.953 1.444 1.223 1.771 1.159\n",
            " 1.967 1.228 1.687 1.252 1.217 0.906 1.097 0.679 0.    1.66  0.813 0.823\n",
            " 2.046 0.697 1.933 0.889 1.199 2.091 0.763 0.691 0.666 0.    1.937 0.836\n",
            " 0.723 0.892 1.11  0.822 1.118 0.777 0.715 1.124 0.    1.063 0.989 1.685\n",
            " 0.706 1.639 0.495 0.897 1.382 0.71  1.468 1.089 0.621 2.889 2.86  0.67\n",
            " 1.982 0.735 1.95  2.364 1.325 0.81  1.175 1.036 0.    1.164 0.987 0.855\n",
            " 0.837 1.449 0.905 0.776 1.666 0.962 0.796 1.084 0.786 1.381 0.824 0.645\n",
            " 0.    0.    0.    0.   ]\n",
            "[0.    0.    0.    0.306 0.303 0.277 0.279 0.195 0.27  0.301 0.219 0.232\n",
            " 0.137 0.213 0.279 0.194 0.255 0.18  0.21  0.293 0.312 0.222 0.236 0.311\n",
            " 0.206 0.213 0.203 0.187 0.241 0.187 0.196 0.196 0.191 0.21  0.204 0.198\n",
            " 0.199 0.164 0.233 0.229 0.19  0.194 0.214 0.217 0.217 0.231 0.228 0.178\n",
            " 0.233 0.181 0.236 0.191 0.216 0.169 0.191 0.191 0.214 0.228 0.186 0.137\n",
            " 0.24  0.23  0.171 0.19  0.216 0.254 0.136 0.197 0.19  0.233 0.254 0.227\n",
            " 0.215 0.171 0.224 0.21  0.181 0.202 0.271 0.178 0.213 0.212 0.277 0.195\n",
            " 0.154 0.207 0.236 0.162 0.208 0.171 0.193 0.174 0.17  0.222 0.179 0.194\n",
            " 0.14  0.    0.    0.    0.   ]\n",
            "(100,)\n",
            "<class 'int'>\n",
            "243\n",
            "/content/gdrive/My Drive/thesis/Data/S10/S10_TEX06.txt\n",
            "10\n",
            "[0.644 1.401 2.402 1.468 2.02  0.822 0.789 0.858 2.345 1.689 1.062 0.996\n",
            " 0.822 0.892 1.484 0.548 1.5   0.728 1.009 4.739 0.695 0.773 0.95  0.864\n",
            " 1.106 0.745 0.64  0.824 2.035 0.993 0.362 0.764 1.125 1.43  1.129 0.447\n",
            " 0.773 0.959 1.23  1.022 0.472 0.809 2.983 0.562 1.024 0.959 0.676 0.748\n",
            " 0.893 0.379 1.167 0.803 1.082 0.957]\n",
            "[0.644 1.401 2.402 1.468 2.02  0.822 0.789 0.858 2.345 1.689 1.062 0.996\n",
            " 0.822 0.892 1.484 0.548 1.5   0.728 1.009 0.    0.695 0.773 0.95  0.864\n",
            " 1.106 0.745 0.64  0.824 2.035 0.993 0.362 0.764 1.125 1.43  1.129 0.447\n",
            " 0.773 0.959 1.23  1.022 0.472 0.809 2.983 0.562 1.024 0.959 0.676 0.748\n",
            " 0.893 0.379 1.167 0.803 1.082 0.957]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.644\n",
            " 1.401 2.402 1.468 2.02  0.822 0.789 0.858 2.345 1.689 1.062 0.996 0.822\n",
            " 0.892 1.484 0.548 1.5   0.728 1.009 0.    0.695 0.773 0.95  0.864 1.106\n",
            " 0.745 0.64  0.824 2.035 0.993 0.362 0.764 1.125 1.43  1.129 0.447 0.773\n",
            " 0.959 1.23  1.022 0.472 0.809 2.983 0.562 1.024 0.959 0.676 0.748 0.893\n",
            " 0.379 1.167 0.803 1.082 0.957 0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.   ]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.237 0.215\n",
            " 0.187 0.186 0.205 0.189 0.181 0.19  0.186 0.236 0.176 0.237 0.206 0.19\n",
            " 0.211 0.248 0.24  0.195 0.141 0.203 0.161 0.173 0.206 0.179 0.154 0.162\n",
            " 0.149 0.149 0.18  0.156 0.137 0.199 0.24  0.245 0.187 0.18  0.183 0.15\n",
            " 0.178 0.163 0.188 0.165 0.22  0.188 0.207 0.206 0.177 0.173 0.191 0.154\n",
            " 0.157 0.214 0.223 0.195 0.162 0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.   ]\n",
            "(100,)\n",
            "<class 'int'>\n",
            "244\n",
            "/content/gdrive/My Drive/thesis/Data/S10/S10_TEX05.txt\n",
            "10\n",
            "[1.295 0.831 1.791 0.726 0.666 2.904 1.196 1.428 1.073 0.914 0.891 0.963\n",
            " 0.479 1.06  2.269 0.946 1.076 1.126 0.471 1.185 2.111 0.736 1.033 0.396\n",
            " 0.992 2.908 6.4   1.112 1.38  2.686 1.742 1.048 2.028 1.816 3.261 0.868\n",
            " 1.207 1.256 2.096 0.694 1.35  0.682 1.538 1.233 4.107 1.574 1.106 0.929\n",
            " 1.067 1.433 0.997 1.473 0.678 1.157 1.257 2.175 1.202 1.389 0.58  0.925\n",
            " 0.993 0.703 1.323 0.77  0.59 ]\n",
            "[1.295 0.831 1.791 0.726 0.666 2.904 1.196 1.428 1.073 0.914 0.891 0.963\n",
            " 0.479 1.06  2.269 0.946 1.076 1.126 0.471 1.185 2.111 0.736 1.033 0.396\n",
            " 0.992 2.908 0.    1.112 1.38  2.686 1.742 1.048 2.028 1.816 0.    0.868\n",
            " 1.207 1.256 2.096 0.694 1.35  0.682 1.538 1.233 0.    1.574 1.106 0.929\n",
            " 1.067 1.433 0.997 1.473 0.678 1.157 1.257 2.175 1.202 1.389 0.58  0.925\n",
            " 0.993 0.703 1.323 0.77  0.59 ]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    1.295 0.831 1.791 0.726 0.666 2.904 1.196\n",
            " 1.428 1.073 0.914 0.891 0.963 0.479 1.06  2.269 0.946 1.076 1.126 0.471\n",
            " 1.185 2.111 0.736 1.033 0.396 0.992 2.908 0.    1.112 1.38  2.686 1.742\n",
            " 1.048 2.028 1.816 0.    0.868 1.207 1.256 2.096 0.694 1.35  0.682 1.538\n",
            " 1.233 0.    1.574 1.106 0.929 1.067 1.433 0.997 1.473 0.678 1.157 1.257\n",
            " 2.175 1.202 1.389 0.58  0.925 0.993 0.703 1.323 0.77  0.59  0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.   ]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.313 0.282 0.265 0.186 0.225 0.265 0.195\n",
            " 0.246 0.156 0.263 0.207 0.228 0.146 0.19  0.22  0.219 0.228 0.215 0.198\n",
            " 0.208 0.212 0.244 0.174 0.138 0.182 0.22  0.204 0.177 0.216 0.204 0.25\n",
            " 0.228 0.162 0.278 0.056 0.23  0.191 0.259 0.22  0.276 0.122 0.179 0.203\n",
            " 0.177 0.224 0.221 0.201 0.194 0.227 0.183 0.147 0.254 0.27  0.231 0.247\n",
            " 0.252 0.161 0.299 0.215 0.183 0.182 0.202 0.113 0.171 0.216 0.24  0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.   ]\n",
            "(100,)\n",
            "<class 'int'>\n",
            "245\n",
            "/content/gdrive/My Drive/thesis/Data/S10/S10_TEX08.txt\n",
            "10\n",
            "[0.582 1.605 0.753 1.235 0.758 0.859 0.934 0.684 0.86  0.779 0.9   1.588\n",
            " 0.535 0.774 0.559 0.712 0.78  0.899 0.371 0.857 0.524 0.756 0.465 0.943\n",
            " 2.209 0.81  0.641 1.058 1.054 1.438 0.818 0.999 1.058 1.546 1.161 1.214\n",
            " 1.233 0.542 1.116 1.047 3.408 0.452 0.817 0.84  1.164 1.529 1.045 1.033\n",
            " 0.863 1.021 0.867 0.872 0.864 0.778 1.252 1.563 2.313 2.295 1.757 1.614\n",
            " 1.021 1.539 0.839 0.842 1.312 0.723 1.012 0.912 0.636 0.631]\n",
            "[0.582 1.605 0.753 1.235 0.758 0.859 0.934 0.684 0.86  0.779 0.9   1.588\n",
            " 0.535 0.774 0.559 0.712 0.78  0.899 0.371 0.857 0.524 0.756 0.465 0.943\n",
            " 2.209 0.81  0.641 1.058 1.054 1.438 0.818 0.999 1.058 1.546 1.161 1.214\n",
            " 1.233 0.542 1.116 1.047 0.    0.452 0.817 0.84  1.164 1.529 1.045 1.033\n",
            " 0.863 1.021 0.867 0.872 0.864 0.778 1.252 1.563 2.313 2.295 1.757 1.614\n",
            " 1.021 1.539 0.839 0.842 1.312 0.723 1.012 0.912 0.636 0.631]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.582 1.605 0.753 1.235 0.758 0.859 0.934 0.684 0.86\n",
            " 0.779 0.9   1.588 0.535 0.774 0.559 0.712 0.78  0.899 0.371 0.857 0.524\n",
            " 0.756 0.465 0.943 2.209 0.81  0.641 1.058 1.054 1.438 0.818 0.999 1.058\n",
            " 1.546 1.161 1.214 1.233 0.542 1.116 1.047 0.    0.452 0.817 0.84  1.164\n",
            " 1.529 1.045 1.033 0.863 1.021 0.867 0.872 0.864 0.778 1.252 1.563 2.313\n",
            " 2.295 1.757 1.614 1.021 1.539 0.839 0.842 1.312 0.723 1.012 0.912 0.636\n",
            " 0.631 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.   ]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.215 0.18  0.131 0.2   0.161 0.193 0.198 0.137 0.209 0.196\n",
            " 0.189 0.137 0.186 0.165 0.208 0.154 0.148 0.215 0.13  0.191 0.163 0.156\n",
            " 0.167 0.183 0.261 0.152 0.166 0.197 0.178 0.229 0.187 0.216 0.157 0.195\n",
            " 0.162 0.172 0.197 0.258 0.248 0.194 0.253 0.16  0.191 0.155 0.212 0.179\n",
            " 0.203 0.198 0.147 0.171 0.222 0.177 0.172 0.197 0.258 0.169 0.144 0.302\n",
            " 0.169 0.224 0.163 0.232 0.18  0.181 0.213 0.173 0.159 0.182 0.17  0.171\n",
            " 0.198 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.   ]\n",
            "(100,)\n",
            "<class 'int'>\n",
            "246\n",
            "/content/gdrive/My Drive/thesis/Data/S04/S04_TEX07.txt\n",
            "4\n",
            "[0.354 1.687 0.532 0.591 0.467 1.522 0.662 1.156 0.69  0.516 0.516 0.514\n",
            " 1.723 0.584 0.504 0.691 0.676 0.354 0.475 0.967 2.812 0.511 0.532 2.947\n",
            " 0.727 0.3   0.479 0.511 0.675 0.547 0.517 0.481 0.539 0.783 1.017 0.495\n",
            " 0.691 1.386 0.522 0.478 0.565 0.519 0.504 0.925 3.117 0.339 0.569 0.678\n",
            " 0.574 0.432 0.696 0.483 0.406 0.44  0.511 0.431 3.263 0.78  0.464 0.56\n",
            " 0.745 0.652 0.721 0.641 0.383 0.373 1.634 0.603 0.449 0.904 0.593 0.606\n",
            " 0.357 1.685 0.243 0.216 1.167 1.213 0.423 1.172 3.863 0.669 0.601 0.49\n",
            " 0.692 0.389 0.625 0.571 0.574 0.733 1.015 0.476 0.48  0.699 0.789 0.356\n",
            " 0.451 0.582 0.497 0.483 0.616 0.908 0.489 0.44  0.291 0.636]\n",
            "[0.354 1.687 0.532 0.591 0.467 1.522 0.662 1.156 0.69  0.516 0.516 0.514\n",
            " 1.723 0.584 0.504 0.691 0.676 0.354 0.475 0.967 2.812 0.511 0.532 2.947\n",
            " 0.727 0.3   0.479 0.511 0.675 0.547 0.517 0.481 0.539 0.783 1.017 0.495\n",
            " 0.691 1.386 0.522 0.478 0.565 0.519 0.504 0.925 0.    0.339 0.569 0.678\n",
            " 0.574 0.432 0.696 0.483 0.406 0.44  0.511 0.431 0.    0.78  0.464 0.56\n",
            " 0.745 0.652 0.721 0.641 0.383 0.373 1.634 0.603 0.449 0.904 0.593 0.606\n",
            " 0.357 1.685 0.243 0.216 1.167 1.213 0.423 1.172 0.    0.669 0.601 0.49\n",
            " 0.692 0.389 0.625 0.571 0.574 0.733 1.015 0.476 0.48  0.699 0.789 0.356\n",
            " 0.451 0.582 0.497 0.483 0.616 0.908 0.489 0.44  0.291 0.636]\n",
            "[0.354 1.687 0.532 0.591 0.467 1.522 0.662 1.156 0.69  0.516 0.516 0.514\n",
            " 1.723 0.584 0.504 0.691 0.676 0.354 0.475 0.967 2.812 0.511 0.532 2.947\n",
            " 0.727 0.3   0.479 0.511 0.675 0.547 0.517 0.481 0.539 0.783 1.017 0.495\n",
            " 0.691 1.386 0.522 0.478 0.565 0.519 0.504 0.925 0.    0.339 0.569 0.678\n",
            " 0.574 0.432 0.696 0.483 0.406 0.44  0.511 0.431 0.    0.78  0.464 0.56\n",
            " 0.745 0.652 0.721 0.641 0.383 0.373 1.634 0.603 0.449 0.904 0.593 0.606\n",
            " 0.357 1.685 0.243 0.216 1.167 1.213 0.423 1.172 0.    0.669 0.601 0.49\n",
            " 0.692 0.389 0.625 0.571 0.574 0.733 1.015 0.476 0.48  0.699 0.789 0.356\n",
            " 0.451 0.582 0.497 0.483]\n",
            "[0.137 0.132 0.165 0.133 0.158 0.155 0.138 0.125 0.158 0.15  0.191 0.149\n",
            " 0.192 0.132 0.154 0.132 0.157 0.12  0.157 0.157 0.154 0.136 0.173 0.157\n",
            " 0.211 0.167 0.132 0.152 0.205 0.154 0.182 0.155 0.198 0.158 0.165 0.17\n",
            " 0.207 0.174 0.16  0.178 0.149 0.191 0.189 0.157 0.196 0.146 0.141 0.145\n",
            " 0.147 0.181 0.19  0.135 0.176 0.143 0.16  0.165 0.199 0.163 0.148 0.158\n",
            " 0.163 0.176 0.139 0.199 0.155 0.164 0.131 0.17  0.182 0.166 0.226 0.149\n",
            " 0.182 0.183 0.126 0.116 0.109 0.171 0.19  0.149 0.175 0.145 0.175 0.154\n",
            " 0.174 0.189 0.206 0.172 0.165 0.198 0.157 0.147 0.162 0.189 0.164 0.19\n",
            " 0.142 0.148 0.181 0.167]\n",
            "(100,)\n",
            "<class 'int'>\n",
            "247\n",
            "/content/gdrive/My Drive/thesis/Data/S04/S04_TEX02.txt\n",
            "4\n",
            "[2.487 0.905 1.272 0.678 1.106 0.577 0.489 0.678 0.384 0.297 0.801 2.005\n",
            " 0.637 0.481 0.714 0.551 0.899 0.445 0.556 0.742 0.723 0.602 0.354 0.417\n",
            " 0.469 3.237 0.748 0.684 0.441 0.414 0.455 0.514 0.334 0.456 0.635 0.593\n",
            " 0.543 0.634 0.524 0.548 0.701 0.471 0.585 0.383 0.389 0.408 0.462 0.439]\n",
            "[2.487 0.905 1.272 0.678 1.106 0.577 0.489 0.678 0.384 0.297 0.801 2.005\n",
            " 0.637 0.481 0.714 0.551 0.899 0.445 0.556 0.742 0.723 0.602 0.354 0.417\n",
            " 0.469 0.    0.748 0.684 0.441 0.414 0.455 0.514 0.334 0.456 0.635 0.593\n",
            " 0.543 0.634 0.524 0.548 0.701 0.471 0.585 0.383 0.389 0.408 0.462 0.439]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    2.487 0.905 1.272 0.678 1.106 0.577 0.489 0.678 0.384 0.297\n",
            " 0.801 2.005 0.637 0.481 0.714 0.551 0.899 0.445 0.556 0.742 0.723 0.602\n",
            " 0.354 0.417 0.469 0.    0.748 0.684 0.441 0.414 0.455 0.514 0.334 0.456\n",
            " 0.635 0.593 0.543 0.634 0.524 0.548 0.701 0.471 0.585 0.383 0.389 0.408\n",
            " 0.462 0.439 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.   ]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.19  0.171 0.187 0.146 0.183 0.166 0.145 0.148 0.175 0.14  0.159\n",
            " 0.13  0.145 0.182 0.173 0.142 0.129 0.161 0.133 0.175 0.191 0.165 0.194\n",
            " 0.206 0.156 0.136 0.156 0.165 0.204 0.178 0.155 0.14  0.158 0.14  0.142\n",
            " 0.145 0.135 0.182 0.155 0.171 0.147 0.176 0.163 0.151 0.143 0.144 0.169\n",
            " 0.156 0.124 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.   ]\n",
            "(100,)\n",
            "<class 'int'>\n",
            "248\n",
            "/content/gdrive/My Drive/thesis/Data/S04/S04_TEX09.txt\n",
            "4\n",
            "[0.823 1.48  0.204 0.911 0.636 0.601 1.18  0.247 1.335 0.89  0.702 0.408\n",
            " 0.658 0.548 0.533 0.615 0.748 0.798 0.55  0.516 0.745 0.586 0.398 2.987\n",
            " 1.067 0.666 0.59  0.708 0.606 3.68  0.513 1.139 0.198 0.798 0.648 0.599\n",
            " 0.65  0.597 0.573 0.81  1.159 0.659 0.324 0.642 0.639 0.625 0.984 0.72\n",
            " 1.399 0.488 0.542 2.795 0.364 0.991 0.481 0.791 0.606 1.206 0.54  0.765\n",
            " 1.092 1.039 0.864 0.662 0.617 0.666 0.714 0.433 0.443 0.557 0.454 0.557\n",
            " 1.016 0.449 0.605 0.683]\n",
            "[0.823 1.48  0.204 0.911 0.636 0.601 1.18  0.247 1.335 0.89  0.702 0.408\n",
            " 0.658 0.548 0.533 0.615 0.748 0.798 0.55  0.516 0.745 0.586 0.398 2.987\n",
            " 1.067 0.666 0.59  0.708 0.606 0.    0.513 1.139 0.198 0.798 0.648 0.599\n",
            " 0.65  0.597 0.573 0.81  1.159 0.659 0.324 0.642 0.639 0.625 0.984 0.72\n",
            " 1.399 0.488 0.542 2.795 0.364 0.991 0.481 0.791 0.606 1.206 0.54  0.765\n",
            " 1.092 1.039 0.864 0.662 0.617 0.666 0.714 0.433 0.443 0.557 0.454 0.557\n",
            " 1.016 0.449 0.605 0.683]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.823 1.48  0.204 0.911 0.636 0.601 1.18  0.247 1.335 0.89  0.702 0.408\n",
            " 0.658 0.548 0.533 0.615 0.748 0.798 0.55  0.516 0.745 0.586 0.398 2.987\n",
            " 1.067 0.666 0.59  0.708 0.606 0.    0.513 1.139 0.198 0.798 0.648 0.599\n",
            " 0.65  0.597 0.573 0.81  1.159 0.659 0.324 0.642 0.639 0.625 0.984 0.72\n",
            " 1.399 0.488 0.542 2.795 0.364 0.991 0.481 0.791 0.606 1.206 0.54  0.765\n",
            " 1.092 1.039 0.864 0.662 0.617 0.666 0.714 0.433 0.443 0.557 0.454 0.557\n",
            " 1.016 0.449 0.605 0.683 0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.   ]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.104\n",
            " 0.128 0.12  0.115 0.152 0.14  0.163 0.13  0.132 0.152 0.153 0.143 0.157\n",
            " 0.19  0.165 0.164 0.156 0.182 0.157 0.156 0.172 0.159 0.18  0.147 0.202\n",
            " 0.183 0.166 0.172 0.139 0.189 0.169 0.172 0.129 0.115 0.174 0.174 0.198\n",
            " 0.164 0.174 0.192 0.145 0.16  0.174 0.158 0.173 0.173 0.14  0.186 0.166\n",
            " 0.196 0.199 0.189 0.154 0.157 0.196 0.198 0.172 0.197 0.182 0.19  0.176\n",
            " 0.188 0.22  0.213 0.174 0.182 0.221 0.166 0.182 0.137 0.172 0.19  0.174\n",
            " 0.174 0.173 0.184 0.166 0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.   ]\n",
            "(100,)\n",
            "<class 'int'>\n",
            "249\n",
            "/content/gdrive/My Drive/thesis/Data/S04/S04_TEX08.txt\n",
            "4\n",
            "[0.62  1.688 1.63  1.253 0.633 0.79  0.56  0.512 0.599 0.642 0.631 0.616\n",
            " 0.867 0.908 0.51  1.007 0.64  0.866 0.723 0.921 0.421 0.681 0.673 0.331\n",
            " 0.913 0.694 0.498 1.613 1.387 0.383 0.976 0.539 0.638 0.582 0.55  3.469\n",
            " 0.721 0.518 0.33  0.308 0.341 0.308 0.542 0.641 0.465 3.283 0.264 0.292\n",
            " 1.8   0.47  1.008 0.942 1.289 0.659 0.635 0.648 1.474 0.591 0.736 0.592\n",
            " 0.815 1.171 4.076 0.785 0.33  0.8   0.856 0.581 0.524 0.658 0.957 0.696\n",
            " 0.75  0.685 0.511 0.6   0.68  0.576 0.29  0.707 0.452 2.185 0.415 0.543\n",
            " 0.883 0.672 0.85  1.187 0.356 0.514 1.098 0.511 0.737]\n",
            "[0.62  1.688 1.63  1.253 0.633 0.79  0.56  0.512 0.599 0.642 0.631 0.616\n",
            " 0.867 0.908 0.51  1.007 0.64  0.866 0.723 0.921 0.421 0.681 0.673 0.331\n",
            " 0.913 0.694 0.498 1.613 1.387 0.383 0.976 0.539 0.638 0.582 0.55  0.\n",
            " 0.721 0.518 0.33  0.308 0.341 0.308 0.542 0.641 0.465 0.    0.264 0.292\n",
            " 1.8   0.47  1.008 0.942 1.289 0.659 0.635 0.648 1.474 0.591 0.736 0.592\n",
            " 0.815 1.171 0.    0.785 0.33  0.8   0.856 0.581 0.524 0.658 0.957 0.696\n",
            " 0.75  0.685 0.511 0.6   0.68  0.576 0.29  0.707 0.452 2.185 0.415 0.543\n",
            " 0.883 0.672 0.85  1.187 0.356 0.514 1.098 0.511 0.737]\n",
            "[0.    0.    0.    0.62  1.688 1.63  1.253 0.633 0.79  0.56  0.512 0.599\n",
            " 0.642 0.631 0.616 0.867 0.908 0.51  1.007 0.64  0.866 0.723 0.921 0.421\n",
            " 0.681 0.673 0.331 0.913 0.694 0.498 1.613 1.387 0.383 0.976 0.539 0.638\n",
            " 0.582 0.55  0.    0.721 0.518 0.33  0.308 0.341 0.308 0.542 0.641 0.465\n",
            " 0.    0.264 0.292 1.8   0.47  1.008 0.942 1.289 0.659 0.635 0.648 1.474\n",
            " 0.591 0.736 0.592 0.815 1.171 0.    0.785 0.33  0.8   0.856 0.581 0.524\n",
            " 0.658 0.957 0.696 0.75  0.685 0.511 0.6   0.68  0.576 0.29  0.707 0.452\n",
            " 2.185 0.415 0.543 0.883 0.672 0.85  1.187 0.356 0.514 1.098 0.511 0.737\n",
            " 0.    0.    0.    0.   ]\n",
            "[0.    0.    0.    0.153 0.124 0.314 0.123 0.167 0.149 0.166 0.096 0.108\n",
            " 0.124 0.18  0.156 0.123 0.145 0.153 0.166 0.133 0.166 0.132 0.149 0.159\n",
            " 0.138 0.154 0.164 0.141 0.134 0.171 0.14  0.363 0.19  0.148 0.162 0.155\n",
            " 0.173 0.148 0.131 0.154 0.158 0.155 0.14  0.14  0.141 0.132 0.189 0.153\n",
            " 0.172 0.122 0.124 0.108 0.137 0.166 0.188 0.144 0.153 0.168 0.189 0.14\n",
            " 0.147 0.163 0.166 0.223 0.174 0.15  0.123 0.162 0.132 0.154 0.163 0.165\n",
            " 0.207 0.235 0.147 0.165 0.172 0.145 0.174 0.141 0.158 0.14  0.157 0.218\n",
            " 0.147 0.191 0.167 0.171 0.18  0.19  0.203 0.198 0.175 0.185 0.185 0.171\n",
            " 0.201 0.    0.    0.    0.   ]\n",
            "(100,)\n",
            "<class 'int'>\n",
            "250\n",
            "/content/gdrive/My Drive/thesis/Data/S04/S04_TEX10.txt\n",
            "4\n",
            "[0.749 1.064 1.147 0.205 0.784 0.569 1.368 0.795 0.779 0.632 0.534 0.646\n",
            " 0.758 1.982 0.586 0.399 0.505 1.258 0.232 1.04  0.582 0.869 0.755 0.37\n",
            " 0.442 2.913 1.102 0.682 0.504 0.683 0.649 2.165 0.202 0.862 0.728 0.841\n",
            " 0.647 0.523 1.359 0.654 0.547 0.625 0.483 0.481 0.399 2.98  0.236 0.191\n",
            " 0.374 0.259 0.241 1.183 0.678 0.68  0.659 0.659 0.522 1.037 0.413 0.523\n",
            " 2.265 0.392 1.205 0.451 0.99  0.588 0.626 0.546 0.674 0.448 0.607 0.642]\n",
            "[0.749 1.064 1.147 0.205 0.784 0.569 1.368 0.795 0.779 0.632 0.534 0.646\n",
            " 0.758 1.982 0.586 0.399 0.505 1.258 0.232 1.04  0.582 0.869 0.755 0.37\n",
            " 0.442 2.913 1.102 0.682 0.504 0.683 0.649 2.165 0.202 0.862 0.728 0.841\n",
            " 0.647 0.523 1.359 0.654 0.547 0.625 0.483 0.481 0.399 2.98  0.236 0.191\n",
            " 0.374 0.259 0.241 1.183 0.678 0.68  0.659 0.659 0.522 1.037 0.413 0.523\n",
            " 2.265 0.392 1.205 0.451 0.99  0.588 0.626 0.546 0.674 0.448 0.607 0.642]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.749 1.064 1.147 0.205 0.784 0.569 1.368 0.795 0.779 0.632\n",
            " 0.534 0.646 0.758 1.982 0.586 0.399 0.505 1.258 0.232 1.04  0.582 0.869\n",
            " 0.755 0.37  0.442 2.913 1.102 0.682 0.504 0.683 0.649 2.165 0.202 0.862\n",
            " 0.728 0.841 0.647 0.523 1.359 0.654 0.547 0.625 0.483 0.481 0.399 2.98\n",
            " 0.236 0.191 0.374 0.259 0.241 1.183 0.678 0.68  0.659 0.659 0.522 1.037\n",
            " 0.413 0.523 2.265 0.392 1.205 0.451 0.99  0.588 0.626 0.546 0.674 0.448\n",
            " 0.607 0.642 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.   ]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.178 0.145 0.127 0.129 0.139 0.135 0.174 0.136 0.173 0.157 0.158\n",
            " 0.155 0.142 0.181 0.168 0.174 0.14  0.152 0.124 0.123 0.132 0.189 0.168\n",
            " 0.17  0.174 0.188 0.16  0.155 0.173 0.167 0.199 0.182 0.129 0.141 0.161\n",
            " 0.165 0.156 0.157 0.149 0.161 0.156 0.208 0.149 0.157 0.182 0.14  0.12\n",
            " 0.107 0.299 0.132 0.09  0.132 0.137 0.158 0.16  0.19  0.179 0.173 0.176\n",
            " 0.154 0.164 0.168 0.142 0.158 0.181 0.18  0.157 0.18  0.191 0.174 0.193\n",
            " 0.167 0.224 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.   ]\n",
            "(100,)\n",
            "<class 'int'>\n",
            "251\n",
            "/content/gdrive/My Drive/thesis/Data/S04/S04_TEX04.txt\n",
            "4\n",
            "[0.652 0.748 0.549 0.581 0.389 0.516 0.441 0.476 0.513 0.558 0.565 0.499\n",
            " 0.484 0.347 0.383 0.615 0.397 0.483 0.441 0.474 0.456 0.473 0.417 0.341\n",
            " 0.427 0.503 0.344 0.375 0.641 0.465 0.43  3.344 0.571 0.574 0.622 0.582\n",
            " 0.432 0.56  0.482 0.444 0.636 0.405 0.601 0.489 0.382 0.394 0.43  0.392\n",
            " 0.52  0.367 0.672 0.376 0.684 0.454 1.067 0.636 0.487 0.469 3.248 0.278\n",
            " 0.357 0.268 0.306 0.449 1.019 0.68  0.487 0.459 0.5   1.665 0.48  2.753\n",
            " 0.857 0.388 0.491 0.432 0.505 0.501 0.525 0.572 1.624 0.662 0.443 0.422\n",
            " 0.6   0.507 0.614 0.339 0.584 1.883 0.552 2.284 1.751 0.273 0.854 0.452\n",
            " 0.607 0.49  0.508 0.572 0.276 0.47  0.474 1.528 0.238 0.936 0.509 2.135\n",
            " 2.667]\n",
            "[0.652 0.748 0.549 0.581 0.389 0.516 0.441 0.476 0.513 0.558 0.565 0.499\n",
            " 0.484 0.347 0.383 0.615 0.397 0.483 0.441 0.474 0.456 0.473 0.417 0.341\n",
            " 0.427 0.503 0.344 0.375 0.641 0.465 0.43  0.    0.571 0.574 0.622 0.582\n",
            " 0.432 0.56  0.482 0.444 0.636 0.405 0.601 0.489 0.382 0.394 0.43  0.392\n",
            " 0.52  0.367 0.672 0.376 0.684 0.454 1.067 0.636 0.487 0.469 0.    0.278\n",
            " 0.357 0.268 0.306 0.449 1.019 0.68  0.487 0.459 0.5   1.665 0.48  2.753\n",
            " 0.857 0.388 0.491 0.432 0.505 0.501 0.525 0.572 1.624 0.662 0.443 0.422\n",
            " 0.6   0.507 0.614 0.339 0.584 1.883 0.552 2.284 1.751 0.273 0.854 0.452\n",
            " 0.607 0.49  0.508 0.572 0.276 0.47  0.474 1.528 0.238 0.936 0.509 2.135\n",
            " 2.667]\n",
            "[0.652 0.748 0.549 0.581 0.389 0.516 0.441 0.476 0.513 0.558 0.565 0.499\n",
            " 0.484 0.347 0.383 0.615 0.397 0.483 0.441 0.474 0.456 0.473 0.417 0.341\n",
            " 0.427 0.503 0.344 0.375 0.641 0.465 0.43  0.    0.571 0.574 0.622 0.582\n",
            " 0.432 0.56  0.482 0.444 0.636 0.405 0.601 0.489 0.382 0.394 0.43  0.392\n",
            " 0.52  0.367 0.672 0.376 0.684 0.454 1.067 0.636 0.487 0.469 0.    0.278\n",
            " 0.357 0.268 0.306 0.449 1.019 0.68  0.487 0.459 0.5   1.665 0.48  2.753\n",
            " 0.857 0.388 0.491 0.432 0.505 0.501 0.525 0.572 1.624 0.662 0.443 0.422\n",
            " 0.6   0.507 0.614 0.339 0.584 1.883 0.552 2.284 1.751 0.273 0.854 0.452\n",
            " 0.607 0.49  0.508 0.572]\n",
            "[0.152 0.148 0.149 0.141 0.133 0.134 0.142 0.141 0.122 0.133 0.107 0.149\n",
            " 0.149 0.171 0.115 0.139 0.14  0.125 0.158 0.139 0.174 0.203 0.134 0.175\n",
            " 0.148 0.121 0.156 0.138 0.129 0.137 0.154 0.13  0.129 0.149 0.156 0.164\n",
            " 0.166 0.156 0.137 0.162 0.192 0.154 0.157 0.155 0.18  0.132 0.154 0.175\n",
            " 0.154 0.167 0.115 0.192 0.148 0.154 0.158 0.18  0.151 0.163 0.15  0.235\n",
            " 0.166 0.183 0.148 0.224 0.175 0.16  0.129 0.176 0.192 0.149 0.208 0.172\n",
            " 0.137 0.171 0.149 0.14  0.165 0.142 0.156 0.154 0.156 0.146 0.141 0.137\n",
            " 0.155 0.154 0.155 0.148 0.157 0.139 0.184 0.181 0.186 0.149 0.158 0.143\n",
            " 0.166 0.174 0.142 0.191]\n",
            "(100,)\n",
            "<class 'int'>\n",
            "252\n",
            "/content/gdrive/My Drive/thesis/Data/S04/S04_TEX05.txt\n",
            "4\n",
            "[0.659 0.5   0.532 0.622 0.425 0.608 0.576 0.536 0.441 0.476 0.482 0.533\n",
            " 0.388 0.509 0.912 0.567 0.536 0.611 2.024 4.559 2.411 0.785 0.577 0.593\n",
            " 0.836 0.498 0.711 6.663 0.287 0.208 0.233 0.258 1.201 2.935 0.865 0.697\n",
            " 0.597 0.868 0.779 1.23  1.46  0.594 0.731 0.548 0.635 0.371 0.983 0.85\n",
            " 0.612 3.934 0.858 1.012 0.59  0.657 0.697 0.806 0.591 0.65  0.615 0.407\n",
            " 1.097 0.614 1.122 0.603 0.379 0.535 0.313 0.443 0.932 0.725 0.729 0.371\n",
            " 0.563 0.568 0.517 0.598 4.333 0.396 0.597 0.783 0.35  0.599 0.663 0.775]\n",
            "[0.659 0.5   0.532 0.622 0.425 0.608 0.576 0.536 0.441 0.476 0.482 0.533\n",
            " 0.388 0.509 0.912 0.567 0.536 0.611 2.024 0.    2.411 0.785 0.577 0.593\n",
            " 0.836 0.498 0.711 0.    0.287 0.208 0.233 0.258 1.201 2.935 0.865 0.697\n",
            " 0.597 0.868 0.779 1.23  1.46  0.594 0.731 0.548 0.635 0.371 0.983 0.85\n",
            " 0.612 0.    0.858 1.012 0.59  0.657 0.697 0.806 0.591 0.65  0.615 0.407\n",
            " 1.097 0.614 1.122 0.603 0.379 0.535 0.313 0.443 0.932 0.725 0.729 0.371\n",
            " 0.563 0.568 0.517 0.598 0.    0.396 0.597 0.783 0.35  0.599 0.663 0.775]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.659 0.5   0.532 0.622\n",
            " 0.425 0.608 0.576 0.536 0.441 0.476 0.482 0.533 0.388 0.509 0.912 0.567\n",
            " 0.536 0.611 2.024 0.    2.411 0.785 0.577 0.593 0.836 0.498 0.711 0.\n",
            " 0.287 0.208 0.233 0.258 1.201 2.935 0.865 0.697 0.597 0.868 0.779 1.23\n",
            " 1.46  0.594 0.731 0.548 0.635 0.371 0.983 0.85  0.612 0.    0.858 1.012\n",
            " 0.59  0.657 0.697 0.806 0.591 0.65  0.615 0.407 1.097 0.614 1.122 0.603\n",
            " 0.379 0.535 0.313 0.443 0.932 0.725 0.729 0.371 0.563 0.568 0.517 0.598\n",
            " 0.    0.396 0.597 0.783 0.35  0.599 0.663 0.775 0.    0.    0.    0.\n",
            " 0.    0.    0.    0.   ]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.16  0.175 0.157 0.173 0.158\n",
            " 0.165 0.14  0.128 0.124 0.141 0.148 0.182 0.154 0.115 0.164 0.166 0.132\n",
            " 0.152 0.174 0.161 0.163 0.149 0.155 0.143 0.149 0.144 0.161 0.164 0.144\n",
            " 0.116 0.124 0.099 0.04  0.136 0.154 0.179 0.163 0.163 0.161 0.149 0.157\n",
            " 0.168 0.183 0.158 0.2   0.155 0.15  0.173 0.18  0.164 0.14  0.195 0.172\n",
            " 0.181 0.157 0.174 0.175 0.232 0.172 0.165 0.19  0.165 0.204 0.2   0.161\n",
            " 0.149 0.137 0.174 0.146 0.204 0.212 0.228 0.159 0.169 0.158 0.167 0.181\n",
            " 0.171 0.184 0.16  0.15  0.183 0.165 0.184 0.166 0.    0.    0.    0.\n",
            " 0.    0.    0.    0.   ]\n",
            "(100,)\n",
            "<class 'int'>\n",
            "253\n",
            "/content/gdrive/My Drive/thesis/Data/S04/S04_TEX06.txt\n",
            "4\n",
            "[1.057 0.531 0.542 0.52  2.981 0.469 0.498 0.159 0.472 0.407 0.474 1.021\n",
            " 0.671 0.672 0.391 0.613 0.393 0.723 0.618 0.956 2.603 0.529 0.501 0.604\n",
            " 0.76  0.383 0.651 0.914 1.16  0.565 0.625 0.938 0.745 0.652 0.723 0.708\n",
            " 0.845 0.641 0.642 0.722 0.825 0.614 0.673 2.689 1.127 0.668 0.665 0.615\n",
            " 0.479 0.656 0.914 0.678 0.504 1.022 0.566 0.558 0.742 0.37  0.642 0.614\n",
            " 0.641 2.055 1.346]\n",
            "[1.057 0.531 0.542 0.52  2.981 0.469 0.498 0.159 0.472 0.407 0.474 1.021\n",
            " 0.671 0.672 0.391 0.613 0.393 0.723 0.618 0.956 2.603 0.529 0.501 0.604\n",
            " 0.76  0.383 0.651 0.914 1.16  0.565 0.625 0.938 0.745 0.652 0.723 0.708\n",
            " 0.845 0.641 0.642 0.722 0.825 0.614 0.673 2.689 1.127 0.668 0.665 0.615\n",
            " 0.479 0.656 0.914 0.678 0.504 1.022 0.566 0.558 0.742 0.37  0.642 0.614\n",
            " 0.641 2.055 1.346]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    1.057 0.531 0.542 0.52  2.981 0.469\n",
            " 0.498 0.159 0.472 0.407 0.474 1.021 0.671 0.672 0.391 0.613 0.393 0.723\n",
            " 0.618 0.956 2.603 0.529 0.501 0.604 0.76  0.383 0.651 0.914 1.16  0.565\n",
            " 0.625 0.938 0.745 0.652 0.723 0.708 0.845 0.641 0.642 0.722 0.825 0.614\n",
            " 0.673 2.689 1.127 0.668 0.665 0.615 0.479 0.656 0.914 0.678 0.504 1.022\n",
            " 0.566 0.558 0.742 0.37  0.642 0.614 0.641 2.055 1.346 0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.   ]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.121 0.154 0.13  0.128 0.207 0.127\n",
            " 0.141 0.074 0.132 0.109 0.127 0.086 0.144 0.122 0.149 0.157 0.143 0.108\n",
            " 0.168 0.139 0.196 0.137 0.167 0.171 0.16  0.179 0.138 0.169 0.21  0.164\n",
            " 0.191 0.147 0.182 0.169 0.166 0.174 0.164 0.168 0.15  0.207 0.186 0.172\n",
            " 0.157 0.173 0.11  0.173 0.162 0.162 0.152 0.148 0.191 0.174 0.146 0.173\n",
            " 0.158 0.182 0.163 0.179 0.19  0.148 0.177 0.164 0.178 0.162 0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.   ]\n",
            "(100,)\n",
            "<class 'int'>\n",
            "254\n",
            "/content/gdrive/My Drive/thesis/Data/S04/S04_TEX03.txt\n",
            "4\n",
            "[0.577 0.535 0.94  0.539 0.511 0.665 0.491 0.507 0.55  0.489 0.444 0.64\n",
            " 0.479 0.607 0.34  0.491 0.757 0.639 0.432 1.649 1.253 0.688 0.642 0.636\n",
            " 0.546 0.573 0.908 3.823 0.655 0.489 0.599 0.732 1.054 0.478 0.62  0.573\n",
            " 0.774 0.524 0.39  0.447 0.512 0.37  0.441 0.522]\n",
            "[0.577 0.535 0.94  0.539 0.511 0.665 0.491 0.507 0.55  0.489 0.444 0.64\n",
            " 0.479 0.607 0.34  0.491 0.757 0.639 0.432 1.649 1.253 0.688 0.642 0.636\n",
            " 0.546 0.573 0.908 0.    0.655 0.489 0.599 0.732 1.054 0.478 0.62  0.573\n",
            " 0.774 0.524 0.39  0.447 0.512 0.37  0.441 0.522]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.577 0.535 0.94  0.539 0.511 0.665 0.491 0.507\n",
            " 0.55  0.489 0.444 0.64  0.479 0.607 0.34  0.491 0.757 0.639 0.432 1.649\n",
            " 1.253 0.688 0.642 0.636 0.546 0.573 0.908 0.    0.655 0.489 0.599 0.732\n",
            " 1.054 0.478 0.62  0.573 0.774 0.524 0.39  0.447 0.512 0.37  0.441 0.522\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.   ]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.135 0.115 0.155 0.146 0.155 0.15  0.118 0.157 0.141\n",
            " 0.146 0.149 0.163 0.138 0.132 0.173 0.14  0.148 0.171 0.148 0.157 0.145\n",
            " 0.123 0.167 0.173 0.183 0.188 0.114 0.145 0.163 0.131 0.165 0.149 0.148\n",
            " 0.174 0.178 0.139 0.164 0.14  0.198 0.158 0.167 0.169 0.189 0.22  0.151\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.   ]\n",
            "(100,)\n",
            "<class 'int'>\n",
            "255\n",
            "/content/gdrive/My Drive/thesis/Data/S04/S04_TEX11.txt\n",
            "4\n",
            "[0.841 0.671 0.506 0.481 0.476 0.918 0.695 0.572 0.599 0.575 0.44  0.366\n",
            " 1.515 0.521 1.459 0.221 0.224 2.022 0.798 0.664 1.507 0.726 0.754 0.597\n",
            " 0.349 0.526 1.106 0.206 0.799 0.642 2.841 0.62  0.343 0.974 0.556 0.581\n",
            " 2.357 0.654 0.349 0.906 0.93  0.834 0.505 0.707 0.681 0.977 0.632 2.022\n",
            " 0.424 0.66  0.236 0.843 0.529 0.518 0.621]\n",
            "[0.841 0.671 0.506 0.481 0.476 0.918 0.695 0.572 0.599 0.575 0.44  0.366\n",
            " 1.515 0.521 1.459 0.221 0.224 2.022 0.798 0.664 1.507 0.726 0.754 0.597\n",
            " 0.349 0.526 1.106 0.206 0.799 0.642 2.841 0.62  0.343 0.974 0.556 0.581\n",
            " 2.357 0.654 0.349 0.906 0.93  0.834 0.505 0.707 0.681 0.977 0.632 2.022\n",
            " 0.424 0.66  0.236 0.843 0.529 0.518 0.621]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.841 0.671\n",
            " 0.506 0.481 0.476 0.918 0.695 0.572 0.599 0.575 0.44  0.366 1.515 0.521\n",
            " 1.459 0.221 0.224 2.022 0.798 0.664 1.507 0.726 0.754 0.597 0.349 0.526\n",
            " 1.106 0.206 0.799 0.642 2.841 0.62  0.343 0.974 0.556 0.581 2.357 0.654\n",
            " 0.349 0.906 0.93  0.834 0.505 0.707 0.681 0.977 0.632 2.022 0.424 0.66\n",
            " 0.236 0.843 0.529 0.518 0.621 0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.   ]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.12  0.162\n",
            " 0.113 0.139 0.15  0.123 0.119 0.131 0.134 0.132 0.122 0.182 0.107 0.122\n",
            " 0.124 0.095 0.09  0.092 0.122 0.165 0.158 0.114 0.177 0.139 0.157 0.133\n",
            " 0.13  0.114 0.091 0.158 0.148 0.143 0.173 0.131 0.163 0.138 0.164 0.169\n",
            " 0.19  0.174 0.125 0.141 0.156 0.158 0.143 0.149 0.172 0.203 0.162 0.179\n",
            " 0.126 0.173 0.162 0.159 0.163 0.149 0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.   ]\n",
            "(100,)\n",
            "<class 'int'>\n",
            "256\n",
            "/content/gdrive/My Drive/thesis/Data/S05/S05_TEX07.txt\n",
            "5\n",
            "[3.662 1.463 0.589 0.382 1.191 0.415 0.638 0.583 1.067 0.48  0.75  0.836\n",
            " 0.757 0.623 1.809 0.856 0.653 0.448 0.424 1.726 1.035 0.819 0.839 0.491\n",
            " 1.049 0.586 0.699 0.451 0.571 1.715 0.915 0.789 0.704 0.614 0.416 0.684\n",
            " 2.127 0.9   0.629 0.649 0.682 0.809 0.69  0.537 0.647 0.641 1.623 0.85\n",
            " 0.438 0.224 0.632]\n",
            "[0.    1.463 0.589 0.382 1.191 0.415 0.638 0.583 1.067 0.48  0.75  0.836\n",
            " 0.757 0.623 1.809 0.856 0.653 0.448 0.424 1.726 1.035 0.819 0.839 0.491\n",
            " 1.049 0.586 0.699 0.451 0.571 1.715 0.915 0.789 0.704 0.614 0.416 0.684\n",
            " 2.127 0.9   0.629 0.649 0.682 0.809 0.69  0.537 0.647 0.641 1.623 0.85\n",
            " 0.438 0.224 0.632]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    1.463 0.589 0.382 1.191 0.415 0.638 0.583 1.067 0.48  0.75  0.836\n",
            " 0.757 0.623 1.809 0.856 0.653 0.448 0.424 1.726 1.035 0.819 0.839 0.491\n",
            " 1.049 0.586 0.699 0.451 0.571 1.715 0.915 0.789 0.704 0.614 0.416 0.684\n",
            " 2.127 0.9   0.629 0.649 0.682 0.809 0.69  0.537 0.647 0.641 1.623 0.85\n",
            " 0.438 0.224 0.632 0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.   ]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.051 0.189 0.081 0.099 0.065 0.072 0.097 0.075 0.065 0.088 0.089 0.062\n",
            " 0.075 0.066 0.058 0.103 0.053 0.065 0.074 0.075 0.027 0.074 0.061 0.087\n",
            " 0.087 0.059 0.056 0.048 0.079 0.064 0.069 0.087 0.071 0.082 0.082 0.065\n",
            " 0.062 0.106 0.047 0.049 0.066 0.057 0.096 0.078 0.131 0.11  0.083 0.081\n",
            " 0.071 0.074 0.149 0.074 0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.   ]\n",
            "(100,)\n",
            "<class 'int'>\n",
            "257\n",
            "/content/gdrive/My Drive/thesis/Data/S05/S05_TEX04.txt\n",
            "5\n",
            "[0.407 0.883 0.447 1.124 1.162 0.525 0.516 2.379 1.104 0.731 1.549 0.986\n",
            " 0.792 0.817 0.782 0.32  0.852 0.487 0.657 0.59  0.631 0.558 2.221 1.596\n",
            " 0.705 0.631 0.807 0.55  0.582 0.735 0.222 0.368 2.578 0.811 1.141 0.29\n",
            " 0.423 0.65  0.557 0.801 0.238 0.133 0.735 0.604 0.398 0.734 0.614 0.806\n",
            " 1.059 0.68  0.341 1.605 0.657 0.585 0.612]\n",
            "[0.407 0.883 0.447 1.124 1.162 0.525 0.516 2.379 1.104 0.731 1.549 0.986\n",
            " 0.792 0.817 0.782 0.32  0.852 0.487 0.657 0.59  0.631 0.558 2.221 1.596\n",
            " 0.705 0.631 0.807 0.55  0.582 0.735 0.222 0.368 2.578 0.811 1.141 0.29\n",
            " 0.423 0.65  0.557 0.801 0.238 0.133 0.735 0.604 0.398 0.734 0.614 0.806\n",
            " 1.059 0.68  0.341 1.605 0.657 0.585 0.612]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.407 0.883\n",
            " 0.447 1.124 1.162 0.525 0.516 2.379 1.104 0.731 1.549 0.986 0.792 0.817\n",
            " 0.782 0.32  0.852 0.487 0.657 0.59  0.631 0.558 2.221 1.596 0.705 0.631\n",
            " 0.807 0.55  0.582 0.735 0.222 0.368 2.578 0.811 1.141 0.29  0.423 0.65\n",
            " 0.557 0.801 0.238 0.133 0.735 0.604 0.398 0.734 0.614 0.806 1.059 0.68\n",
            " 0.341 1.605 0.657 0.585 0.612 0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.   ]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.114 0.057\n",
            " 0.071 0.064 0.08  0.058 0.064 0.089 0.096 0.073 0.058 0.087 0.1   0.089\n",
            " 0.104 0.086 0.075 0.07  0.049 0.082 0.082 0.083 0.081 0.063 0.047 0.099\n",
            " 0.033 0.067 0.082 0.098 0.079 0.082 0.061 0.077 0.057 0.073 0.074 0.065\n",
            " 0.081 0.081 0.078 0.025 0.015 0.086 0.04  0.084 0.073 0.067 0.066 0.113\n",
            " 0.082 0.057 0.074 0.064 0.061 0.065 0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.   ]\n",
            "(100,)\n",
            "<class 'int'>\n",
            "258\n",
            "/content/gdrive/My Drive/thesis/Data/S05/S05_TEX02.txt\n",
            "5\n",
            "[0.913 0.818 2.335 0.495 1.066 0.481 1.176 1.488 0.593 0.654 3.953 1.766\n",
            " 0.861 2.094 0.392 1.891 0.855 0.612 0.574 2.295 1.155 0.64  0.967 0.581\n",
            " 0.714 1.889 0.856 0.867 2.236 1.012 0.723 0.689 1.024 0.624 0.691 0.756\n",
            " 0.524 0.805 1.196 0.782 1.311 0.602 1.011 0.487 0.324 1.166]\n",
            "[0.913 0.818 2.335 0.495 1.066 0.481 1.176 1.488 0.593 0.654 0.    1.766\n",
            " 0.861 2.094 0.392 1.891 0.855 0.612 0.574 2.295 1.155 0.64  0.967 0.581\n",
            " 0.714 1.889 0.856 0.867 2.236 1.012 0.723 0.689 1.024 0.624 0.691 0.756\n",
            " 0.524 0.805 1.196 0.782 1.311 0.602 1.011 0.487 0.324 1.166]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.913 0.818 2.335 0.495 1.066 0.481 1.176 1.488 0.593\n",
            " 0.654 0.    1.766 0.861 2.094 0.392 1.891 0.855 0.612 0.574 2.295 1.155\n",
            " 0.64  0.967 0.581 0.714 1.889 0.856 0.867 2.236 1.012 0.723 0.689 1.024\n",
            " 0.624 0.691 0.756 0.524 0.805 1.196 0.782 1.311 0.602 1.011 0.487 0.324\n",
            " 1.166 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.   ]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.095 0.032 0.067 0.085 0.082 0.055 0.064 0.035 0.078 0.066\n",
            " 0.076 0.073 0.035 0.073 0.033 0.073 0.078 0.104 0.09  0.081 0.098 0.065\n",
            " 0.099 0.129 0.106 0.057 0.072 0.089 0.096 0.088 0.116 0.099 0.091 0.108\n",
            " 0.081 0.08  0.074 0.073 0.049 0.06  0.126 0.061 0.091 0.095 0.082 0.074\n",
            " 0.079 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.   ]\n",
            "(100,)\n",
            "<class 'int'>\n",
            "259\n",
            "/content/gdrive/My Drive/thesis/Data/S05/S05_TEX06.txt\n",
            "5\n",
            "[1.714 1.603 0.404 1.492 0.43  1.08  0.623 0.585 0.688 0.715 0.886 0.768\n",
            " 0.836 1.009 0.771 0.578 0.424 0.507 0.681 0.742 0.833 0.414 0.706 0.34\n",
            " 0.606 2.797 0.83  0.621 1.01  0.902 0.835 0.725 0.831 1.119 0.985 0.62\n",
            " 0.918 0.774 0.321 0.698 2.196 0.837 0.441 1.508 0.958 1.28  0.94  0.825\n",
            " 1.109 0.951 0.614 1.071 0.723 0.698 0.583 0.768 0.537 3.194 0.763 0.843\n",
            " 0.849 0.756 0.603 0.524 1.199 0.654 0.659 0.466 0.776 0.694 0.482 0.666]\n",
            "[1.714 1.603 0.404 1.492 0.43  1.08  0.623 0.585 0.688 0.715 0.886 0.768\n",
            " 0.836 1.009 0.771 0.578 0.424 0.507 0.681 0.742 0.833 0.414 0.706 0.34\n",
            " 0.606 2.797 0.83  0.621 1.01  0.902 0.835 0.725 0.831 1.119 0.985 0.62\n",
            " 0.918 0.774 0.321 0.698 2.196 0.837 0.441 1.508 0.958 1.28  0.94  0.825\n",
            " 1.109 0.951 0.614 1.071 0.723 0.698 0.583 0.768 0.537 0.    0.763 0.843\n",
            " 0.849 0.756 0.603 0.524 1.199 0.654 0.659 0.466 0.776 0.694 0.482 0.666]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    1.714 1.603 0.404 1.492 0.43  1.08  0.623 0.585 0.688 0.715\n",
            " 0.886 0.768 0.836 1.009 0.771 0.578 0.424 0.507 0.681 0.742 0.833 0.414\n",
            " 0.706 0.34  0.606 2.797 0.83  0.621 1.01  0.902 0.835 0.725 0.831 1.119\n",
            " 0.985 0.62  0.918 0.774 0.321 0.698 2.196 0.837 0.441 1.508 0.958 1.28\n",
            " 0.94  0.825 1.109 0.951 0.614 1.071 0.723 0.698 0.583 0.768 0.537 0.\n",
            " 0.763 0.843 0.849 0.756 0.603 0.524 1.199 0.654 0.659 0.466 0.776 0.694\n",
            " 0.482 0.666 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.   ]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.129 0.153 0.096 0.092 0.062 0.09  0.082 0.092 0.063 0.074 0.082\n",
            " 0.06  0.082 0.095 0.067 0.086 0.056 0.098 0.065 0.066 0.105 0.062 0.088\n",
            " 0.073 0.058 0.108 0.105 0.048 0.057 0.13  0.074 0.072 0.052 0.071 0.09\n",
            " 0.079 0.073 0.071 0.039 0.073 0.073 0.139 0.108 0.076 0.071 0.095 0.137\n",
            " 0.086 0.043 0.082 0.071 0.104 0.082 0.114 0.083 0.106 0.095 0.056 0.037\n",
            " 0.083 0.038 0.078 0.079 0.065 0.072 0.063 0.098 0.064 0.064 0.079 0.073\n",
            " 0.083 0.073 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.   ]\n",
            "(100,)\n",
            "<class 'int'>\n",
            "260\n",
            "/content/gdrive/My Drive/thesis/Data/S05/S05_TEX03.txt\n",
            "5\n",
            "[0.978 0.63  0.662 0.553 0.564 1.694 0.806 0.643 0.918 0.757 0.314 0.647\n",
            " 0.383 0.657 0.76  0.57  1.572 0.76  0.371 0.614 0.607 0.35  0.723 0.284\n",
            " 1.791 1.273 2.622 0.698 0.479 0.62  0.865 0.808 0.799 0.545 0.591 0.466\n",
            " 0.672 1.138 0.584 0.352 0.577 1.852 1.072 2.692 1.248 0.087 0.869 0.603\n",
            " 0.768 4.815 0.695 0.499 0.843 0.349 0.61  0.658 0.843 0.815 0.704 0.389\n",
            " 1.127 0.345 0.234 0.732 0.457 0.4  ]\n",
            "[0.978 0.63  0.662 0.553 0.564 1.694 0.806 0.643 0.918 0.757 0.314 0.647\n",
            " 0.383 0.657 0.76  0.57  1.572 0.76  0.371 0.614 0.607 0.35  0.723 0.284\n",
            " 1.791 1.273 2.622 0.698 0.479 0.62  0.865 0.808 0.799 0.545 0.591 0.466\n",
            " 0.672 1.138 0.584 0.352 0.577 1.852 1.072 2.692 1.248 0.087 0.869 0.603\n",
            " 0.768 0.    0.695 0.499 0.843 0.349 0.61  0.658 0.843 0.815 0.704 0.389\n",
            " 1.127 0.345 0.234 0.732 0.457 0.4  ]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.978 0.63  0.662 0.553 0.564 1.694 0.806\n",
            " 0.643 0.918 0.757 0.314 0.647 0.383 0.657 0.76  0.57  1.572 0.76  0.371\n",
            " 0.614 0.607 0.35  0.723 0.284 1.791 1.273 2.622 0.698 0.479 0.62  0.865\n",
            " 0.808 0.799 0.545 0.591 0.466 0.672 1.138 0.584 0.352 0.577 1.852 1.072\n",
            " 2.692 1.248 0.087 0.869 0.603 0.768 0.    0.695 0.499 0.843 0.349 0.61\n",
            " 0.658 0.843 0.815 0.704 0.389 1.127 0.345 0.234 0.732 0.457 0.4   0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.   ]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.104 0.09  0.082 0.086 0.082 0.083 0.093 0.093\n",
            " 0.093 0.08  0.096 0.081 0.083 0.082 0.039 0.112 0.09  0.09  0.086 0.065\n",
            " 0.083 0.116 0.082 0.091 0.08  0.093 0.11  0.116 0.124 0.069 0.034 0.082\n",
            " 0.087 0.078 0.033 0.115 0.139 0.116 0.067 0.064 0.045 0.059 0.085 0.07\n",
            " 0.078 0.054 0.056 0.045 0.066 0.07  0.087 0.065 0.082 0.088 0.069 0.084\n",
            " 0.056 0.062 0.071 0.098 0.109 0.105 0.1   0.124 0.056 0.048 0.072 0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.   ]\n",
            "(100,)\n",
            "<class 'int'>\n",
            "261\n",
            "/content/gdrive/My Drive/thesis/Data/S05/S05_TEX08.txt\n",
            "5\n",
            "[1.561 3.681 0.583 0.64  0.707 0.589 1.116 0.723 1.214 0.808 0.724 0.698\n",
            " 1.204 1.114 0.414 0.832 0.466 0.858 0.831 0.574 0.531 1.165 1.863 1.065\n",
            " 0.603 0.45  0.867 0.572 1.54  1.405 5.711 0.759 0.678 0.591 0.724 0.834\n",
            " 0.298 0.696 1.36  0.469 0.533 0.722 0.786 0.731 0.757 0.431 0.123 0.946\n",
            " 0.802 0.313 0.74  0.599 0.282 0.615 0.885 0.647 0.539]\n",
            "[1.561 0.    0.583 0.64  0.707 0.589 1.116 0.723 1.214 0.808 0.724 0.698\n",
            " 1.204 1.114 0.414 0.832 0.466 0.858 0.831 0.574 0.531 1.165 1.863 1.065\n",
            " 0.603 0.45  0.867 0.572 1.54  1.405 0.    0.759 0.678 0.591 0.724 0.834\n",
            " 0.298 0.696 1.36  0.469 0.533 0.722 0.786 0.731 0.757 0.431 0.123 0.946\n",
            " 0.802 0.313 0.74  0.599 0.282 0.615 0.885 0.647 0.539]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    1.561 0.    0.583\n",
            " 0.64  0.707 0.589 1.116 0.723 1.214 0.808 0.724 0.698 1.204 1.114 0.414\n",
            " 0.832 0.466 0.858 0.831 0.574 0.531 1.165 1.863 1.065 0.603 0.45  0.867\n",
            " 0.572 1.54  1.405 0.    0.759 0.678 0.591 0.724 0.834 0.298 0.696 1.36\n",
            " 0.469 0.533 0.722 0.786 0.731 0.757 0.431 0.123 0.946 0.802 0.313 0.74\n",
            " 0.599 0.282 0.615 0.885 0.647 0.539 0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.   ]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.12  0.116 0.058\n",
            " 0.057 0.082 0.073 0.057 0.082 0.09  0.074 0.08  0.046 0.071 0.106 0.098\n",
            " 0.082 0.073 0.074 0.039 0.073 0.054 0.106 0.072 0.054 0.071 0.074 0.065\n",
            " 0.08  0.097 0.087 0.07  0.097 0.088 0.1   0.056 0.073 0.07  0.079 0.124\n",
            " 0.06  0.066 0.056 0.099 0.07  0.07  0.087 0.023 0.09  0.065 0.039 0.082\n",
            " 0.073 0.098 0.065 0.073 0.072 0.089 0.056 0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.   ]\n",
            "(100,)\n",
            "<class 'int'>\n",
            "262\n",
            "/content/gdrive/My Drive/thesis/Data/S05/S05_TEX10.txt\n",
            "5\n",
            "[0.561 0.54  0.825 0.908 0.966 0.32  0.573 1.493 1.005 0.545 0.908 0.483\n",
            " 1.273 0.789 1.579 0.724 0.686 0.886 2.221 0.518 0.533 2.466 0.679 0.809\n",
            " 0.661 0.641 0.589 0.474 0.527 0.689 1.457 0.806 0.742 1.072 0.932 2.177\n",
            " 1.077 0.565 0.515 0.683 0.606 0.717 1.542 0.688 0.729 0.207 0.374 0.683\n",
            " 0.582 0.349 0.649 0.899 0.464 0.482 0.505]\n",
            "[0.561 0.54  0.825 0.908 0.966 0.32  0.573 1.493 1.005 0.545 0.908 0.483\n",
            " 1.273 0.789 1.579 0.724 0.686 0.886 2.221 0.518 0.533 2.466 0.679 0.809\n",
            " 0.661 0.641 0.589 0.474 0.527 0.689 1.457 0.806 0.742 1.072 0.932 2.177\n",
            " 1.077 0.565 0.515 0.683 0.606 0.717 1.542 0.688 0.729 0.207 0.374 0.683\n",
            " 0.582 0.349 0.649 0.899 0.464 0.482 0.505]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.561 0.54\n",
            " 0.825 0.908 0.966 0.32  0.573 1.493 1.005 0.545 0.908 0.483 1.273 0.789\n",
            " 1.579 0.724 0.686 0.886 2.221 0.518 0.533 2.466 0.679 0.809 0.661 0.641\n",
            " 0.589 0.474 0.527 0.689 1.457 0.806 0.742 1.072 0.932 2.177 1.077 0.565\n",
            " 0.515 0.683 0.606 0.717 1.542 0.688 0.729 0.207 0.374 0.683 0.582 0.349\n",
            " 0.649 0.899 0.464 0.482 0.505 0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.   ]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.061 0.05\n",
            " 0.042 0.072 0.088 0.052 0.082 0.082 0.068 0.069 0.032 0.072 0.062 0.071\n",
            " 0.046 0.054 0.105 0.084 0.069 0.086 0.049 0.066 0.077 0.071 0.087 0.065\n",
            " 0.074 0.075 0.074 0.037 0.081 0.087 0.062 0.06  0.078 0.044 0.045 0.108\n",
            " 0.032 0.074 0.073 0.09  0.041 0.059 0.078 0.074 0.066 0.065 0.072 0.072\n",
            " 0.065 0.064 0.081 0.066 0.048 0.075 0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.   ]\n",
            "(100,)\n",
            "<class 'int'>\n",
            "263\n",
            "/content/gdrive/My Drive/thesis/Data/S05/S05_TEX05.txt\n",
            "5\n",
            "[0.437 0.597 0.951 0.904 0.408 0.975 0.555 0.785 0.482 0.789 0.647 0.738\n",
            " 0.585 0.999 0.539 0.85  0.54  0.422 0.392 0.789 2.039 1.48  0.648 0.665\n",
            " 0.566 0.674 0.54  0.632 0.74  0.894 0.602 0.707 1.925 1.013 1.086 0.408\n",
            " 0.759 0.611 0.575 0.757 0.842 1.726 0.918 0.766 0.653 1.601 0.436 0.332\n",
            " 0.558 0.477 2.47  1.497 0.627 0.795 0.703 0.849 0.722 0.64  0.867 1.661\n",
            " 0.842 0.673 0.371 0.902 0.67  0.64  0.791 2.188 1.357 0.807 1.036 0.725\n",
            " 0.804 0.623 0.457 0.851 0.629 0.901 0.697 2.651 0.926 0.554 0.664 0.632\n",
            " 1.484 0.395 0.631 0.837 0.973 0.871 0.123 0.631 0.257 0.4   0.258]\n",
            "[0.437 0.597 0.951 0.904 0.408 0.975 0.555 0.785 0.482 0.789 0.647 0.738\n",
            " 0.585 0.999 0.539 0.85  0.54  0.422 0.392 0.789 2.039 1.48  0.648 0.665\n",
            " 0.566 0.674 0.54  0.632 0.74  0.894 0.602 0.707 1.925 1.013 1.086 0.408\n",
            " 0.759 0.611 0.575 0.757 0.842 1.726 0.918 0.766 0.653 1.601 0.436 0.332\n",
            " 0.558 0.477 2.47  1.497 0.627 0.795 0.703 0.849 0.722 0.64  0.867 1.661\n",
            " 0.842 0.673 0.371 0.902 0.67  0.64  0.791 2.188 1.357 0.807 1.036 0.725\n",
            " 0.804 0.623 0.457 0.851 0.629 0.901 0.697 2.651 0.926 0.554 0.664 0.632\n",
            " 1.484 0.395 0.631 0.837 0.973 0.871 0.123 0.631 0.257 0.4   0.258]\n",
            "[0.    0.    0.437 0.597 0.951 0.904 0.408 0.975 0.555 0.785 0.482 0.789\n",
            " 0.647 0.738 0.585 0.999 0.539 0.85  0.54  0.422 0.392 0.789 2.039 1.48\n",
            " 0.648 0.665 0.566 0.674 0.54  0.632 0.74  0.894 0.602 0.707 1.925 1.013\n",
            " 1.086 0.408 0.759 0.611 0.575 0.757 0.842 1.726 0.918 0.766 0.653 1.601\n",
            " 0.436 0.332 0.558 0.477 2.47  1.497 0.627 0.795 0.703 0.849 0.722 0.64\n",
            " 0.867 1.661 0.842 0.673 0.371 0.902 0.67  0.64  0.791 2.188 1.357 0.807\n",
            " 1.036 0.725 0.804 0.623 0.457 0.851 0.629 0.901 0.697 2.651 0.926 0.554\n",
            " 0.664 0.632 1.484 0.395 0.631 0.837 0.973 0.871 0.123 0.631 0.257 0.4\n",
            " 0.258 0.    0.    0.   ]\n",
            "[0.    0.    0.094 0.064 0.065 0.089 0.074 0.081 0.046 0.065 0.07  0.055\n",
            " 0.088 0.073 0.067 0.063 0.064 0.074 0.087 0.071 0.083 0.074 0.108 0.071\n",
            " 0.081 0.089 0.081 0.081 0.098 0.074 0.082 0.09  0.061 0.125 0.083 0.052\n",
            " 0.038 0.091 0.057 0.062 0.076 0.098 0.098 0.104 0.084 0.089 0.069 0.091\n",
            " 0.086 0.082 0.058 0.091 0.087 0.044 0.086 0.073 0.069 0.08  0.105 0.091\n",
            " 0.089 0.054 0.066 0.096 0.096 0.116 0.079 0.082 0.133 0.098 0.071 0.087\n",
            " 0.119 0.048 0.122 0.141 0.083 0.075 0.07  0.066 0.07  0.08  0.091 0.095\n",
            " 0.098 0.083 0.091 0.045 0.1   0.101 0.086 0.078 0.061 0.048 0.09  0.074\n",
            " 0.082 0.073 0.    0.    0.   ]\n",
            "(100,)\n",
            "<class 'int'>\n",
            "264\n",
            "/content/gdrive/My Drive/thesis/Data/S05/S05_TEX09.txt\n",
            "5\n",
            "[2.341 3.018 0.355 0.375 0.738 1.785 0.94  0.897 0.594 0.951 0.613 0.357\n",
            " 0.325 0.682 0.982 0.755 1.121 0.567 0.649 0.516 1.305 2.089 0.521 0.442\n",
            " 0.869 0.738 1.79  0.822 0.387 0.606 1.295 0.83  0.47  1.057 0.457 0.893\n",
            " 0.678 0.649 0.707 0.825 1.605 0.809 0.653 0.633 0.817 0.681 0.564 0.666]\n",
            "[2.341 0.    0.355 0.375 0.738 1.785 0.94  0.897 0.594 0.951 0.613 0.357\n",
            " 0.325 0.682 0.982 0.755 1.121 0.567 0.649 0.516 1.305 2.089 0.521 0.442\n",
            " 0.869 0.738 1.79  0.822 0.387 0.606 1.295 0.83  0.47  1.057 0.457 0.893\n",
            " 0.678 0.649 0.707 0.825 1.605 0.809 0.653 0.633 0.817 0.681 0.564 0.666]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    2.341 0.    0.355 0.375 0.738 1.785 0.94  0.897 0.594 0.951\n",
            " 0.613 0.357 0.325 0.682 0.982 0.755 1.121 0.567 0.649 0.516 1.305 2.089\n",
            " 0.521 0.442 0.869 0.738 1.79  0.822 0.387 0.606 1.295 0.83  0.47  1.057\n",
            " 0.457 0.893 0.678 0.649 0.707 0.825 1.605 0.809 0.653 0.633 0.817 0.681\n",
            " 0.564 0.666 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.   ]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.111 0.157 0.055 0.007 0.065 0.082 0.061 0.085 0.078 0.049 0.08\n",
            " 0.09  0.075 0.057 0.131 0.106 0.083 0.109 0.048 0.073 0.071 0.072 0.087\n",
            " 0.057 0.064 0.069 0.072 0.06  0.104 0.058 0.051 0.084 0.054 0.025 0.066\n",
            " 0.073 0.062 0.084 0.091 0.149 0.106 0.071 0.079 0.048 0.072 0.061 0.071\n",
            " 0.082 0.072 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.   ]\n",
            "(100,)\n",
            "<class 'int'>\n",
            "265\n",
            "/content/gdrive/My Drive/thesis/Data/S06/S06_TEX03.txt\n",
            "6\n",
            "[0.787 0.867 1.005 0.78  1.05  5.903 1.252 1.99  1.322 1.868 1.237 0.206\n",
            " 4.283 2.29  0.412 1.514 1.088 0.884 0.531 0.772 1.109 0.662 0.908 0.58\n",
            " 0.859 0.922 0.886 0.226 7.154 0.285 0.208 0.191 0.208 0.216 0.192 0.466\n",
            " 0.324 2.348 1.255 0.57  1.464 2.387 0.882 0.245 4.176 0.495 2.83  1.226\n",
            " 1.67  1.046 0.935 0.761 0.316 5.372 0.971 4.48  1.234 0.79  0.883 1.012\n",
            " 0.564 6.752 0.388 0.5   0.49  1.765 1.642 0.693 0.837 1.098 0.472 6.328\n",
            " 0.538 0.816 1.731]\n",
            "[0.787 0.867 1.005 0.78  1.05  0.    1.252 1.99  1.322 1.868 1.237 0.206\n",
            " 0.    2.29  0.412 1.514 1.088 0.884 0.531 0.772 1.109 0.662 0.908 0.58\n",
            " 0.859 0.922 0.886 0.226 0.    0.285 0.208 0.191 0.208 0.216 0.192 0.466\n",
            " 0.324 2.348 1.255 0.57  1.464 2.387 0.882 0.245 0.    0.495 2.83  1.226\n",
            " 1.67  1.046 0.935 0.761 0.316 0.    0.971 0.    1.234 0.79  0.883 1.012\n",
            " 0.564 0.    0.388 0.5   0.49  1.765 1.642 0.693 0.837 1.098 0.472 0.\n",
            " 0.538 0.816 1.731]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.787 0.867 1.005 0.78  1.05  0.    1.252 1.99  1.322 1.868 1.237 0.206\n",
            " 0.    2.29  0.412 1.514 1.088 0.884 0.531 0.772 1.109 0.662 0.908 0.58\n",
            " 0.859 0.922 0.886 0.226 0.    0.285 0.208 0.191 0.208 0.216 0.192 0.466\n",
            " 0.324 2.348 1.255 0.57  1.464 2.387 0.882 0.245 0.    0.495 2.83  1.226\n",
            " 1.67  1.046 0.935 0.761 0.316 0.    0.971 0.    1.234 0.79  0.883 1.012\n",
            " 0.564 0.    0.388 0.5   0.49  1.765 1.642 0.693 0.837 1.098 0.472 0.\n",
            " 0.538 0.816 1.731 0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.   ]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.196 0.207 0.246 0.189 0.182 0.104 0.245 0.307 0.17  0.177 0.155 0.149\n",
            " 0.175 0.215 0.145 0.165 0.247 0.082 0.13  0.148 0.165 0.087 0.107 0.18\n",
            " 0.148 0.187 0.262 0.018 0.164 0.134 0.132 0.115 0.091 0.083 0.049 0.057\n",
            " 0.023 0.065 0.103 0.103 0.115 0.114 0.112 0.128 0.357 0.228 0.173 0.136\n",
            " 0.14  0.156 0.124 0.178 0.132 0.073 0.36  0.461 0.178 0.157 0.156 0.146\n",
            " 0.057 0.066 0.062 0.124 0.172 0.098 0.062 0.115 0.171 0.139 0.196 0.132\n",
            " 0.212 0.148 0.13  0.144 0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.   ]\n",
            "(100,)\n",
            "<class 'int'>\n",
            "266\n",
            "/content/gdrive/My Drive/thesis/Data/S06/S06_TEX02.txt\n",
            "6\n",
            "[ 1.012  2.204  4.644  0.64   0.608  1.207  1.439  1.537  0.557  1.237\n",
            "  0.603  0.577  0.323  0.258  5.338  0.74   0.191  2.335  5.1    1.218\n",
            "  2.556  5.53   0.998  1.493  0.665  1.729  0.523  3.5    1.963  6.92\n",
            "  0.546  0.476  0.791  0.524  0.704 10.389  4.892  1.592  0.769  0.732\n",
            "  0.976  0.639  2.853  1.752  0.222  8.47   0.66   1.958  1.654  0.478\n",
            "  0.628  1.769  2.861  1.462  2.238  4.637  1.497  7.789  0.932  2.261\n",
            "  0.802  0.992  1.246  0.933  1.322  0.989  4.88   2.729  0.751  1.323\n",
            "  4.065  0.496  3.026  0.256  2.148  1.526  1.11   0.83   1.246  1.321\n",
            "  1.204  0.681  0.675  1.791  0.428  5.424  2.221  1.2    0.692  0.624\n",
            "  0.557  0.541  1.148  1.02   0.651  0.974  0.437  5.857  0.494  3.163\n",
            "  1.629  2.079  5.059  0.731  1.325  1.003  0.665  1.298  0.482  0.738\n",
            "  0.443  0.498  0.715  1.524  1.02   1.196  0.532  4.56   2.547  2.247\n",
            "  4.019  0.937  1.656  0.929  1.971  0.869  1.094  0.963  1.152  0.994\n",
            "  5.944  1.372 14.021  1.558  1.072  0.77   1.247  0.698  0.799  0.373\n",
            "  0.491  2.218  0.797  0.375  0.674  0.49   4.055  2.344  5.962  0.519\n",
            "  0.65   0.838  0.851  0.529  1.084  1.115  1.355  1.255  0.773  0.727\n",
            "  4.219  0.67   0.589  2.749  0.811  1.83   0.764  0.908  0.648  0.763\n",
            "  0.607  5.478  0.357  0.483  0.441  0.483  0.565  0.523  1.915  1.423\n",
            "  2.257  0.706  1.135  1.219  0.672  3.301  0.698  1.073  2.389  1.867\n",
            "  0.699  0.64   1.637  0.475  1.105  4.453 18.848]\n",
            "[1.012 2.204 0.    0.64  0.608 1.207 1.439 1.537 0.557 1.237 0.603 0.577\n",
            " 0.323 0.258 0.    0.74  0.191 2.335 0.    1.218 2.556 0.    0.998 1.493\n",
            " 0.665 1.729 0.523 0.    1.963 0.    0.546 0.476 0.791 0.524 0.704 0.\n",
            " 0.    1.592 0.769 0.732 0.976 0.639 2.853 1.752 0.222 0.    0.66  1.958\n",
            " 1.654 0.478 0.628 1.769 2.861 1.462 2.238 0.    1.497 0.    0.932 2.261\n",
            " 0.802 0.992 1.246 0.933 1.322 0.989 0.    2.729 0.751 1.323 0.    0.496\n",
            " 0.    0.256 2.148 1.526 1.11  0.83  1.246 1.321 1.204 0.681 0.675 1.791\n",
            " 0.428 0.    2.221 1.2   0.692 0.624 0.557 0.541 1.148 1.02  0.651 0.974\n",
            " 0.437 0.    0.494 0.    1.629 2.079 0.    0.731 1.325 1.003 0.665 1.298\n",
            " 0.482 0.738 0.443 0.498 0.715 1.524 1.02  1.196 0.532 0.    2.547 2.247\n",
            " 0.    0.937 1.656 0.929 1.971 0.869 1.094 0.963 1.152 0.994 0.    1.372\n",
            " 0.    1.558 1.072 0.77  1.247 0.698 0.799 0.373 0.491 2.218 0.797 0.375\n",
            " 0.674 0.49  0.    2.344 0.    0.519 0.65  0.838 0.851 0.529 1.084 1.115\n",
            " 1.355 1.255 0.773 0.727 0.    0.67  0.589 2.749 0.811 1.83  0.764 0.908\n",
            " 0.648 0.763 0.607 0.    0.357 0.483 0.441 0.483 0.565 0.523 1.915 1.423\n",
            " 2.257 0.706 1.135 1.219 0.672 0.    0.698 1.073 2.389 1.867 0.699 0.64\n",
            " 1.637 0.475 1.105 0.    0.   ]\n",
            "[1.012 2.204 0.    0.64  0.608 1.207 1.439 1.537 0.557 1.237 0.603 0.577\n",
            " 0.323 0.258 0.    0.74  0.191 2.335 0.    1.218 2.556 0.    0.998 1.493\n",
            " 0.665 1.729 0.523 0.    1.963 0.    0.546 0.476 0.791 0.524 0.704 0.\n",
            " 0.    1.592 0.769 0.732 0.976 0.639 2.853 1.752 0.222 0.    0.66  1.958\n",
            " 1.654 0.478 0.628 1.769 2.861 1.462 2.238 0.    1.497 0.    0.932 2.261\n",
            " 0.802 0.992 1.246 0.933 1.322 0.989 0.    2.729 0.751 1.323 0.    0.496\n",
            " 0.    0.256 2.148 1.526 1.11  0.83  1.246 1.321 1.204 0.681 0.675 1.791\n",
            " 0.428 0.    2.221 1.2   0.692 0.624 0.557 0.541 1.148 1.02  0.651 0.974\n",
            " 0.437 0.    0.494 0.   ]\n",
            "[0.151 0.203 0.144 0.174 0.211 0.197 0.196 0.261 0.188 0.222 0.165 0.161\n",
            " 0.157 0.033 0.175 0.174 0.082 0.133 0.098 0.145 0.151 0.165 0.179 0.163\n",
            " 0.181 0.158 0.142 0.173 0.125 0.174 0.196 0.175 0.431 0.088 0.036 0.097\n",
            " 0.318 0.259 0.195 0.149 0.191 0.205 0.172 0.146 0.055 0.025 0.176 0.19\n",
            " 0.171 0.203 0.124 0.145 0.07  0.377 0.212 0.161 0.162 0.162 0.203 0.177\n",
            " 0.178 0.175 0.13  0.148 0.12  0.153 0.12  0.186 0.15  0.173 0.171 0.045\n",
            " 0.116 0.031 0.092 0.138 0.133 0.144 0.104 0.23  0.156 0.167 0.183 0.14\n",
            " 0.16  0.157 0.187 0.177 0.158 0.124 0.157 0.148 0.196 0.147 0.142 0.122\n",
            " 0.212 0.131 0.227 0.249]\n",
            "(100,)\n",
            "<class 'int'>\n",
            "267\n",
            "/content/gdrive/My Drive/thesis/Data/S06/S06_TEX04.txt\n",
            "6\n",
            "[ 0.303  1.78   0.764  1.314  0.565  3.935  3.743  0.71   2.243  1.159\n",
            "  1.998  1.52   1.305  1.154  0.891  0.862  0.867  1.128  1.625  2.958\n",
            "  3.09   4.305  0.564  1.366  1.794  0.926  1.306  0.855  1.045  1.746\n",
            "  0.632  0.713  0.824  1.142  0.98   2.977  9.151  0.44   1.205  8.108\n",
            "  0.47   0.491  2.919  0.171  2.399  0.411  2.282  1.329  2.652  1.953\n",
            "  0.745  0.357  3.691  0.506  1.649  1.702  0.766  0.473  0.498  0.783\n",
            "  0.505  3.648  2.955  0.846  1.288  1.232  0.456  1.04   0.739  1.04\n",
            "  0.722  0.815  4.298  2.651  0.455  0.175  1.8    1.318  1.015  1.026\n",
            "  4.374  3.062  1.152  1.429  2.944  0.956 13.667  0.759  0.424  0.731\n",
            "  0.4    0.491  1.151  1.106  0.979  1.551  1.572]\n",
            "[0.303 1.78  0.764 1.314 0.565 0.    0.    0.71  2.243 1.159 1.998 1.52\n",
            " 1.305 1.154 0.891 0.862 0.867 1.128 1.625 2.958 0.    0.    0.564 1.366\n",
            " 1.794 0.926 1.306 0.855 1.045 1.746 0.632 0.713 0.824 1.142 0.98  2.977\n",
            " 0.    0.44  1.205 0.    0.47  0.491 2.919 0.171 2.399 0.411 2.282 1.329\n",
            " 2.652 1.953 0.745 0.357 0.    0.506 1.649 1.702 0.766 0.473 0.498 0.783\n",
            " 0.505 0.    2.955 0.846 1.288 1.232 0.456 1.04  0.739 1.04  0.722 0.815\n",
            " 0.    2.651 0.455 0.175 1.8   1.318 1.015 1.026 0.    0.    1.152 1.429\n",
            " 2.944 0.956 0.    0.759 0.424 0.731 0.4   0.491 1.151 1.106 0.979 1.551\n",
            " 1.572]\n",
            "[0.    0.303 1.78  0.764 1.314 0.565 0.    0.    0.71  2.243 1.159 1.998\n",
            " 1.52  1.305 1.154 0.891 0.862 0.867 1.128 1.625 2.958 0.    0.    0.564\n",
            " 1.366 1.794 0.926 1.306 0.855 1.045 1.746 0.632 0.713 0.824 1.142 0.98\n",
            " 2.977 0.    0.44  1.205 0.    0.47  0.491 2.919 0.171 2.399 0.411 2.282\n",
            " 1.329 2.652 1.953 0.745 0.357 0.    0.506 1.649 1.702 0.766 0.473 0.498\n",
            " 0.783 0.505 0.    2.955 0.846 1.288 1.232 0.456 1.04  0.739 1.04  0.722\n",
            " 0.815 0.    2.651 0.455 0.175 1.8   1.318 1.015 1.026 0.    0.    1.152\n",
            " 1.429 2.944 0.956 0.    0.759 0.424 0.731 0.4   0.491 1.151 1.106 0.979\n",
            " 1.551 1.572 0.    0.   ]\n",
            "[0.    0.129 0.159 0.182 0.257 0.131 0.165 0.212 0.161 0.192 0.134 0.205\n",
            " 0.128 0.153 0.215 0.139 0.155 0.173 0.138 0.132 0.069 0.295 0.199 0.173\n",
            " 0.208 0.163 0.173 0.154 0.128 0.17  0.173 0.124 0.148 0.226 0.198 0.204\n",
            " 0.139 0.212 0.106 0.173 0.199 0.195 0.157 0.089 0.054 0.041 0.145 0.15\n",
            " 0.154 0.213 0.186 0.162 0.132 0.099 0.165 0.15  0.113 0.116 0.131 0.123\n",
            " 0.082 0.139 0.066 0.125 0.088 0.149 0.123 0.13  0.164 0.106 0.097 0.163\n",
            " 0.122 0.156 0.371 0.388 0.116 0.241 0.278 0.24  0.132 0.104 0.261 0.316\n",
            " 0.152 0.138 0.128 0.153 0.235 0.174 0.192 0.201 0.149 0.233 0.102 0.052\n",
            " 0.121 0.157 0.197 0.    0.   ]\n",
            "(100,)\n",
            "<class 'int'>\n",
            "268\n",
            "/content/gdrive/My Drive/thesis/Data/S06/S06_TEX05.txt\n",
            "6\n",
            "[1.164 7.167 1.537 1.38  3.193 0.394 0.649 0.591 0.365 1.342 3.172 2.86\n",
            " 0.656 1.24  1.505 1.274 1.181 2.94  2.603 1.179 1.117 1.421 1.286 2.947\n",
            " 4.799 0.435 0.932 2.37  0.115 0.39  0.274 1.75  1.289 1.489 0.879 1.817\n",
            " 2.106 1.247 1.138 0.857 1.895 3.837 0.855 1.339 1.478 6.968 0.824 1.334\n",
            " 2.021 0.564 0.926 0.99  1.511 0.76  1.111 0.734 0.295 0.641 0.482 0.326\n",
            " 1.165 0.646]\n",
            "[1.164 0.    1.537 1.38  0.    0.394 0.649 0.591 0.365 1.342 0.    2.86\n",
            " 0.656 1.24  1.505 1.274 1.181 2.94  2.603 1.179 1.117 1.421 1.286 2.947\n",
            " 0.    0.435 0.932 2.37  0.115 0.39  0.274 1.75  1.289 1.489 0.879 1.817\n",
            " 2.106 1.247 1.138 0.857 1.895 0.    0.855 1.339 1.478 0.    0.824 1.334\n",
            " 2.021 0.564 0.926 0.99  1.511 0.76  1.111 0.734 0.295 0.641 0.482 0.326\n",
            " 1.165 0.646]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    1.164 0.    1.537 1.38  0.\n",
            " 0.394 0.649 0.591 0.365 1.342 0.    2.86  0.656 1.24  1.505 1.274 1.181\n",
            " 2.94  2.603 1.179 1.117 1.421 1.286 2.947 0.    0.435 0.932 2.37  0.115\n",
            " 0.39  0.274 1.75  1.289 1.489 0.879 1.817 2.106 1.247 1.138 0.857 1.895\n",
            " 0.    0.855 1.339 1.478 0.    0.824 1.334 2.021 0.564 0.926 0.99  1.511\n",
            " 0.76  1.111 0.734 0.295 0.641 0.482 0.326 1.165 0.646 0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.   ]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.164 0.071 0.144 0.146 0.203 0.151\n",
            " 0.166 0.199 0.131 0.156 0.204 0.156 0.199 0.208 0.206 0.156 0.245 0.161\n",
            " 0.173 0.154 0.215 0.22  0.181 0.199 0.202 0.218 0.314 0.199 0.021 0.23\n",
            " 0.165 0.075 0.17  0.153 0.136 0.156 0.192 0.131 0.131 0.19  0.18  0.149\n",
            " 0.118 0.154 0.161 0.188 0.102 0.152 0.14  0.14  0.183 0.089 0.121 0.072\n",
            " 0.095 0.164 0.128 0.108 0.191 0.125 0.17  0.13  0.223 0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.   ]\n",
            "(100,)\n",
            "<class 'int'>\n",
            "269\n",
            "/content/gdrive/My Drive/thesis/Data/S06/S06_TEX09.txt\n",
            "6\n",
            "[1.746 1.158 1.877 1.315 1.073 0.806 1.322 3.257 0.421 2.672 1.606 1.634\n",
            " 1.105 0.68  1.24  0.894 0.922 1.143 2.499 1.042 0.885 0.907 2.354 0.831\n",
            " 1.009 0.617 3.097 1.893 2.031 1.227 1.315 0.837 1.105 0.649 1.148 3.782\n",
            " 0.64  1.292 1.677 0.342 2.413 0.635 1.427 2.228 6.614 0.866 4.966 0.711\n",
            " 1.972 1.063 3.235 2.204 1.361 1.041 1.613 0.855 1.655 1.114 0.981 0.781\n",
            " 3.216 1.007 0.671 0.808 1.253 0.441 0.766 0.932 0.613 3.409]\n",
            "[1.746 1.158 1.877 1.315 1.073 0.806 1.322 0.    0.421 2.672 1.606 1.634\n",
            " 1.105 0.68  1.24  0.894 0.922 1.143 2.499 1.042 0.885 0.907 2.354 0.831\n",
            " 1.009 0.617 0.    1.893 2.031 1.227 1.315 0.837 1.105 0.649 1.148 0.\n",
            " 0.64  1.292 1.677 0.342 2.413 0.635 1.427 2.228 0.    0.866 0.    0.711\n",
            " 1.972 1.063 0.    2.204 1.361 1.041 1.613 0.855 1.655 1.114 0.981 0.781\n",
            " 0.    1.007 0.671 0.808 1.253 0.441 0.766 0.932 0.613 0.   ]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    1.746 1.158 1.877 1.315 1.073 0.806 1.322 0.    0.421\n",
            " 2.672 1.606 1.634 1.105 0.68  1.24  0.894 0.922 1.143 2.499 1.042 0.885\n",
            " 0.907 2.354 0.831 1.009 0.617 0.    1.893 2.031 1.227 1.315 0.837 1.105\n",
            " 0.649 1.148 0.    0.64  1.292 1.677 0.342 2.413 0.635 1.427 2.228 0.\n",
            " 0.866 0.    0.711 1.972 1.063 0.    2.204 1.361 1.041 1.613 0.855 1.655\n",
            " 1.114 0.981 0.781 0.    1.007 0.671 0.808 1.253 0.441 0.766 0.932 0.613\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.   ]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.19  0.138 0.194 0.055 0.103 0.111 0.078 0.128 0.18  0.058\n",
            " 0.171 0.068 0.122 0.098 0.159 0.09  0.086 0.103 0.133 0.152 0.117 0.105\n",
            " 0.187 0.128 0.128 0.143 0.155 0.187 0.046 0.12  0.041 0.063 0.056 0.092\n",
            " 0.09  0.141 0.173 0.123 0.038 0.09  0.062 0.145 0.117 0.069 0.129 0.107\n",
            " 0.098 0.103 0.099 0.156 0.098 0.139 0.13  0.124 0.121 0.105 0.147 0.122\n",
            " 0.122 0.071 0.122 0.097 0.113 0.107 0.098 0.027 0.091 0.155 0.072 0.073\n",
            " 0.04  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.   ]\n",
            "(100,)\n",
            "<class 'int'>\n",
            "270\n",
            "/content/gdrive/My Drive/thesis/Data/S06/S06_TEX08.txt\n",
            "6\n",
            "[ 1.282 11.967  1.301  0.868  1.464  1.029  1.054  1.097  0.916  1.074\n",
            "  0.815  1.146  0.721  1.256  0.914  4.998  0.354  0.209  0.208  0.868\n",
            "  5.693  0.829  0.59   0.798  0.749  0.78   1.624  5.786  1.391  2.265\n",
            "  0.683  0.859  1.571  1.145  0.881  0.689  0.533  1.023  0.949  0.79\n",
            "  0.81   0.968  1.379  0.867  1.221  1.34   0.536  2.964  1.871  0.603\n",
            "  1.614  0.534  0.797  1.133  4.947  1.55   1.483  1.769  1.373  1.099\n",
            "  0.824  1.227  1.44   1.006  2.029  0.836  1.798  0.924  0.906  0.288\n",
            "  4.569  0.411  0.242  0.181  2.517  1.048  0.75 ]\n",
            "[1.282 0.    1.301 0.868 1.464 1.029 1.054 1.097 0.916 1.074 0.815 1.146\n",
            " 0.721 1.256 0.914 0.    0.354 0.209 0.208 0.868 0.    0.829 0.59  0.798\n",
            " 0.749 0.78  1.624 0.    1.391 2.265 0.683 0.859 1.571 1.145 0.881 0.689\n",
            " 0.533 1.023 0.949 0.79  0.81  0.968 1.379 0.867 1.221 1.34  0.536 2.964\n",
            " 1.871 0.603 1.614 0.534 0.797 1.133 0.    1.55  1.483 1.769 1.373 1.099\n",
            " 0.824 1.227 1.44  1.006 2.029 0.836 1.798 0.924 0.906 0.288 0.    0.411\n",
            " 0.242 0.181 2.517 1.048 0.75 ]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    1.282\n",
            " 0.    1.301 0.868 1.464 1.029 1.054 1.097 0.916 1.074 0.815 1.146 0.721\n",
            " 1.256 0.914 0.    0.354 0.209 0.208 0.868 0.    0.829 0.59  0.798 0.749\n",
            " 0.78  1.624 0.    1.391 2.265 0.683 0.859 1.571 1.145 0.881 0.689 0.533\n",
            " 1.023 0.949 0.79  0.81  0.968 1.379 0.867 1.221 1.34  0.536 2.964 1.871\n",
            " 0.603 1.614 0.534 0.797 1.133 0.    1.55  1.483 1.769 1.373 1.099 0.824\n",
            " 1.227 1.44  1.006 2.029 0.836 1.798 0.924 0.906 0.288 0.    0.411 0.242\n",
            " 0.181 2.517 1.048 0.75  0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.   ]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.08\n",
            " 0.054 0.194 0.124 0.164 0.178 0.105 0.189 0.123 0.121 0.154 0.229 0.12\n",
            " 0.215 0.072 0.181 0.214 0.117 0.108 0.09  0.22  0.196 0.165 0.15  0.158\n",
            " 0.158 0.176 0.196 0.139 0.12  0.166 0.174 0.161 0.17  0.155 0.222 0.148\n",
            " 0.212 0.139 0.02  0.253 0.167 0.154 0.171 0.221 0.179 0.128 0.033 0.119\n",
            " 0.127 0.224 0.122 0.137 0.156 0.128 0.219 0.132 0.113 0.115 0.105 0.113\n",
            " 0.128 0.147 0.211 0.112 0.219 0.165 0.129 0.22  0.021 0.148 0.086 0.115\n",
            " 0.081 0.075 0.144 0.219 0.158 0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.   ]\n",
            "(100,)\n",
            "<class 'int'>\n",
            "271\n",
            "/content/gdrive/My Drive/thesis/Data/S06/S06_TEX10.txt\n",
            "6\n",
            "[1.167 2.599 3.2   1.194 0.724 1.238 1.89  0.856 0.501 0.96  0.797 1.586\n",
            " 1.127 2.583 2.199 1.247 1.349 3.199 3.065 0.74  1.269 1.714 1.173 0.791\n",
            " 1.436 3.984 1.144 1.146 1.358 0.815 1.742 9.003 0.833 0.744 0.892 0.876\n",
            " 0.872 0.742 0.716 0.781 0.993 0.504 1.199 1.354 1.237 1.653 1.293 1.013\n",
            " 1.143 2.668 1.625 0.862 0.482 0.957 1.686 0.722 0.76  1.031 0.972 0.222\n",
            " 2.327]\n",
            "[1.167 2.599 0.    1.194 0.724 1.238 1.89  0.856 0.501 0.96  0.797 1.586\n",
            " 1.127 2.583 2.199 1.247 1.349 0.    0.    0.74  1.269 1.714 1.173 0.791\n",
            " 1.436 0.    1.144 1.146 1.358 0.815 1.742 0.    0.833 0.744 0.892 0.876\n",
            " 0.872 0.742 0.716 0.781 0.993 0.504 1.199 1.354 1.237 1.653 1.293 1.013\n",
            " 1.143 2.668 1.625 0.862 0.482 0.957 1.686 0.722 0.76  1.031 0.972 0.222\n",
            " 2.327]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    1.167 2.599 0.    1.194 0.724\n",
            " 1.238 1.89  0.856 0.501 0.96  0.797 1.586 1.127 2.583 2.199 1.247 1.349\n",
            " 0.    0.    0.74  1.269 1.714 1.173 0.791 1.436 0.    1.144 1.146 1.358\n",
            " 0.815 1.742 0.    0.833 0.744 0.892 0.876 0.872 0.742 0.716 0.781 0.993\n",
            " 0.504 1.199 1.354 1.237 1.653 1.293 1.013 1.143 2.668 1.625 0.862 0.482\n",
            " 0.957 1.686 0.722 0.76  1.031 0.972 0.222 2.327 0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.   ]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.178 0.126 0.112 0.036 0.106\n",
            " 0.096 0.097 0.101 0.135 0.1   0.105 0.097 0.13  0.075 0.078 0.133 0.165\n",
            " 0.113 0.139 0.098 0.115 0.1   0.065 0.09  0.096 0.165 0.095 0.13  0.158\n",
            " 0.113 0.112 0.147 0.155 0.145 0.149 0.113 0.126 0.11  0.117 0.115 0.107\n",
            " 0.12  0.064 0.096 0.113 0.106 0.091 0.085 0.095 0.116 0.207 0.137 0.09\n",
            " 0.115 0.097 0.116 0.199 0.162 0.121 0.054 0.173 0.057 0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.   ]\n",
            "(100,)\n",
            "<class 'int'>\n",
            "272\n",
            "/content/gdrive/My Drive/thesis/Data/S06/S06_TEX11.txt\n",
            "6\n",
            "[0.752 0.666 0.623 1.391 0.889 1.996 2.935 1.21  1.172 3.167 1.063 1.181\n",
            " 0.86  0.802 0.789 0.86  1.646 1.105 0.671 2.328 4.367 0.785 0.794 1.018\n",
            " 1.484 0.937 0.95  0.688 1.023 0.754 0.953 0.71  0.698 1.291 7.493 1.35\n",
            " 1.466 1.074 0.987 0.872 1.308 0.487 0.491 1.582 1.23  1.496 1.314 0.691\n",
            " 0.18  0.473 0.877 1.08  0.44  0.813 0.999 0.696]\n",
            "[0.752 0.666 0.623 1.391 0.889 1.996 2.935 1.21  1.172 0.    1.063 1.181\n",
            " 0.86  0.802 0.789 0.86  1.646 1.105 0.671 2.328 0.    0.785 0.794 1.018\n",
            " 1.484 0.937 0.95  0.688 1.023 0.754 0.953 0.71  0.698 1.291 0.    1.35\n",
            " 1.466 1.074 0.987 0.872 1.308 0.487 0.491 1.582 1.23  1.496 1.314 0.691\n",
            " 0.18  0.473 0.877 1.08  0.44  0.813 0.999 0.696]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.752 0.666\n",
            " 0.623 1.391 0.889 1.996 2.935 1.21  1.172 0.    1.063 1.181 0.86  0.802\n",
            " 0.789 0.86  1.646 1.105 0.671 2.328 0.    0.785 0.794 1.018 1.484 0.937\n",
            " 0.95  0.688 1.023 0.754 0.953 0.71  0.698 1.291 0.    1.35  1.466 1.074\n",
            " 0.987 0.872 1.308 0.487 0.491 1.582 1.23  1.496 1.314 0.691 0.18  0.473\n",
            " 0.877 1.08  0.44  0.813 0.999 0.696 0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.   ]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.129 0.199 0.232\n",
            " 0.182 0.146 0.173 0.113 0.12  0.124 0.132 0.106 0.157 0.132 0.194 0.164\n",
            " 0.091 0.069 0.111 0.145 0.14  0.106 0.112 0.108 0.128 0.149 0.112 0.081\n",
            " 0.129 0.081 0.088 0.166 0.11  0.157 0.158 0.113 0.146 0.132 0.13  0.136\n",
            " 0.164 0.123 0.129 0.165 0.157 0.155 0.104 0.08  0.146 0.062 0.114 0.14\n",
            " 0.096 0.096 0.097 0.057 0.139 0.1   0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.   ]\n",
            "(100,)\n",
            "<class 'int'>\n",
            "273\n",
            "/content/gdrive/My Drive/thesis/Data/S06/S06_TEX07.txt\n",
            "6\n",
            "[0.838 0.853 0.752 1.007 1.642 0.942 1.629 0.838 0.808 1.235 1.006 1.091\n",
            " 1.007 1.064 2.292 1.371 1.705 0.732 0.973 1.389 1.137 1.721 0.433 1.133\n",
            " 1.379 1.855 1.121 1.155 2.085 2.079 0.42  0.441 0.693 0.154 3.653 0.407\n",
            " 0.275 0.216 0.216 0.207 2.182 2.494 0.728 1.425 0.529 1.131 2.498 0.837\n",
            " 0.899 0.847 2.118 0.499 0.464 4.875 0.546 0.925 2.11  4.375 1.043 0.564\n",
            " 1.685 1.887 0.529 1.013 2.399 0.863 0.873 0.804 1.491 0.119 0.837 2.054\n",
            " 0.719 0.424 0.257 0.275 0.167 0.624 0.415 0.514 0.722 3.173 2.224 1.144\n",
            " 1.758 0.771 0.849 0.764 0.574 1.059 5.588 1.075 0.242 1.482 1.18  1.156\n",
            " 1.103 0.783 0.956 1.08  0.82  0.666 0.791 0.19  0.309 1.566 4.379 3.218\n",
            " 3.192 1.904 0.892 0.8   0.728 0.93  1.256 4.342 0.865 0.404]\n",
            "[0.838 0.853 0.752 1.007 1.642 0.942 1.629 0.838 0.808 1.235 1.006 1.091\n",
            " 1.007 1.064 2.292 1.371 1.705 0.732 0.973 1.389 1.137 1.721 0.433 1.133\n",
            " 1.379 1.855 1.121 1.155 2.085 2.079 0.42  0.441 0.693 0.154 0.    0.407\n",
            " 0.275 0.216 0.216 0.207 2.182 2.494 0.728 1.425 0.529 1.131 2.498 0.837\n",
            " 0.899 0.847 2.118 0.499 0.464 0.    0.546 0.925 2.11  0.    1.043 0.564\n",
            " 1.685 1.887 0.529 1.013 2.399 0.863 0.873 0.804 1.491 0.119 0.837 2.054\n",
            " 0.719 0.424 0.257 0.275 0.167 0.624 0.415 0.514 0.722 0.    2.224 1.144\n",
            " 1.758 0.771 0.849 0.764 0.574 1.059 0.    1.075 0.242 1.482 1.18  1.156\n",
            " 1.103 0.783 0.956 1.08  0.82  0.666 0.791 0.19  0.309 1.566 0.    0.\n",
            " 0.    1.904 0.892 0.8   0.728 0.93  1.256 0.    0.865 0.404]\n",
            "[0.838 0.853 0.752 1.007 1.642 0.942 1.629 0.838 0.808 1.235 1.006 1.091\n",
            " 1.007 1.064 2.292 1.371 1.705 0.732 0.973 1.389 1.137 1.721 0.433 1.133\n",
            " 1.379 1.855 1.121 1.155 2.085 2.079 0.42  0.441 0.693 0.154 0.    0.407\n",
            " 0.275 0.216 0.216 0.207 2.182 2.494 0.728 1.425 0.529 1.131 2.498 0.837\n",
            " 0.899 0.847 2.118 0.499 0.464 0.    0.546 0.925 2.11  0.    1.043 0.564\n",
            " 1.685 1.887 0.529 1.013 2.399 0.863 0.873 0.804 1.491 0.119 0.837 2.054\n",
            " 0.719 0.424 0.257 0.275 0.167 0.624 0.415 0.514 0.722 0.    2.224 1.144\n",
            " 1.758 0.771 0.849 0.764 0.574 1.059 0.    1.075 0.242 1.482 1.18  1.156\n",
            " 1.103 0.783 0.956 1.08 ]\n",
            "[0.179 0.277 0.224 0.182 0.151 0.156 0.137 0.161 0.138 0.17  0.208 0.189\n",
            " 0.196 0.195 0.154 0.189 0.14  0.216 0.227 0.181 0.121 0.149 0.174 0.132\n",
            " 0.114 0.164 0.187 0.141 0.225 0.138 0.219 0.283 0.148 0.088 0.059 0.249\n",
            " 0.082 0.116 0.09  0.083 0.067 0.162 0.137 0.123 0.17  0.073 0.132 0.185\n",
            " 0.137 0.154 0.087 0.356 0.24  0.283 0.187 0.14  0.113 0.14  0.327 0.341\n",
            " 0.292 0.112 0.129 0.048 0.198 0.145 0.145 0.103 0.097 0.07  0.042 0.205\n",
            " 0.043 0.198 0.099 0.083 0.031 0.064 0.231 0.256 0.24  0.227 0.136 0.158\n",
            " 0.144 0.181 0.148 0.265 0.249 0.231 0.253 0.159 0.049 0.089 0.179 0.188\n",
            " 0.219 0.098 0.18  0.104]\n",
            "(100,)\n",
            "<class 'int'>\n",
            "274\n",
            "/content/gdrive/My Drive/thesis/Data/S06/S06_TEX06.txt\n",
            "6\n",
            "[1.0380e+00 8.2300e-01 1.1950e+00 9.9200e-01 5.3110e+00 2.2800e+00\n",
            " 8.0200e-01 3.9200e-01 4.5700e-01 3.8020e+00 4.1900e-01 1.6910e+00\n",
            " 1.5470e+00 1.5630e+00 1.0210e+00 1.2040e+00 8.9320e+00 2.5830e+00\n",
            " 9.0000e-01 7.7000e-01 1.7810e+00 8.4820e+00 1.3710e+00 1.3150e+00\n",
            " 1.0370e+00 9.5700e-01 9.1300e-01 8.2800e-01 4.2400e-01 8.7500e-01\n",
            " 1.3890e+00 2.5110e+00 1.6180e+00 7.7600e-01 1.9710e+00 9.3200e-01\n",
            " 7.3400e-01 1.0670e+00 3.0600e+00 7.1300e-01 9.8200e-01 1.7430e+00\n",
            " 3.9860e+00 9.1200e-01 3.8670e+00 7.7181e+01 9.3300e-01 1.4530e+00\n",
            " 1.3710e+00 1.6560e+00 2.9190e+00 6.7700e-01 6.4000e-01 2.1812e+01\n",
            " 1.4130e+00 9.4600e-01 1.0410e+00 1.0220e+00 1.7270e+00 1.7350e+00\n",
            " 1.4350e+00 5.7900e-01 1.0570e+00 4.6910e+00 3.2700e-01 2.8400e-01\n",
            " 2.9290e+00 1.0640e+00 1.5610e+00 7.3400e-01 8.0600e-01 3.1660e+00\n",
            " 1.2150e+00 8.9000e-01 4.3800e-01 4.2500e-01 1.5120e+00 3.8510e+00\n",
            " 1.1100e+00 7.3000e-02 2.6910e+00 4.2060e+00 4.8470e+00 2.7730e+00\n",
            " 4.2500e+00 1.1200e+00 1.2740e+00 1.3200e+00 7.8400e-01 8.6200e-01\n",
            " 8.0800e-01 9.6400e-01 8.0700e-01]\n",
            "[1.038 0.823 1.195 0.992 0.    2.28  0.802 0.392 0.457 0.    0.419 1.691\n",
            " 1.547 1.563 1.021 1.204 0.    2.583 0.9   0.77  1.781 0.    1.371 1.315\n",
            " 1.037 0.957 0.913 0.828 0.424 0.875 1.389 2.511 1.618 0.776 1.971 0.932\n",
            " 0.734 1.067 0.    0.713 0.982 1.743 0.    0.912 0.    0.    0.933 1.453\n",
            " 1.371 1.656 2.919 0.677 0.64  0.    1.413 0.946 1.041 1.022 1.727 1.735\n",
            " 1.435 0.579 1.057 0.    0.327 0.284 2.929 1.064 1.561 0.734 0.806 0.\n",
            " 1.215 0.89  0.438 0.425 1.512 0.    1.11  0.073 2.691 0.    0.    2.773\n",
            " 0.    1.12  1.274 1.32  0.784 0.862 0.808 0.964 0.807]\n",
            "[0.    0.    0.    1.038 0.823 1.195 0.992 0.    2.28  0.802 0.392 0.457\n",
            " 0.    0.419 1.691 1.547 1.563 1.021 1.204 0.    2.583 0.9   0.77  1.781\n",
            " 0.    1.371 1.315 1.037 0.957 0.913 0.828 0.424 0.875 1.389 2.511 1.618\n",
            " 0.776 1.971 0.932 0.734 1.067 0.    0.713 0.982 1.743 0.    0.912 0.\n",
            " 0.    0.933 1.453 1.371 1.656 2.919 0.677 0.64  0.    1.413 0.946 1.041\n",
            " 1.022 1.727 1.735 1.435 0.579 1.057 0.    0.327 0.284 2.929 1.064 1.561\n",
            " 0.734 0.806 0.    1.215 0.89  0.438 0.425 1.512 0.    1.11  0.073 2.691\n",
            " 0.    0.    2.773 0.    1.12  1.274 1.32  0.784 0.862 0.808 0.964 0.807\n",
            " 0.    0.    0.    0.   ]\n",
            "[0.    0.    0.    0.153 0.155 0.163 0.141 0.179 0.172 0.128 0.156 0.164\n",
            " 0.148 0.243 0.207 0.163 0.171 0.246 0.181 0.097 0.18  0.107 0.22  0.167\n",
            " 0.123 0.244 0.162 0.161 0.121 0.202 0.178 0.199 0.107 0.154 0.046 0.079\n",
            " 0.214 0.162 0.077 0.144 0.215 0.146 0.203 0.253 0.221 0.239 0.152 0.18\n",
            " 0.135 0.146 0.144 0.119 0.105 0.169 0.093 0.122 0.124 0.187 0.162 0.115\n",
            " 0.163 0.171 0.183 0.126 0.228 0.406 0.166 0.177 0.124 0.065 0.103 0.204\n",
            " 0.172 0.111 0.17  0.18  0.169 0.145 0.114 0.113 0.13  0.312 0.024 0.191\n",
            " 0.235 0.278 0.161 0.488 0.223 0.106 0.211 0.09  0.087 0.073 0.195 0.081\n",
            " 0.103 0.    0.    0.    0.   ]\n",
            "(100,)\n",
            "<class 'int'>\n",
            "275\n",
            "/content/gdrive/My Drive/thesis/Data/S01/S01_TEX05.txt\n",
            "1\n",
            "[0.619 0.598 0.69  0.598 0.967 1.288 0.71  2.506 0.635 0.459 1.064 2.426\n",
            " 1.978 0.702 0.681 3.509 0.636 0.91  1.003 0.663 2.426 0.705 2.978 0.595\n",
            " 1.465 0.819 0.698 0.638 0.615 2.556 0.803 8.891 2.318 0.711 1.339 1.314\n",
            " 0.87  2.86  6.588 3.217 0.879 0.587 1.024 1.738 0.603 0.714 0.309 0.622\n",
            " 0.885 0.876 0.887 0.588 1.339 1.487 0.637 0.525]\n",
            "[0.619 0.598 0.69  0.598 0.967 1.288 0.71  2.506 0.635 0.459 1.064 2.426\n",
            " 1.978 0.702 0.681 0.    0.636 0.91  1.003 0.663 2.426 0.705 2.978 0.595\n",
            " 1.465 0.819 0.698 0.638 0.615 2.556 0.803 0.    2.318 0.711 1.339 1.314\n",
            " 0.87  2.86  0.    0.    0.879 0.587 1.024 1.738 0.603 0.714 0.309 0.622\n",
            " 0.885 0.876 0.887 0.588 1.339 1.487 0.637 0.525]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.619 0.598\n",
            " 0.69  0.598 0.967 1.288 0.71  2.506 0.635 0.459 1.064 2.426 1.978 0.702\n",
            " 0.681 0.    0.636 0.91  1.003 0.663 2.426 0.705 2.978 0.595 1.465 0.819\n",
            " 0.698 0.638 0.615 2.556 0.803 0.    2.318 0.711 1.339 1.314 0.87  2.86\n",
            " 0.    0.    0.879 0.587 1.024 1.738 0.603 0.714 0.309 0.622 0.885 0.876\n",
            " 0.887 0.588 1.339 1.487 0.637 0.525 0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.   ]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.096 0.066 0.099\n",
            " 0.107 0.09  0.07  0.087 0.065 0.052 0.083 0.064 0.071 0.088 0.086 0.074\n",
            " 0.073 0.096 0.082 0.086 0.088 0.065 0.098 0.082 0.095 0.081 0.078 0.082\n",
            " 0.089 0.075 0.099 0.094 0.073 0.112 0.128 0.09  0.063 0.087 0.079 0.061\n",
            " 0.094 0.053 0.063 0.081 0.072 0.07  0.082 0.058 0.058 0.057 0.086 0.05\n",
            " 0.077 0.079 0.137 0.071 0.074 0.048 0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.   ]\n",
            "(100,)\n",
            "<class 'int'>\n",
            "276\n",
            "/content/gdrive/My Drive/thesis/Data/S01/S01_TEX03.txt\n",
            "1\n",
            "[ 0.913  1.348  0.636  0.996  2.104  0.891  0.778  1.981  0.661  0.81\n",
            "  0.896  1.544  0.432  0.698  0.506  1.831  0.637  0.804  0.512  0.837\n",
            "  2.555  0.814  1.143  0.441  1.163  0.563  2.377  2.535  1.173  1.93\n",
            "  0.636  0.565 12.423  2.093  0.85   0.67   0.576  1.755  0.685  0.543\n",
            "  0.88   2.622  0.708  0.759  0.828  1.573  0.879  0.619  2.003  0.638\n",
            "  0.631  0.705  3.294  0.887  0.766  0.478]\n",
            "[0.913 1.348 0.636 0.996 2.104 0.891 0.778 1.981 0.661 0.81  0.896 1.544\n",
            " 0.432 0.698 0.506 1.831 0.637 0.804 0.512 0.837 2.555 0.814 1.143 0.441\n",
            " 1.163 0.563 2.377 2.535 1.173 1.93  0.636 0.565 0.    2.093 0.85  0.67\n",
            " 0.576 1.755 0.685 0.543 0.88  2.622 0.708 0.759 0.828 1.573 0.879 0.619\n",
            " 2.003 0.638 0.631 0.705 0.    0.887 0.766 0.478]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.913 1.348\n",
            " 0.636 0.996 2.104 0.891 0.778 1.981 0.661 0.81  0.896 1.544 0.432 0.698\n",
            " 0.506 1.831 0.637 0.804 0.512 0.837 2.555 0.814 1.143 0.441 1.163 0.563\n",
            " 2.377 2.535 1.173 1.93  0.636 0.565 0.    2.093 0.85  0.67  0.576 1.755\n",
            " 0.685 0.543 0.88  2.622 0.708 0.759 0.828 1.573 0.879 0.619 2.003 0.638\n",
            " 0.631 0.705 0.    0.887 0.766 0.478 0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.   ]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.071 0.08  0.078\n",
            " 0.099 0.115 0.089 0.071 0.09  0.112 0.074 0.062 0.088 0.065 0.09  0.074\n",
            " 0.074 0.063 0.066 0.059 0.072 0.057 0.045 0.087 0.066 0.081 0.081 0.083\n",
            " 0.032 0.073 0.063 0.069 0.065 0.074 0.062 0.042 0.078 0.107 0.08  0.068\n",
            " 0.089 0.061 0.071 0.081 0.088 0.068 0.089 0.043 0.078 0.116 0.072 0.074\n",
            " 0.108 0.084 0.077 0.071 0.07  0.065 0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.   ]\n",
            "(100,)\n",
            "<class 'int'>\n",
            "277\n",
            "/content/gdrive/My Drive/thesis/Data/S01/S01_TEX09.txt\n",
            "1\n",
            "[0.9   0.578 1.129 0.668 1.088 3.609 0.646 0.599 0.624 1.733 1.541 0.886\n",
            " 1.399 0.668 0.563 0.765 1.535 0.872 0.502 0.643 0.875 2.615 1.274 0.419\n",
            " 0.575 1.391 0.812 0.636 1.068 0.465 1.98  0.694 3.004 1.193 1.224 1.554\n",
            " 0.644 1.375 0.831 0.893 0.627 0.768 2.06  6.989 0.746 0.612 1.696 0.574\n",
            " 1.317 1.877 1.486 0.612 0.588 1.029 0.874 3.071 0.724 0.541]\n",
            "[0.9   0.578 1.129 0.668 1.088 0.    0.646 0.599 0.624 1.733 1.541 0.886\n",
            " 1.399 0.668 0.563 0.765 1.535 0.872 0.502 0.643 0.875 2.615 1.274 0.419\n",
            " 0.575 1.391 0.812 0.636 1.068 0.465 1.98  0.694 0.    1.193 1.224 1.554\n",
            " 0.644 1.375 0.831 0.893 0.627 0.768 2.06  0.    0.746 0.612 1.696 0.574\n",
            " 1.317 1.877 1.486 0.612 0.588 1.029 0.874 0.    0.724 0.541]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.9   0.578 1.129\n",
            " 0.668 1.088 0.    0.646 0.599 0.624 1.733 1.541 0.886 1.399 0.668 0.563\n",
            " 0.765 1.535 0.872 0.502 0.643 0.875 2.615 1.274 0.419 0.575 1.391 0.812\n",
            " 0.636 1.068 0.465 1.98  0.694 0.    1.193 1.224 1.554 0.644 1.375 0.831\n",
            " 0.893 0.627 0.768 2.06  0.    0.746 0.612 1.696 0.574 1.317 1.877 1.486\n",
            " 0.612 0.588 1.029 0.874 0.    0.724 0.541 0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.   ]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.071 0.053 0.066 0.091\n",
            " 0.063 0.064 0.072 0.082 0.065 0.074 0.068 0.074 0.07  0.092 0.081 0.108\n",
            " 0.099 0.085 0.094 0.124 0.072 0.077 0.047 0.086 0.091 0.072 0.11  0.088\n",
            " 0.092 0.071 0.079 0.06  0.047 0.068 0.064 0.103 0.094 0.092 0.095 0.096\n",
            " 0.083 0.072 0.076 0.093 0.069 0.081 0.1   0.073 0.065 0.077 0.077 0.096\n",
            " 0.083 0.069 0.079 0.111 0.091 0.09  0.105 0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.   ]\n",
            "(100,)\n",
            "<class 'int'>\n",
            "278\n",
            "/content/gdrive/My Drive/thesis/Data/S01/S01_TEX07.txt\n",
            "1\n",
            "[0.813 0.93  0.665 0.563 0.614 0.435 0.598 0.54  1.756 3.307 0.734 0.615\n",
            " 1.139 2.806 0.961 0.903 0.816 0.771 1.687 0.623 0.64  0.714 1.106 0.573\n",
            " 1.    3.349 1.854 0.471 1.257 0.669 3.511 0.971 1.334 0.58  0.744 0.832\n",
            " 0.923 3.034 1.592 0.431 0.708 1.773 0.733 0.509 0.819 0.714 0.361 1.977\n",
            " 0.492 1.424 1.228 0.655]\n",
            "[0.813 0.93  0.665 0.563 0.614 0.435 0.598 0.54  1.756 0.    0.734 0.615\n",
            " 1.139 2.806 0.961 0.903 0.816 0.771 1.687 0.623 0.64  0.714 1.106 0.573\n",
            " 1.    0.    1.854 0.471 1.257 0.669 0.    0.971 1.334 0.58  0.744 0.832\n",
            " 0.923 0.    1.592 0.431 0.708 1.773 0.733 0.509 0.819 0.714 0.361 1.977\n",
            " 0.492 1.424 1.228 0.655]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.813 0.93  0.665 0.563 0.614 0.435 0.598 0.54  1.756 0.    0.734 0.615\n",
            " 1.139 2.806 0.961 0.903 0.816 0.771 1.687 0.623 0.64  0.714 1.106 0.573\n",
            " 1.    0.    1.854 0.471 1.257 0.669 0.    0.971 1.334 0.58  0.744 0.832\n",
            " 0.923 0.    1.592 0.431 0.708 1.773 0.733 0.509 0.819 0.714 0.361 1.977\n",
            " 0.492 1.424 1.228 0.655 0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.   ]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.069\n",
            " 0.072 0.07  0.087 0.081 0.108 0.088 0.08  0.071 0.068 0.126 0.092 0.091\n",
            " 0.089 0.091 0.07  0.089 0.096 0.124 0.107 0.115 0.098 0.107 0.115 0.115\n",
            " 0.079 0.096 0.102 0.106 0.078 0.091 0.077 0.064 0.116 0.094 0.08  0.08\n",
            " 0.064 0.12  0.074 0.117 0.106 0.077 0.093 0.1   0.102 0.103 0.133 0.1\n",
            " 0.131 0.122 0.082 0.084 0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.   ]\n",
            "(100,)\n",
            "<class 'int'>\n",
            "279\n",
            "/content/gdrive/My Drive/thesis/Data/S01/S01_TEX10.txt\n",
            "1\n",
            "[0.882 0.586 0.49  0.893 2.109 0.874 1.11  1.999 0.677 0.631 0.883 0.712\n",
            " 0.608 0.631 1.222 1.52  0.692 0.663 0.868 0.62  0.448 0.821 0.549 0.753\n",
            " 1.478 3.169 0.668 0.631 2.413 0.579 0.674 0.477 0.595 0.623 1.916 2.151\n",
            " 1.102 0.64  1.983 1.026 0.491 1.041 1.78  0.586 1.404 0.599 2.007 0.929\n",
            " 0.713 0.388 0.835 1.029]\n",
            "[0.882 0.586 0.49  0.893 2.109 0.874 1.11  1.999 0.677 0.631 0.883 0.712\n",
            " 0.608 0.631 1.222 1.52  0.692 0.663 0.868 0.62  0.448 0.821 0.549 0.753\n",
            " 1.478 0.    0.668 0.631 2.413 0.579 0.674 0.477 0.595 0.623 1.916 2.151\n",
            " 1.102 0.64  1.983 1.026 0.491 1.041 1.78  0.586 1.404 0.599 2.007 0.929\n",
            " 0.713 0.388 0.835 1.029]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.882 0.586 0.49  0.893 2.109 0.874 1.11  1.999 0.677 0.631 0.883 0.712\n",
            " 0.608 0.631 1.222 1.52  0.692 0.663 0.868 0.62  0.448 0.821 0.549 0.753\n",
            " 1.478 0.    0.668 0.631 2.413 0.579 0.674 0.477 0.595 0.623 1.916 2.151\n",
            " 1.102 0.64  1.983 1.026 0.491 1.041 1.78  0.586 1.404 0.599 2.007 0.929\n",
            " 0.713 0.388 0.835 1.029 0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.   ]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.096\n",
            " 0.07  0.1   0.074 0.095 0.079 0.045 0.107 0.078 0.065 0.108 0.106 0.1\n",
            " 0.09  0.099 0.064 0.066 0.08  0.091 0.069 0.106 0.107 0.094 0.066 0.079\n",
            " 0.078 0.085 0.082 0.083 0.046 0.093 0.066 0.062 0.065 0.073 0.129 0.096\n",
            " 0.091 0.057 0.052 0.065 0.081 0.07  0.078 0.074 0.065 0.082 0.096 0.049\n",
            " 0.09  0.075 0.063 0.056 0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.   ]\n",
            "(100,)\n",
            "<class 'int'>\n",
            "280\n",
            "/content/gdrive/My Drive/thesis/Data/S01/S01_TEX06.txt\n",
            "1\n",
            "[0.645 1.164 0.765 0.573 0.74  0.615 0.85  0.671 0.422 4.672 0.723 1.115\n",
            " 0.489 1.172 0.538 0.727 0.884 0.485 1.154 0.774 1.074 2.535 1.043 0.464\n",
            " 0.548 0.715 1.194 1.554 1.153 2.446 0.72  2.562 1.121 0.752 1.069 1.584\n",
            " 0.547 0.591 0.732 4.413 0.873 0.679 1.035 0.746 0.438 1.216 0.773 2.502\n",
            " 0.812 0.989 1.214 0.453 0.869 2.542 0.736 0.802 0.659 0.741 0.706 0.718\n",
            " 0.68  0.955 3.127 0.58  0.752 0.871 0.363 0.699 0.498 1.001 0.512 0.606\n",
            " 1.909 0.771 0.362 0.69  1.404 0.415 0.737 0.85  0.675 0.689 1.541 0.647\n",
            " 0.68  1.112 0.817 0.53  0.518 0.825 1.542 1.678 1.018 0.841 0.911 2.077\n",
            " 0.655 0.39  1.822 0.729 2.484 1.074 0.915 0.815 0.376]\n",
            "[0.645 1.164 0.765 0.573 0.74  0.615 0.85  0.671 0.422 0.    0.723 1.115\n",
            " 0.489 1.172 0.538 0.727 0.884 0.485 1.154 0.774 1.074 2.535 1.043 0.464\n",
            " 0.548 0.715 1.194 1.554 1.153 2.446 0.72  2.562 1.121 0.752 1.069 1.584\n",
            " 0.547 0.591 0.732 0.    0.873 0.679 1.035 0.746 0.438 1.216 0.773 2.502\n",
            " 0.812 0.989 1.214 0.453 0.869 2.542 0.736 0.802 0.659 0.741 0.706 0.718\n",
            " 0.68  0.955 0.    0.58  0.752 0.871 0.363 0.699 0.498 1.001 0.512 0.606\n",
            " 1.909 0.771 0.362 0.69  1.404 0.415 0.737 0.85  0.675 0.689 1.541 0.647\n",
            " 0.68  1.112 0.817 0.53  0.518 0.825 1.542 1.678 1.018 0.841 0.911 2.077\n",
            " 0.655 0.39  1.822 0.729 2.484 1.074 0.915 0.815 0.376]\n",
            "[0.645 1.164 0.765 0.573 0.74  0.615 0.85  0.671 0.422 0.    0.723 1.115\n",
            " 0.489 1.172 0.538 0.727 0.884 0.485 1.154 0.774 1.074 2.535 1.043 0.464\n",
            " 0.548 0.715 1.194 1.554 1.153 2.446 0.72  2.562 1.121 0.752 1.069 1.584\n",
            " 0.547 0.591 0.732 0.    0.873 0.679 1.035 0.746 0.438 1.216 0.773 2.502\n",
            " 0.812 0.989 1.214 0.453 0.869 2.542 0.736 0.802 0.659 0.741 0.706 0.718\n",
            " 0.68  0.955 0.    0.58  0.752 0.871 0.363 0.699 0.498 1.001 0.512 0.606\n",
            " 1.909 0.771 0.362 0.69  1.404 0.415 0.737 0.85  0.675 0.689 1.541 0.647\n",
            " 0.68  1.112 0.817 0.53  0.518 0.825 1.542 1.678 1.018 0.841 0.911 2.077\n",
            " 0.655 0.39  1.822 0.729]\n",
            "[0.079 0.092 0.082 0.097 0.106 0.096 0.098 0.069 0.106 0.091 0.072 0.097\n",
            " 0.104 0.064 0.105 0.091 0.07  0.082 0.114 0.106 0.097 0.103 0.094 0.098\n",
            " 0.101 0.084 0.082 0.127 0.079 0.089 0.054 0.072 0.128 0.095 0.05  0.076\n",
            " 0.106 0.065 0.09  0.071 0.122 0.105 0.091 0.093 0.079 0.082 0.071 0.095\n",
            " 0.093 0.047 0.061 0.061 0.057 0.051 0.071 0.048 0.053 0.075 0.073 0.042\n",
            " 0.071 0.04  0.064 0.065 0.066 0.06  0.03  0.057 0.049 0.056 0.07  0.049\n",
            " 0.04  0.067 0.053 0.073 0.047 0.025 0.05  0.044 0.083 0.107 0.107 0.096\n",
            " 0.105 0.039 0.04  0.047 0.048 0.054 0.077 0.063 0.079 0.051 0.05  0.06\n",
            " 0.06  0.047 0.08  0.06 ]\n",
            "(100,)\n",
            "<class 'int'>\n",
            "281\n",
            "/content/gdrive/My Drive/thesis/Data/S01/S01_TEX11.txt\n",
            "1\n",
            "[0.698 0.614 0.81  1.188 0.553 0.668 0.478 0.735 1.938 0.519 0.44  1.008\n",
            " 0.58  0.574 0.947 1.511 0.757 1.095 0.589 0.639 1.884 1.699 0.688 1.857\n",
            " 1.479 0.571 0.581 0.565 1.081 4.31  0.533 0.608 1.106 0.555 0.439 0.933\n",
            " 1.721 0.957 0.66  0.894 1.243 1.106 1.138 0.548 0.59  0.622 1.458 1.094\n",
            " 0.697 1.206 1.083 0.69  4.007 0.578]\n",
            "[0.698 0.614 0.81  1.188 0.553 0.668 0.478 0.735 1.938 0.519 0.44  1.008\n",
            " 0.58  0.574 0.947 1.511 0.757 1.095 0.589 0.639 1.884 1.699 0.688 1.857\n",
            " 1.479 0.571 0.581 0.565 1.081 0.    0.533 0.608 1.106 0.555 0.439 0.933\n",
            " 1.721 0.957 0.66  0.894 1.243 1.106 1.138 0.548 0.59  0.622 1.458 1.094\n",
            " 0.697 1.206 1.083 0.69  0.    0.578]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.698\n",
            " 0.614 0.81  1.188 0.553 0.668 0.478 0.735 1.938 0.519 0.44  1.008 0.58\n",
            " 0.574 0.947 1.511 0.757 1.095 0.589 0.639 1.884 1.699 0.688 1.857 1.479\n",
            " 0.571 0.581 0.565 1.081 0.    0.533 0.608 1.106 0.555 0.439 0.933 1.721\n",
            " 0.957 0.66  0.894 1.243 1.106 1.138 0.548 0.59  0.622 1.458 1.094 0.697\n",
            " 1.206 1.083 0.69  0.    0.578 0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.   ]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.065 0.08\n",
            " 0.082 0.06  0.078 0.073 0.061 0.066 0.07  0.077 0.106 0.089 0.08  0.115\n",
            " 0.063 0.088 0.106 0.072 0.098 0.099 0.083 0.076 0.107 0.091 0.086 0.07\n",
            " 0.063 0.057 0.073 0.088 0.067 0.091 0.098 0.054 0.098 0.074 0.098 0.079\n",
            " 0.095 0.057 0.077 0.082 0.097 0.09  0.072 0.065 0.066 0.087 0.09  0.083\n",
            " 0.073 0.071 0.078 0.085 0.089 0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.   ]\n",
            "(100,)\n",
            "<class 'int'>\n",
            "282\n",
            "/content/gdrive/My Drive/thesis/Data/S01/S01_TEX08.txt\n",
            "1\n",
            "[ 1.127  0.815  0.849  0.816  0.88   0.737  0.593  3.591  2.186  0.635\n",
            "  1.251  1.248  0.686  0.959  0.539  1.155  3.134 10.028  0.844  0.646\n",
            "  1.358  0.681  0.77   0.375  0.698  0.784  0.71   0.435  0.823  1.466\n",
            "  0.603  0.631  0.65   0.69   0.689  0.764  1.2    0.695  2.523  0.714\n",
            "  0.738  0.563  1.574  1.463  0.704  0.884  0.389  0.547  0.457  0.673]\n",
            "[1.127 0.815 0.849 0.816 0.88  0.737 0.593 0.    2.186 0.635 1.251 1.248\n",
            " 0.686 0.959 0.539 1.155 0.    0.    0.844 0.646 1.358 0.681 0.77  0.375\n",
            " 0.698 0.784 0.71  0.435 0.823 1.466 0.603 0.631 0.65  0.69  0.689 0.764\n",
            " 1.2   0.695 2.523 0.714 0.738 0.563 1.574 1.463 0.704 0.884 0.389 0.547\n",
            " 0.457 0.673]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    1.127 0.815 0.849 0.816 0.88  0.737 0.593 0.    2.186 0.635 1.251\n",
            " 1.248 0.686 0.959 0.539 1.155 0.    0.    0.844 0.646 1.358 0.681 0.77\n",
            " 0.375 0.698 0.784 0.71  0.435 0.823 1.466 0.603 0.631 0.65  0.69  0.689\n",
            " 0.764 1.2   0.695 2.523 0.714 0.738 0.563 1.574 1.463 0.704 0.884 0.389\n",
            " 0.547 0.457 0.673 0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.   ]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.078 0.098 0.106 0.071 0.061 0.071 0.107 0.062 0.036 0.07  0.083 0.079\n",
            " 0.113 0.083 0.079 0.088 0.098 0.079 0.059 0.079 0.04  0.085 0.104 0.09\n",
            " 0.064 0.08  0.077 0.1   0.082 0.054 0.051 0.106 0.066 0.055 0.105 0.09\n",
            " 0.048 0.079 0.083 0.068 0.061 0.055 0.074 0.069 0.029 0.081 0.003 0.063\n",
            " 0.082 0.091 0.084 0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.   ]\n",
            "(100,)\n",
            "<class 'int'>\n",
            "283\n",
            "/content/gdrive/My Drive/thesis/Data/S01/S01_TEX04.txt\n",
            "1\n",
            "[0.415 0.866 0.596 0.775 4.313 0.78  0.694 1.165 0.531 0.323 2.301 1.732\n",
            " 1.543 0.789 0.555 2.039 0.709 1.098 0.514 0.656 0.944 0.661 0.646 0.593\n",
            " 1.22  1.191 0.378 0.69  0.665 4.24  0.478 1.324 0.898 1.719 0.811 1.087\n",
            " 1.147 0.705 1.18  0.841 0.537 0.909 0.943 1.67  0.64  0.674]\n",
            "[0.415 0.866 0.596 0.775 0.    0.78  0.694 1.165 0.531 0.323 2.301 1.732\n",
            " 1.543 0.789 0.555 2.039 0.709 1.098 0.514 0.656 0.944 0.661 0.646 0.593\n",
            " 1.22  1.191 0.378 0.69  0.665 0.    0.478 1.324 0.898 1.719 0.811 1.087\n",
            " 1.147 0.705 1.18  0.841 0.537 0.909 0.943 1.67  0.64  0.674]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.415 0.866 0.596 0.775 0.    0.78  0.694 1.165 0.531\n",
            " 0.323 2.301 1.732 1.543 0.789 0.555 2.039 0.709 1.098 0.514 0.656 0.944\n",
            " 0.661 0.646 0.593 1.22  1.191 0.378 0.69  0.665 0.    0.478 1.324 0.898\n",
            " 1.719 0.811 1.087 1.147 0.705 1.18  0.841 0.537 0.909 0.943 1.67  0.64\n",
            " 0.674 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.   ]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.09  0.091 0.089 0.099 0.079 0.095 0.079 0.065 0.115 0.065\n",
            " 0.09  0.089 0.077 0.087 0.081 0.073 0.086 0.099 0.13  0.091 0.074 0.069\n",
            " 0.089 0.084 0.104 0.098 0.103 0.091 0.09  0.114 0.094 0.073 0.054 0.095\n",
            " 0.087 0.064 0.076 0.124 0.083 0.107 0.105 0.133 0.095 0.091 0.099 0.082\n",
            " 0.106 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.   ]\n",
            "(100,)\n",
            "<class 'int'>\n",
            "284\n",
            "/content/gdrive/My Drive/thesis/Data/S01/S01_TEX02.txt\n",
            "1\n",
            "[0.561 0.457 1.672 0.373 0.851 0.585 0.674 1.475 1.658 0.883 0.662 0.565\n",
            " 0.64  1.183 0.669 0.709 0.514 0.85  0.763 3.099 0.748 1.191 0.756 1.729\n",
            " 0.511 1.85  2.259 1.138 0.57  1.197 2.405 0.789 0.713 1.757 0.846 0.653\n",
            " 0.731 1.4   1.252 1.457 0.578 0.616 0.756 0.468 1.206 0.678 2.289 0.511]\n",
            "[0.561 0.457 1.672 0.373 0.851 0.585 0.674 1.475 1.658 0.883 0.662 0.565\n",
            " 0.64  1.183 0.669 0.709 0.514 0.85  0.763 0.    0.748 1.191 0.756 1.729\n",
            " 0.511 1.85  2.259 1.138 0.57  1.197 2.405 0.789 0.713 1.757 0.846 0.653\n",
            " 0.731 1.4   1.252 1.457 0.578 0.616 0.756 0.468 1.206 0.678 2.289 0.511]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.561 0.457 1.672 0.373 0.851 0.585 0.674 1.475 1.658 0.883\n",
            " 0.662 0.565 0.64  1.183 0.669 0.709 0.514 0.85  0.763 0.    0.748 1.191\n",
            " 0.756 1.729 0.511 1.85  2.259 1.138 0.57  1.197 2.405 0.789 0.713 1.757\n",
            " 0.846 0.653 0.731 1.4   1.252 1.457 0.578 0.616 0.756 0.468 1.206 0.678\n",
            " 2.289 0.511 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.   ]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.102 0.098 0.081 0.08  0.106 0.077 0.073 0.083 0.085 0.082 0.112\n",
            " 0.074 0.082 0.081 0.087 0.109 0.098 0.066 0.123 0.073 0.107 0.048 0.088\n",
            " 0.054 0.07  0.074 0.077 0.053 0.062 0.1   0.09  0.054 0.096 0.096 0.069\n",
            " 0.07  0.099 0.074 0.078 0.107 0.096 0.108 0.09  0.073 0.087 0.086 0.081\n",
            " 0.119 0.098 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.   ]\n",
            "(100,)\n",
            "<class 'int'>\n",
            "285\n",
            "/content/gdrive/My Drive/thesis/Data/S02/S02_TEX04.txt\n",
            "2\n",
            "[ 0.564  0.81   5.086  1.068  6.269  1.137  4.332  0.771  2.551  0.536\n",
            "  1.907  2.897  0.934  1.108  2.862  0.919  1.355  3.42   1.032  0.69\n",
            "  3.714  2.312  2.353  3.184  0.95   1.101  0.953  3.271  2.862  0.399\n",
            "  0.706  3.756  0.747  1.159  0.452  0.732  1.079  2.488  4.041  1.02\n",
            "  1.325  0.816  2.302  1.7    1.621  7.983  0.529  3.436  2.589  1.13\n",
            "  1.034  1.117  3.077  0.669  0.837  1.     2.461  1.404  0.939  1.634\n",
            "  0.68   1.221  3.433  1.021  2.209  1.498  0.89   0.821  1.561  1.296\n",
            "  0.689  0.83   9.879  1.056  3.472  0.74   0.823  2.63   3.937  0.935\n",
            "  0.548  1.886  2.048  1.486 12.073  0.932  3.304  0.744  1.323  0.647\n",
            "  2.345  2.471  0.716  0.36 ]\n",
            "[0.564 0.81  0.    1.068 0.    1.137 0.    0.771 2.551 0.536 1.907 2.897\n",
            " 0.934 1.108 2.862 0.919 1.355 0.    1.032 0.69  0.    2.312 2.353 0.\n",
            " 0.95  1.101 0.953 0.    2.862 0.399 0.706 0.    0.747 1.159 0.452 0.732\n",
            " 1.079 2.488 0.    1.02  1.325 0.816 2.302 1.7   1.621 0.    0.529 0.\n",
            " 2.589 1.13  1.034 1.117 0.    0.669 0.837 1.    2.461 1.404 0.939 1.634\n",
            " 0.68  1.221 0.    1.021 2.209 1.498 0.89  0.821 1.561 1.296 0.689 0.83\n",
            " 0.    1.056 0.    0.74  0.823 2.63  0.    0.935 0.548 1.886 2.048 1.486\n",
            " 0.    0.932 0.    0.744 1.323 0.647 2.345 2.471 0.716 0.36 ]\n",
            "[0.    0.    0.    0.564 0.81  0.    1.068 0.    1.137 0.    0.771 2.551\n",
            " 0.536 1.907 2.897 0.934 1.108 2.862 0.919 1.355 0.    1.032 0.69  0.\n",
            " 2.312 2.353 0.    0.95  1.101 0.953 0.    2.862 0.399 0.706 0.    0.747\n",
            " 1.159 0.452 0.732 1.079 2.488 0.    1.02  1.325 0.816 2.302 1.7   1.621\n",
            " 0.    0.529 0.    2.589 1.13  1.034 1.117 0.    0.669 0.837 1.    2.461\n",
            " 1.404 0.939 1.634 0.68  1.221 0.    1.021 2.209 1.498 0.89  0.821 1.561\n",
            " 1.296 0.689 0.83  0.    1.056 0.    0.74  0.823 2.63  0.    0.935 0.548\n",
            " 1.886 2.048 1.486 0.    0.932 0.    0.744 1.323 0.647 2.345 2.471 0.716\n",
            " 0.36  0.    0.    0.   ]\n",
            "[0.    0.    0.095 0.104 0.091 0.14  0.094 0.139 0.141 0.119 0.122 0.104\n",
            " 0.082 0.137 0.114 0.144 0.133 0.104 0.098 0.122 0.109 0.097 0.156 0.13\n",
            " 0.129 0.11  0.141 0.12  0.133 0.068 0.104 0.116 0.107 0.133 0.124 0.098\n",
            " 0.136 0.116 0.098 0.083 0.137 0.117 0.102 0.107 0.138 0.103 0.089 0.155\n",
            " 0.089 0.083 0.12  0.107 0.123 0.102 0.115 0.12  0.107 0.093 0.081 0.129\n",
            " 0.119 0.103 0.122 0.124 0.181 0.129 0.132 0.149 0.08  0.129 0.131 0.131\n",
            " 0.124 0.124 0.1   0.101 0.149 0.091 0.141 0.099 0.092 0.11  0.122 0.106\n",
            " 0.123 0.134 0.059 0.135 0.142 0.112 0.149 0.106 0.064 0.128 0.091 0.116\n",
            " 0.079 0.    0.    0.   ]\n",
            "(100,)\n",
            "<class 'int'>\n",
            "286\n",
            "/content/gdrive/My Drive/thesis/Data/S02/S02_TEX08.txt\n",
            "2\n",
            "[0.557 1.273 2.067 6.757 2.149 1.297 1.147 1.588 3.148 0.68  1.093 4.593\n",
            " 1.576 3.165 0.927 1.568 2.284 0.857 1.659 4.09  2.162 1.176 0.762 0.733\n",
            " 3.849 1.996 0.653 0.902 1.625 4.101 1.013 0.53  0.809 3.754 0.645 0.566\n",
            " 0.698 2.82  1.6   0.849 0.998 2.56  0.772 2.963 2.369 5.461 2.395 2.865\n",
            " 3.831 0.727 0.474 1.282 1.152 1.566 2.027 3.038 1.273 1.853 2.572 1.704\n",
            " 2.327 1.188 2.613 0.826 0.927 4.8   0.456 0.785 3.254 1.807 1.336 0.696\n",
            " 2.279 0.661 8.792 1.489 0.405 1.414 4.626 0.933 1.994 1.421 1.667 0.856\n",
            " 0.811 0.768 2.405 0.794 1.005 0.474 2.71  0.53  0.827 1.434 1.72  2.488\n",
            " 2.157 1.066 1.503 0.558 2.062]\n",
            "[0.557 1.273 2.067 0.    2.149 1.297 1.147 1.588 0.    0.68  1.093 0.\n",
            " 1.576 0.    0.927 1.568 2.284 0.857 1.659 0.    2.162 1.176 0.762 0.733\n",
            " 0.    1.996 0.653 0.902 1.625 0.    1.013 0.53  0.809 0.    0.645 0.566\n",
            " 0.698 2.82  1.6   0.849 0.998 2.56  0.772 2.963 2.369 0.    2.395 2.865\n",
            " 0.    0.727 0.474 1.282 1.152 1.566 2.027 0.    1.273 1.853 2.572 1.704\n",
            " 2.327 1.188 2.613 0.826 0.927 0.    0.456 0.785 0.    1.807 1.336 0.696\n",
            " 2.279 0.661 0.    1.489 0.405 1.414 0.    0.933 1.994 1.421 1.667 0.856\n",
            " 0.811 0.768 2.405 0.794 1.005 0.474 2.71  0.53  0.827 1.434 1.72  2.488\n",
            " 2.157 1.066 1.503 0.558 2.062]\n",
            "[0.557 1.273 2.067 0.    2.149 1.297 1.147 1.588 0.    0.68  1.093 0.\n",
            " 1.576 0.    0.927 1.568 2.284 0.857 1.659 0.    2.162 1.176 0.762 0.733\n",
            " 0.    1.996 0.653 0.902 1.625 0.    1.013 0.53  0.809 0.    0.645 0.566\n",
            " 0.698 2.82  1.6   0.849 0.998 2.56  0.772 2.963 2.369 0.    2.395 2.865\n",
            " 0.    0.727 0.474 1.282 1.152 1.566 2.027 0.    1.273 1.853 2.572 1.704\n",
            " 2.327 1.188 2.613 0.826 0.927 0.    0.456 0.785 0.    1.807 1.336 0.696\n",
            " 2.279 0.661 0.    1.489 0.405 1.414 0.    0.933 1.994 1.421 1.667 0.856\n",
            " 0.811 0.768 2.405 0.794 1.005 0.474 2.71  0.53  0.827 1.434 1.72  2.488\n",
            " 2.157 1.066 1.503 0.558]\n",
            "[0.149 0.099 0.113 0.089 0.097 0.158 0.074 0.107 0.106 0.123 0.116 0.103\n",
            " 0.146 0.107 0.133 0.061 0.064 0.106 0.112 0.066 0.18  0.078 0.082 0.101\n",
            " 0.099 0.113 0.112 0.082 0.088 0.125 0.103 0.054 0.072 0.111 0.13  0.115\n",
            " 0.14  0.123 0.078 0.106 0.089 0.104 0.112 0.104 0.092 0.108 0.104 0.102\n",
            " 0.146 0.145 0.075 0.09  0.122 0.174 0.053 0.17  0.106 0.12  0.121 0.132\n",
            " 0.14  0.107 0.124 0.095 0.1   0.145 0.115 0.115 0.12  0.124 0.103 0.114\n",
            " 0.14  0.104 0.1   0.115 0.105 0.124 0.13  0.115 0.163 0.07  0.104 0.124\n",
            " 0.107 0.119 0.123 0.136 0.098 0.082 0.073 0.114 0.074 0.103 0.116 0.131\n",
            " 0.128 0.106 0.137 0.105]\n",
            "(100,)\n",
            "<class 'int'>\n",
            "287\n",
            "/content/gdrive/My Drive/thesis/Data/S02/S02_TEX10.txt\n",
            "2\n",
            "[0.946 1.345 0.579 2.086 3.374 1.314 0.814 0.829 1.339 0.421 4.779 0.677\n",
            " 0.391 1.282 4.444 0.387 0.814 2.787 0.821 0.653 2.005 1.005 5.915 0.788\n",
            " 0.759 2.736 3.318 2.416 0.963 2.458 1.638 0.627 0.628 2.967 0.904 3.584\n",
            " 1.004 2.195 1.18  0.756 3.117 2.18  0.662 0.73  0.784 4.48  1.132 1.027\n",
            " 1.767 0.878 3.077 1.264 1.991 0.954 0.91  3.322 0.899 0.548]\n",
            "[0.946 1.345 0.579 2.086 0.    1.314 0.814 0.829 1.339 0.421 0.    0.677\n",
            " 0.391 1.282 0.    0.387 0.814 2.787 0.821 0.653 2.005 1.005 0.    0.788\n",
            " 0.759 2.736 0.    2.416 0.963 2.458 1.638 0.627 0.628 2.967 0.904 0.\n",
            " 1.004 2.195 1.18  0.756 0.    2.18  0.662 0.73  0.784 0.    1.132 1.027\n",
            " 1.767 0.878 0.    1.264 1.991 0.954 0.91  0.    0.899 0.548]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.946 1.345 0.579\n",
            " 2.086 0.    1.314 0.814 0.829 1.339 0.421 0.    0.677 0.391 1.282 0.\n",
            " 0.387 0.814 2.787 0.821 0.653 2.005 1.005 0.    0.788 0.759 2.736 0.\n",
            " 2.416 0.963 2.458 1.638 0.627 0.628 2.967 0.904 0.    1.004 2.195 1.18\n",
            " 0.756 0.    2.18  0.662 0.73  0.784 0.    1.132 1.027 1.767 0.878 0.\n",
            " 1.264 1.991 0.954 0.91  0.    0.899 0.548 0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.   ]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.12  0.062 0.113 0.066\n",
            " 0.115 0.097 0.104 0.087 0.131 0.07  0.058 0.17  0.149 0.157 0.153 0.079\n",
            " 0.057 0.098 0.119 0.079 0.098 0.12  0.13  0.095 0.079 0.135 0.116 0.162\n",
            " 0.08  0.13  0.122 0.104 0.133 0.111 0.085 0.086 0.116 0.109 0.124 0.116\n",
            " 0.133 0.124 0.088 0.115 0.1   0.113 0.076 0.108 0.102 0.122 0.124 0.121\n",
            " 0.137 0.097 0.116 0.153 0.111 0.102 0.068 0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.   ]\n",
            "(100,)\n",
            "<class 'int'>\n",
            "288\n",
            "/content/gdrive/My Drive/thesis/Data/S02/S02_TEX06.txt\n",
            "2\n",
            "[2.045 2.388 0.692 0.504 0.421 1.871 1.125 1.    0.652 0.597 0.478 2.575\n",
            " 2.356 0.42  2.734 0.834 0.728 4.381 1.32  1.373 2.99  7.605 2.769 1.336\n",
            " 2.046 1.532 1.027 0.902 0.812 2.598 1.022 0.518 1.537 0.843 2.417 3.007\n",
            " 3.813 0.954 0.589 3.703 1.536 0.652 0.648 2.551 0.781 0.748 0.607 2.491\n",
            " 0.79  0.624 4.008 0.822 0.965 2.232 0.857 4.671 1.229 1.048 2.148 1.313\n",
            " 1.581 2.543 1.167 3.863 0.876 1.314 2.29  0.925 2.665 3.605 1.154 0.715\n",
            " 0.645 4.169 0.694 0.714 0.583 3.532 0.754 0.519 1.455 0.955 0.814 3.906\n",
            " 0.784 0.571 0.618 3.717 1.148 0.81  1.196 3.328 8.317 3.09  2.491 1.108\n",
            " 0.823 3.361 2.906 0.82  0.966 0.681 0.43  4.112 2.124 0.671 3.724 1.647\n",
            " 0.395 0.902 0.441 3.49  0.652 0.772 2.679 0.867 0.612 1.083 2.033]\n",
            "[2.045 2.388 0.692 0.504 0.421 1.871 1.125 1.    0.652 0.597 0.478 2.575\n",
            " 2.356 0.42  2.734 0.834 0.728 0.    1.32  1.373 2.99  0.    2.769 1.336\n",
            " 2.046 1.532 1.027 0.902 0.812 2.598 1.022 0.518 1.537 0.843 2.417 0.\n",
            " 0.    0.954 0.589 0.    1.536 0.652 0.648 2.551 0.781 0.748 0.607 2.491\n",
            " 0.79  0.624 0.    0.822 0.965 2.232 0.857 0.    1.229 1.048 2.148 1.313\n",
            " 1.581 2.543 1.167 0.    0.876 1.314 2.29  0.925 2.665 0.    1.154 0.715\n",
            " 0.645 0.    0.694 0.714 0.583 0.    0.754 0.519 1.455 0.955 0.814 0.\n",
            " 0.784 0.571 0.618 0.    1.148 0.81  1.196 0.    0.    0.    2.491 1.108\n",
            " 0.823 0.    2.906 0.82  0.966 0.681 0.43  0.    2.124 0.671 0.    1.647\n",
            " 0.395 0.902 0.441 0.    0.652 0.772 2.679 0.867 0.612 1.083 2.033]\n",
            "[2.045 2.388 0.692 0.504 0.421 1.871 1.125 1.    0.652 0.597 0.478 2.575\n",
            " 2.356 0.42  2.734 0.834 0.728 0.    1.32  1.373 2.99  0.    2.769 1.336\n",
            " 2.046 1.532 1.027 0.902 0.812 2.598 1.022 0.518 1.537 0.843 2.417 0.\n",
            " 0.    0.954 0.589 0.    1.536 0.652 0.648 2.551 0.781 0.748 0.607 2.491\n",
            " 0.79  0.624 0.    0.822 0.965 2.232 0.857 0.    1.229 1.048 2.148 1.313\n",
            " 1.581 2.543 1.167 0.    0.876 1.314 2.29  0.925 2.665 0.    1.154 0.715\n",
            " 0.645 0.    0.694 0.714 0.583 0.    0.754 0.519 1.455 0.955 0.814 0.\n",
            " 0.784 0.571 0.618 0.    1.148 0.81  1.196 0.    0.    0.    2.491 1.108\n",
            " 0.823 0.    2.906 0.82 ]\n",
            "[0.139 0.113 0.092 0.107 0.11  0.071 0.11  0.09  0.105 0.142 0.118 0.088\n",
            " 0.147 0.103 0.09  0.124 0.129 0.09  0.095 0.071 0.128 0.103 0.124 0.138\n",
            " 0.098 0.136 0.042 0.086 0.106 0.083 0.085 0.111 0.158 0.116 0.171 0.138\n",
            " 0.138 0.129 0.122 0.09  0.135 0.12  0.108 0.148 0.148 0.132 0.123 0.155\n",
            " 0.14  0.107 0.122 0.145 0.129 0.12  0.098 0.096 0.121 0.095 0.119 0.172\n",
            " 0.114 0.137 0.12  0.133 0.084 0.105 0.137 0.156 0.137 0.18  0.147 0.141\n",
            " 0.116 0.135 0.12  0.123 0.125 0.131 0.18  0.116 0.095 0.128 0.119 0.095\n",
            " 0.16  0.149 0.134 0.123 0.136 0.151 0.123 0.108 0.094 0.102 0.144 0.13\n",
            " 0.119 0.093 0.137 0.107]\n",
            "(100,)\n",
            "<class 'int'>\n",
            "289\n",
            "/content/gdrive/My Drive/thesis/Data/S02/S02_TEX03.txt\n",
            "2\n",
            "[ 2.407  4.51   1.23   1.865  1.571  1.25   3.039  0.413  0.903  2.906\n",
            "  1.217  0.66   1.419  2.484  1.304  1.994  0.786  1.73   0.825  0.718\n",
            "  4.854  0.945  1.164  0.703  2.391  3.035  1.108  5.147  3.119  2.836\n",
            "  1.234  3.806  3.436  4.99   3.969  1.119  1.163  1.063  2.979  1.126\n",
            "  0.81   2.991 13.329  3.365  1.756  1.553  0.931  4.296  1.102  1.137\n",
            "  0.342  0.957  2.207  3.596  1.124  0.859  0.838  1.784  0.849  0.506]\n",
            "[2.407 0.    1.23  1.865 1.571 1.25  0.    0.413 0.903 2.906 1.217 0.66\n",
            " 1.419 2.484 1.304 1.994 0.786 1.73  0.825 0.718 0.    0.945 1.164 0.703\n",
            " 2.391 0.    1.108 0.    0.    2.836 1.234 0.    0.    0.    0.    1.119\n",
            " 1.163 1.063 2.979 1.126 0.81  2.991 0.    0.    1.756 1.553 0.931 0.\n",
            " 1.102 1.137 0.342 0.957 2.207 0.    1.124 0.859 0.838 1.784 0.849 0.506]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    2.407 0.    1.23  1.865\n",
            " 1.571 1.25  0.    0.413 0.903 2.906 1.217 0.66  1.419 2.484 1.304 1.994\n",
            " 0.786 1.73  0.825 0.718 0.    0.945 1.164 0.703 2.391 0.    1.108 0.\n",
            " 0.    2.836 1.234 0.    0.    0.    0.    1.119 1.163 1.063 2.979 1.126\n",
            " 0.81  2.991 0.    0.    1.756 1.553 0.931 0.    1.102 1.137 0.342 0.957\n",
            " 2.207 0.    1.124 0.859 0.838 1.784 0.849 0.506 0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.   ]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.094 0.114 0.148 0.165 0.078\n",
            " 0.128 0.124 0.1   0.128 0.114 0.116 0.12  0.092 0.101 0.085 0.127 0.137\n",
            " 0.147 0.112 0.116 0.096 0.145 0.147 0.137 0.09  0.11  0.073 0.096 0.091\n",
            " 0.156 0.128 0.149 0.106 0.091 0.136 0.138 0.091 0.125 0.116 0.153 0.131\n",
            " 0.161 0.16  0.141 0.091 0.155 0.122 0.121 0.12  0.106 0.098 0.097 0.121\n",
            " 0.238 0.14  0.113 0.11  0.135 0.096 0.111 0.103 0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.   ]\n",
            "(100,)\n",
            "<class 'int'>\n",
            "290\n",
            "/content/gdrive/My Drive/thesis/Data/S02/S02_TEX02.txt\n",
            "2\n",
            "[0.885 1.065 1.058 1.318 2.802 1.656 0.506 1.215 0.713 1.699 3.156 0.924\n",
            " 4.978 3.836 0.858 2.756 0.787 0.629 1.218 0.626 0.636 1.127 3.69  0.983\n",
            " 1.883 0.435 1.282 2.537 4.784 0.39  0.331 0.308 0.233 0.249 0.241 0.232\n",
            " 0.431 2.967 0.963 0.943 2.674 1.017 0.803 1.43  0.718 0.749 1.508 4.571\n",
            " 1.272 1.12  3.775 1.82  0.914 0.778 1.246 3.533 2.968 0.895 1.303 7.053\n",
            " 0.854 0.755 0.888 1.94  9.828 0.637 0.502 0.514 0.53  1.672 4.025 0.457\n",
            " 0.509 4.845 5.559 4.958 4.102 1.516 1.208 1.289 3.858 2.832 5.779 0.825\n",
            " 3.613 8.322 0.757 0.824 0.597 0.765 6.466 1.234 2.062 2.714 2.577 0.413\n",
            " 2.852 2.285 2.277 0.8   2.207 0.866 5.8   2.67  2.769 0.846 1.206 5.663\n",
            " 2.601 1.589 1.052 0.46 ]\n",
            "[0.885 1.065 1.058 1.318 2.802 1.656 0.506 1.215 0.713 1.699 0.    0.924\n",
            " 0.    0.    0.858 2.756 0.787 0.629 1.218 0.626 0.636 1.127 0.    0.983\n",
            " 1.883 0.435 1.282 2.537 0.    0.39  0.331 0.308 0.233 0.249 0.241 0.232\n",
            " 0.431 2.967 0.963 0.943 2.674 1.017 0.803 1.43  0.718 0.749 1.508 0.\n",
            " 1.272 1.12  0.    1.82  0.914 0.778 1.246 0.    2.968 0.895 1.303 0.\n",
            " 0.854 0.755 0.888 1.94  0.    0.637 0.502 0.514 0.53  1.672 0.    0.457\n",
            " 0.509 0.    0.    0.    0.    1.516 1.208 1.289 0.    2.832 0.    0.825\n",
            " 0.    0.    0.757 0.824 0.597 0.765 0.    1.234 2.062 2.714 2.577 0.413\n",
            " 2.852 2.285 2.277 0.8   2.207 0.866 0.    2.67  2.769 0.846 1.206 0.\n",
            " 2.601 1.589 1.052 0.46 ]\n",
            "[0.885 1.065 1.058 1.318 2.802 1.656 0.506 1.215 0.713 1.699 0.    0.924\n",
            " 0.    0.    0.858 2.756 0.787 0.629 1.218 0.626 0.636 1.127 0.    0.983\n",
            " 1.883 0.435 1.282 2.537 0.    0.39  0.331 0.308 0.233 0.249 0.241 0.232\n",
            " 0.431 2.967 0.963 0.943 2.674 1.017 0.803 1.43  0.718 0.749 1.508 0.\n",
            " 1.272 1.12  0.    1.82  0.914 0.778 1.246 0.    2.968 0.895 1.303 0.\n",
            " 0.854 0.755 0.888 1.94  0.    0.637 0.502 0.514 0.53  1.672 0.    0.457\n",
            " 0.509 0.    0.    0.    0.    1.516 1.208 1.289 0.    2.832 0.    0.825\n",
            " 0.    0.    0.757 0.824 0.597 0.765 0.    1.234 2.062 2.714 2.577 0.413\n",
            " 2.852 2.285 2.277 0.8  ]\n",
            "[0.093 0.097 0.121 0.126 0.095 0.102 0.133 0.118 0.115 0.149 0.154 0.121\n",
            " 0.061 0.126 0.144 0.125 0.111 0.113 0.14  0.126 0.132 0.11  0.105 0.128\n",
            " 0.159 0.137 0.175 0.122 0.153 0.131 0.123 0.133 0.091 0.115 0.107 0.091\n",
            " 0.133 0.134 0.123 0.125 0.161 0.154 0.126 0.145 0.153 0.132 0.131 0.093\n",
            " 0.157 0.14  0.099 0.154 0.121 0.129 0.124 0.115 0.105 0.101 0.112 0.121\n",
            " 0.138 0.123 0.157 0.091 0.153 0.096 0.098 0.162 0.187 0.156 0.161 0.108\n",
            " 0.108 0.097 0.121 0.102 0.129 0.098 0.111 0.099 0.105 0.145 0.155 0.132\n",
            " 0.138 0.122 0.133 0.132 0.13  0.09  0.139 0.136 0.115 0.088 0.141 0.105\n",
            " 0.074 0.146 0.139 0.124]\n",
            "(100,)\n",
            "<class 'int'>\n",
            "291\n",
            "/content/gdrive/My Drive/thesis/Data/S02/S02_TEX09.txt\n",
            "2\n",
            "[0.479 0.775 1.052 1.358 2.149 4.54  1.792 3.231 2.251 0.921 3.378 1.736\n",
            " 0.838 0.814 0.661 3.419 1.082 0.477 0.647 0.54  2.246 1.255 0.644 3.385\n",
            " 1.125 0.424 0.665 5.993 1.758 0.773 2.652 2.227 0.737 0.663 3.219 0.811\n",
            " 0.802 0.569 1.058 2.884 0.678 0.988 1.704 2.708 2.437 0.445 0.325 0.689\n",
            " 2.841 1.085 1.133 0.943 2.998 3.033 0.712 2.044 4.879 0.899 0.495 2.944\n",
            " 1.305 1.36  2.82  0.922 1.065 0.515 4.117 1.15  0.522 0.918 0.595 1.205\n",
            " 2.111 0.716 2.623 1.162 1.139 0.901 6.218 1.13  0.942 3.642 1.72  0.77\n",
            " 1.118 3.038 0.959 2.41  1.473 1.218 2.654 1.203 0.687 0.789 4.097 0.448\n",
            " 1.499 1.62  1.47  0.887 0.738 1.645]\n",
            "[0.479 0.775 1.052 1.358 2.149 0.    1.792 0.    2.251 0.921 0.    1.736\n",
            " 0.838 0.814 0.661 0.    1.082 0.477 0.647 0.54  2.246 1.255 0.644 0.\n",
            " 1.125 0.424 0.665 0.    1.758 0.773 2.652 2.227 0.737 0.663 0.    0.811\n",
            " 0.802 0.569 1.058 2.884 0.678 0.988 1.704 2.708 2.437 0.445 0.325 0.689\n",
            " 2.841 1.085 1.133 0.943 2.998 0.    0.712 2.044 0.    0.899 0.495 2.944\n",
            " 1.305 1.36  2.82  0.922 1.065 0.515 0.    1.15  0.522 0.918 0.595 1.205\n",
            " 2.111 0.716 2.623 1.162 1.139 0.901 0.    1.13  0.942 0.    1.72  0.77\n",
            " 1.118 0.    0.959 2.41  1.473 1.218 2.654 1.203 0.687 0.789 0.    0.448\n",
            " 1.499 1.62  1.47  0.887 0.738 1.645]\n",
            "[0.479 0.775 1.052 1.358 2.149 0.    1.792 0.    2.251 0.921 0.    1.736\n",
            " 0.838 0.814 0.661 0.    1.082 0.477 0.647 0.54  2.246 1.255 0.644 0.\n",
            " 1.125 0.424 0.665 0.    1.758 0.773 2.652 2.227 0.737 0.663 0.    0.811\n",
            " 0.802 0.569 1.058 2.884 0.678 0.988 1.704 2.708 2.437 0.445 0.325 0.689\n",
            " 2.841 1.085 1.133 0.943 2.998 0.    0.712 2.044 0.    0.899 0.495 2.944\n",
            " 1.305 1.36  2.82  0.922 1.065 0.515 0.    1.15  0.522 0.918 0.595 1.205\n",
            " 2.111 0.716 2.623 1.162 1.139 0.901 0.    1.13  0.942 0.    1.72  0.77\n",
            " 1.118 0.    0.959 2.41  1.473 1.218 2.654 1.203 0.687 0.789 0.    0.448\n",
            " 1.499 1.62  1.47  0.887]\n",
            "[0.128 0.098 0.087 0.074 0.121 0.089 0.119 0.115 0.115 0.074 0.116 0.111\n",
            " 0.12  0.087 0.095 0.099 0.136 0.093 0.107 0.073 0.065 0.112 0.096 0.139\n",
            " 0.077 0.124 0.091 0.115 0.129 0.124 0.131 0.18  0.146 0.098 0.099 0.112\n",
            " 0.107 0.111 0.082 0.087 0.104 0.132 0.091 0.107 0.181 0.111 0.083 0.073\n",
            " 0.124 0.107 0.085 0.1   0.104 0.104 0.113 0.116 0.124 0.13  0.086 0.107\n",
            " 0.12  0.119 0.105 0.119 0.095 0.069 0.07  0.083 0.122 0.106 0.095 0.114\n",
            " 0.082 0.081 0.087 0.107 0.066 0.116 0.112 0.124 0.116 0.079 0.118 0.075\n",
            " 0.062 0.107 0.085 0.157 0.081 0.087 0.149 0.103 0.121 0.082 0.074 0.099\n",
            " 0.091 0.087 0.078 0.095]\n",
            "(100,)\n",
            "<class 'int'>\n",
            "292\n",
            "/content/gdrive/My Drive/thesis/Data/S02/S02_TEX07.txt\n",
            "2\n",
            "[0.736 0.658 2.237 0.496 0.999 3.474 0.512 0.657 1.391 1.502 2.322 2.549\n",
            " 0.901 0.968 0.896 0.82  5.799 1.827 0.63  1.222 0.846 7.036 1.903 1.26\n",
            " 1.287 1.928 0.966 0.704 2.488 0.972 2.516 0.79  2.628 0.683 2.53  0.939\n",
            " 0.752 0.698 4.055 1.373 2.387 1.449 1.089 0.791 1.823 0.731 1.046 2.8\n",
            " 1.137 5.365 2.281 0.919 0.935 0.589 3.352 1.12  1.303 2.135 0.949 1.904\n",
            " 1.346 4.244 3.052 1.96  0.77  0.658 4.464 2.535 3.196 0.491 1.905 0.863\n",
            " 3.156 6.348 0.813 1.806 2.159 1.027 0.808 0.645 2.489 0.643 0.793 1.027\n",
            " 0.532 0.607 3.9   0.76 ]\n",
            "[0.736 0.658 2.237 0.496 0.999 0.    0.512 0.657 1.391 1.502 2.322 2.549\n",
            " 0.901 0.968 0.896 0.82  0.    1.827 0.63  1.222 0.846 0.    1.903 1.26\n",
            " 1.287 1.928 0.966 0.704 2.488 0.972 2.516 0.79  2.628 0.683 2.53  0.939\n",
            " 0.752 0.698 0.    1.373 2.387 1.449 1.089 0.791 1.823 0.731 1.046 2.8\n",
            " 1.137 0.    2.281 0.919 0.935 0.589 0.    1.12  1.303 2.135 0.949 1.904\n",
            " 1.346 0.    0.    1.96  0.77  0.658 0.    2.535 0.    0.491 1.905 0.863\n",
            " 0.    0.    0.813 1.806 2.159 1.027 0.808 0.645 2.489 0.643 0.793 1.027\n",
            " 0.532 0.607 0.    0.76 ]\n",
            "[0.    0.    0.    0.    0.    0.    0.736 0.658 2.237 0.496 0.999 0.\n",
            " 0.512 0.657 1.391 1.502 2.322 2.549 0.901 0.968 0.896 0.82  0.    1.827\n",
            " 0.63  1.222 0.846 0.    1.903 1.26  1.287 1.928 0.966 0.704 2.488 0.972\n",
            " 2.516 0.79  2.628 0.683 2.53  0.939 0.752 0.698 0.    1.373 2.387 1.449\n",
            " 1.089 0.791 1.823 0.731 1.046 2.8   1.137 0.    2.281 0.919 0.935 0.589\n",
            " 0.    1.12  1.303 2.135 0.949 1.904 1.346 0.    0.    1.96  0.77  0.658\n",
            " 0.    2.535 0.    0.491 1.905 0.863 0.    0.    0.813 1.806 2.159 1.027\n",
            " 0.808 0.645 2.489 0.643 0.793 1.027 0.532 0.607 0.    0.76  0.    0.\n",
            " 0.    0.    0.    0.   ]\n",
            "[0.    0.    0.    0.    0.    0.086 0.14  0.129 0.104 0.081 0.08  0.096\n",
            " 0.092 0.091 0.113 0.116 0.138 0.115 0.112 0.1   0.102 0.087 0.114 0.09\n",
            " 0.117 0.124 0.134 0.12  0.112 0.057 0.124 0.107 0.088 0.099 0.12  0.103\n",
            " 0.096 0.104 0.093 0.116 0.137 0.103 0.082 0.124 0.15  0.148 0.137 0.075\n",
            " 0.1   0.114 0.077 0.111 0.137 0.114 0.124 0.127 0.107 0.134 0.114 0.156\n",
            " 0.12  0.096 0.115 0.115 0.105 0.086 0.112 0.065 0.129 0.138 0.133 0.147\n",
            " 0.137 0.145 0.124 0.098 0.121 0.105 0.106 0.122 0.131 0.119 0.112 0.098\n",
            " 0.096 0.115 0.127 0.108 0.112 0.141 0.124 0.099 0.103 0.099 0.    0.\n",
            " 0.    0.    0.    0.   ]\n",
            "(100,)\n",
            "<class 'int'>\n",
            "293\n",
            "/content/gdrive/My Drive/thesis/Data/S02/S02_TEX05.txt\n",
            "2\n",
            "[0.967 0.712 2.444 1.718 0.992 0.879 2.561 0.961 0.645 1.147 0.528 2.477\n",
            " 1.471 1.101 3.248 1.621 2.321 2.131 1.96  1.064 1.079 1.273 0.971 0.801\n",
            " 1.911 2.85  0.677 1.738 1.031 1.64  0.637 3.623 2.057 0.884 1.069 0.789\n",
            " 2.45  1.072 0.798 1.79  2.883 0.539 0.918 3.861 0.949 1.129 0.707 4.298\n",
            " 1.261 1.871 0.936 3.148 0.495 3.137 0.891 1.127 3.326 1.286 7.279 2.8\n",
            " 2.311]\n",
            "[0.967 0.712 2.444 1.718 0.992 0.879 2.561 0.961 0.645 1.147 0.528 2.477\n",
            " 1.471 1.101 0.    1.621 2.321 2.131 1.96  1.064 1.079 1.273 0.971 0.801\n",
            " 1.911 2.85  0.677 1.738 1.031 1.64  0.637 0.    2.057 0.884 1.069 0.789\n",
            " 2.45  1.072 0.798 1.79  2.883 0.539 0.918 0.    0.949 1.129 0.707 0.\n",
            " 1.261 1.871 0.936 0.    0.495 0.    0.891 1.127 0.    1.286 0.    2.8\n",
            " 2.311]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.967 0.712 2.444 1.718 0.992\n",
            " 0.879 2.561 0.961 0.645 1.147 0.528 2.477 1.471 1.101 0.    1.621 2.321\n",
            " 2.131 1.96  1.064 1.079 1.273 0.971 0.801 1.911 2.85  0.677 1.738 1.031\n",
            " 1.64  0.637 0.    2.057 0.884 1.069 0.789 2.45  1.072 0.798 1.79  2.883\n",
            " 0.539 0.918 0.    0.949 1.129 0.707 0.    1.261 1.871 0.936 0.    0.495\n",
            " 0.    0.891 1.127 0.    1.286 0.    2.8   2.311 0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.   ]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.182 0.096 0.123 0.129 0.132\n",
            " 0.128 0.112 0.11  0.096 0.091 0.082 0.119 0.119 0.135 0.124 0.123 0.131\n",
            " 0.128 0.174 0.091 0.091 0.15  0.114 0.141 0.121 0.146 0.129 0.117 0.098\n",
            " 0.105 0.127 0.097 0.163 0.116 0.095 0.156 0.115 0.124 0.066 0.115 0.145\n",
            " 0.128 0.071 0.158 0.11  0.125 0.109 0.183 0.138 0.158 0.114 0.175 0.101\n",
            " 0.096 0.116 0.132 0.142 0.156 0.124 0.114 0.13  0.112 0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.   ]\n",
            "(100,)\n",
            "<class 'int'>\n",
            "294\n",
            "/content/gdrive/My Drive/thesis/Data/S02/S02_TEX11.txt\n",
            "2\n",
            "[0.783 0.931 0.678 1.366 2.351 1.919 2.19  1.092 0.904 0.757 2.841 0.788\n",
            " 1.658 3.161 0.871 1.709 4.449 0.78  0.898 2.048 2.229 0.571 0.623 3.3\n",
            " 1.023 2.468 1.143 1.025 2.625 1.65  2.878 1.106 0.755 1.237 1.291 0.695\n",
            " 3.503 0.603 3.958 0.829 0.496 0.383 2.458 0.942 0.603 0.648 3.28  0.708\n",
            " 0.666 0.722 0.495 0.533 1.215 1.427]\n",
            "[0.783 0.931 0.678 1.366 2.351 1.919 2.19  1.092 0.904 0.757 2.841 0.788\n",
            " 1.658 0.    0.871 1.709 0.    0.78  0.898 2.048 2.229 0.571 0.623 0.\n",
            " 1.023 2.468 1.143 1.025 2.625 1.65  2.878 1.106 0.755 1.237 1.291 0.695\n",
            " 0.    0.603 0.    0.829 0.496 0.383 2.458 0.942 0.603 0.648 0.    0.708\n",
            " 0.666 0.722 0.495 0.533 1.215 1.427]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.783\n",
            " 0.931 0.678 1.366 2.351 1.919 2.19  1.092 0.904 0.757 2.841 0.788 1.658\n",
            " 0.    0.871 1.709 0.    0.78  0.898 2.048 2.229 0.571 0.623 0.    1.023\n",
            " 2.468 1.143 1.025 2.625 1.65  2.878 1.106 0.755 1.237 1.291 0.695 0.\n",
            " 0.603 0.    0.829 0.496 0.383 2.458 0.942 0.603 0.648 0.    0.708 0.666\n",
            " 0.722 0.495 0.533 1.215 1.427 0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.   ]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.123 0.104\n",
            " 0.121 0.123 0.079 0.146 0.153 0.123 0.112 0.096 0.145 0.103 0.113 0.141\n",
            " 0.095 0.121 0.167 0.094 0.128 0.095 0.106 0.079 0.107 0.106 0.13  0.112\n",
            " 0.102 0.107 0.119 0.119 0.181 0.103 0.094 0.13  0.097 0.113 0.125 0.135\n",
            " 0.156 0.079 0.112 0.108 0.114 0.14  0.07  0.141 0.091 0.126 0.124 0.115\n",
            " 0.149 0.135 0.125 0.114 0.134 0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.   ]\n",
            "(100,)\n",
            "<class 'int'>\n",
            "295\n",
            "/content/gdrive/My Drive/thesis/Data/S03/S03_TEX09.txt\n",
            "3\n",
            "[ 0.928  0.244  0.95   1.021 10.501  0.253  0.216  0.225  0.312  2.301\n",
            "  1.604  2.25   1.617  7.791  0.232  3.777  0.963  0.787  0.174  3.194\n",
            "  1.135  0.115  1.6    1.262  1.356  3.648  1.228  1.062  2.527  0.209\n",
            "  3.44   1.172  1.326  0.838  1.244  0.266  4.696  0.173  3.907  0.604\n",
            "  0.458  0.292  0.232  2.338  1.498  0.145  5.434  1.077  1.285  1.036\n",
            "  0.853  0.236  4.407  1.52   0.921  0.821  2.071  4.184  1.29   1.148\n",
            "  0.832  1.298  0.614  1.079  4.891  1.469  1.368  2.551  0.27   3.076\n",
            "  0.958  0.872  5.833  1.129  1.891  2.86   3.817  2.653  1.03   0.52\n",
            "  0.924  0.496  3.594  0.799  1.568  0.265  4.233  2.877  4.105  0.561\n",
            "  2.34   1.326  0.539  1.656  3.094  1.717  1.456  1.468  0.241  0.357]\n",
            "[0.928 0.244 0.95  1.021 0.    0.253 0.216 0.225 0.312 2.301 1.604 2.25\n",
            " 1.617 0.    0.232 0.    0.963 0.787 0.174 0.    1.135 0.115 1.6   1.262\n",
            " 1.356 0.    1.228 1.062 2.527 0.209 0.    1.172 1.326 0.838 1.244 0.266\n",
            " 0.    0.173 0.    0.604 0.458 0.292 0.232 2.338 1.498 0.145 0.    1.077\n",
            " 1.285 1.036 0.853 0.236 0.    1.52  0.921 0.821 2.071 0.    1.29  1.148\n",
            " 0.832 1.298 0.614 1.079 0.    1.469 1.368 2.551 0.27  0.    0.958 0.872\n",
            " 0.    1.129 1.891 2.86  0.    2.653 1.03  0.52  0.924 0.496 0.    0.799\n",
            " 1.568 0.265 0.    2.877 0.    0.561 2.34  1.326 0.539 1.656 0.    1.717\n",
            " 1.456 1.468 0.241 0.357]\n",
            "[0.928 0.244 0.95  1.021 0.    0.253 0.216 0.225 0.312 2.301 1.604 2.25\n",
            " 1.617 0.    0.232 0.    0.963 0.787 0.174 0.    1.135 0.115 1.6   1.262\n",
            " 1.356 0.    1.228 1.062 2.527 0.209 0.    1.172 1.326 0.838 1.244 0.266\n",
            " 0.    0.173 0.    0.604 0.458 0.292 0.232 2.338 1.498 0.145 0.    1.077\n",
            " 1.285 1.036 0.853 0.236 0.    1.52  0.921 0.821 2.071 0.    1.29  1.148\n",
            " 0.832 1.298 0.614 1.079 0.    1.469 1.368 2.551 0.27  0.    0.958 0.872\n",
            " 0.    1.129 1.891 2.86  0.    2.653 1.03  0.52  0.924 0.496 0.    0.799\n",
            " 1.568 0.265 0.    2.877 0.    0.561 2.34  1.326 0.539 1.656 0.    1.717\n",
            " 1.456 1.468 0.241 0.357]\n",
            "[0.183 0.078 0.092 0.123 0.073 0.102 0.074 0.075 0.065 0.035 0.07  0.103\n",
            " 0.14  0.101 0.097 0.073 0.077 0.12  0.081 0.098 0.105 0.066 0.134 0.096\n",
            " 0.097 0.104 0.122 0.098 0.092 0.09  0.09  0.09  0.132 0.111 0.12  0.048\n",
            " 0.107 0.079 0.107 0.055 0.191 0.141 0.074 0.116 0.096 0.034 0.098 0.083\n",
            " 0.094 0.098 0.118 0.129 0.109 0.097 0.117 0.093 0.094 0.11  0.132 0.114\n",
            " 0.105 0.095 0.095 0.103 0.122 0.075 0.085 0.096 0.129 0.117 0.073 0.112\n",
            " 0.078 0.111 0.112 0.116 0.107 0.113 0.119 0.095 0.073 0.096 0.108 0.123\n",
            " 0.078 0.123 0.099 0.112 0.094 0.136 0.107 0.111 0.14  0.089 0.088 0.103\n",
            " 0.172 0.112 0.098 0.082]\n",
            "(100,)\n",
            "<class 'int'>\n",
            "296\n",
            "/content/gdrive/My Drive/thesis/Data/S03/S03_TEX10.txt\n",
            "3\n",
            "[1.169 0.459 0.19  0.69  0.44  5.011 0.745 0.667 2.45  1.56  0.228 0.54\n",
            " 1.991 0.212 1.416 1.082 0.169 0.708 2.553 0.247 0.656 2.593 5.157 1.228\n",
            " 0.841 1.122 0.235 9.77  3.567 0.629 1.125 0.282 3.807 0.781 1.083 0.842\n",
            " 0.964 1.061 6.02  1.14  1.093 0.726 1.157 1.059 1.607 2.451 1.19  1.344]\n",
            "[1.169 0.459 0.19  0.69  0.44  0.    0.745 0.667 2.45  1.56  0.228 0.54\n",
            " 1.991 0.212 1.416 1.082 0.169 0.708 2.553 0.247 0.656 2.593 0.    1.228\n",
            " 0.841 1.122 0.235 0.    0.    0.629 1.125 0.282 0.    0.781 1.083 0.842\n",
            " 0.964 1.061 0.    1.14  1.093 0.726 1.157 1.059 1.607 2.451 1.19  1.344]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    1.169 0.459 0.19  0.69  0.44  0.    0.745 0.667 2.45  1.56\n",
            " 0.228 0.54  1.991 0.212 1.416 1.082 0.169 0.708 2.553 0.247 0.656 2.593\n",
            " 0.    1.228 0.841 1.122 0.235 0.    0.    0.629 1.125 0.282 0.    0.781\n",
            " 1.083 0.842 0.964 1.061 0.    1.14  1.093 0.726 1.157 1.059 1.607 2.451\n",
            " 1.19  1.344 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.   ]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.13  0.117 0.023 0.091 0.148 0.116 0.098 0.109 0.116 0.1   0.104\n",
            " 0.091 0.031 0.031 0.141 0.104 0.043 0.091 0.09  0.086 0.547 0.057 0.164\n",
            " 0.084 0.103 0.094 0.11  0.107 0.093 0.133 0.085 0.149 0.133 0.156 0.108\n",
            " 0.087 0.101 0.068 0.061 0.051 0.141 0.102 0.15  0.132 0.127 0.15  0.075\n",
            " 0.09  0.1   0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.   ]\n",
            "(100,)\n",
            "<class 'int'>\n",
            "297\n",
            "/content/gdrive/My Drive/thesis/Data/S03/S03_TEX03.txt\n",
            "3\n",
            "[1.921 5.756 0.66  4.369 3.469 1.202 0.548 2.556 0.435 0.538 0.862 0.436\n",
            " 4.571 1.533 1.549 1.114 1.188 2.177 3.295 0.985 0.975 0.737 0.446 3.635\n",
            " 0.854 0.78  0.227 2.942 2.2   1.163 1.222 0.222 2.507 1.801 1.7   2.063\n",
            " 3.372 0.66  2.573 0.242 1.545 1.109 0.516]\n",
            "[1.921 0.    0.66  0.    0.    1.202 0.548 2.556 0.435 0.538 0.862 0.436\n",
            " 0.    1.533 1.549 1.114 1.188 2.177 0.    0.985 0.975 0.737 0.446 0.\n",
            " 0.854 0.78  0.227 2.942 2.2   1.163 1.222 0.222 2.507 1.801 1.7   2.063\n",
            " 0.    0.66  2.573 0.242 1.545 1.109 0.516]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    1.921 0.    0.66  0.    0.    1.202 0.548 2.556\n",
            " 0.435 0.538 0.862 0.436 0.    1.533 1.549 1.114 1.188 2.177 0.    0.985\n",
            " 0.975 0.737 0.446 0.    0.854 0.78  0.227 2.942 2.2   1.163 1.222 0.222\n",
            " 2.507 1.801 1.7   2.063 0.    0.66  2.573 0.242 1.545 1.109 0.516 0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.   ]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.121 0.076 0.112 0.126 0.093 0.114 0.099 0.082\n",
            " 0.101 0.107 0.068 0.111 0.117 0.073 0.044 0.084 0.116 0.099 0.108 0.061\n",
            " 0.098 0.094 0.147 0.116 0.08  0.065 0.083 0.107 0.097 0.048 0.05  0.122\n",
            " 0.099 0.102 0.12  0.105 0.094 0.095 0.081 0.109 0.1   0.099 0.113 0.061\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.   ]\n",
            "(100,)\n",
            "<class 'int'>\n",
            "298\n",
            "/content/gdrive/My Drive/thesis/Data/S03/S03_TEX05.txt\n",
            "3\n",
            "[1.211 2.804 1.445 1.627 1.645 0.78  2.128 0.216 3.035 3.244 1.487 1.966\n",
            " 2.604 0.605 0.991 0.282 6.31  0.621 0.756 0.199 3.345 1.413 2.791 2.608\n",
            " 1.34  1.202 1.103 0.743 0.246 3.601 1.054 2.022 1.669 1.467 3.189 5.699\n",
            " 0.517 0.837 0.217 6.166 1.159 0.463 1.878 0.646 0.225 1.403 0.532 0.842\n",
            " 0.213 2.82  1.322 1.269 0.807 0.315 1.007 2.168 6.685 5.16  1.236 0.216\n",
            " 3.194 0.189]\n",
            "[1.211 2.804 1.445 1.627 1.645 0.78  2.128 0.216 0.    0.    1.487 1.966\n",
            " 2.604 0.605 0.991 0.282 0.    0.621 0.756 0.199 0.    1.413 2.791 2.608\n",
            " 1.34  1.202 1.103 0.743 0.246 0.    1.054 2.022 1.669 1.467 0.    0.\n",
            " 0.517 0.837 0.217 0.    1.159 0.463 1.878 0.646 0.225 1.403 0.532 0.842\n",
            " 0.213 2.82  1.322 1.269 0.807 0.315 1.007 2.168 0.    0.    1.236 0.216\n",
            " 0.    0.189]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    1.211 2.804 1.445 1.627 1.645\n",
            " 0.78  2.128 0.216 0.    0.    1.487 1.966 2.604 0.605 0.991 0.282 0.\n",
            " 0.621 0.756 0.199 0.    1.413 2.791 2.608 1.34  1.202 1.103 0.743 0.246\n",
            " 0.    1.054 2.022 1.669 1.467 0.    0.    0.517 0.837 0.217 0.    1.159\n",
            " 0.463 1.878 0.646 0.225 1.403 0.532 0.842 0.213 2.82  1.322 1.269 0.807\n",
            " 0.315 1.007 2.168 0.    0.    1.236 0.216 0.    0.189 0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.   ]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.104 0.089 0.111 0.252 0.113 0.097\n",
            " 0.082 0.115 0.097 0.106 0.106 0.024 0.06  0.107 0.084 0.107 0.058 0.112\n",
            " 0.089 0.09  0.099 0.102 0.086 0.112 0.114 0.093 0.114 0.109 0.114 0.1\n",
            " 0.089 0.098 0.087 0.124 0.069 0.097 0.098 0.08  0.09  0.106 0.148 0.119\n",
            " 0.129 0.079 0.106 0.082 0.074 0.082 0.104 0.083 0.063 0.113 0.099 0.131\n",
            " 0.123 0.048 0.089 0.087 0.164 0.091 0.065 0.07  0.073 0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.   ]\n",
            "(100,)\n",
            "<class 'int'>\n",
            "299\n",
            "/content/gdrive/My Drive/thesis/Data/S03/S03_TEX08.txt\n",
            "3\n",
            "[ 0.873  1.37   4.854  1.234  0.858  0.215  3.058  1.289  0.843  0.869\n",
            "  0.249  5.157  0.94   0.626  0.782  0.192  2.931  1.257  1.282  0.672\n",
            "  5.713  1.362  1.128  2.119  2.677  0.771  2.097  4.856  2.594  0.185\n",
            "  1.114  1.198  0.86   0.878  0.205  5.327  1.782  0.19   1.527  0.702\n",
            "  1.009  1.899  0.849  6.899  1.88   0.807  1.001  0.214 12.771  1.74\n",
            "  1.199  0.714  0.232  6.482  1.689  3.134  1.314  1.847  1.534  1.823\n",
            "  6.05   0.929  0.355  0.847  0.199  2.503  1.518  5.519  1.154  0.939\n",
            "  0.915  4.647  0.413  1.13   1.671  1.823  0.548  1.281  1.495]\n",
            "[0.873 1.37  0.    1.234 0.858 0.215 0.    1.289 0.843 0.869 0.249 0.\n",
            " 0.94  0.626 0.782 0.192 2.931 1.257 1.282 0.672 0.    1.362 1.128 2.119\n",
            " 2.677 0.771 2.097 0.    2.594 0.185 1.114 1.198 0.86  0.878 0.205 0.\n",
            " 1.782 0.19  1.527 0.702 1.009 1.899 0.849 0.    1.88  0.807 1.001 0.214\n",
            " 0.    1.74  1.199 0.714 0.232 0.    1.689 0.    1.314 1.847 1.534 1.823\n",
            " 0.    0.929 0.355 0.847 0.199 2.503 1.518 0.    1.154 0.939 0.915 0.\n",
            " 0.413 1.13  1.671 1.823 0.548 1.281 1.495]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.873 1.37\n",
            " 0.    1.234 0.858 0.215 0.    1.289 0.843 0.869 0.249 0.    0.94  0.626\n",
            " 0.782 0.192 2.931 1.257 1.282 0.672 0.    1.362 1.128 2.119 2.677 0.771\n",
            " 2.097 0.    2.594 0.185 1.114 1.198 0.86  0.878 0.205 0.    1.782 0.19\n",
            " 1.527 0.702 1.009 1.899 0.849 0.    1.88  0.807 1.001 0.214 0.    1.74\n",
            " 1.199 0.714 0.232 0.    1.689 0.    1.314 1.847 1.534 1.823 0.    0.929\n",
            " 0.355 0.847 0.199 2.503 1.518 0.    1.154 0.939 0.915 0.    0.413 1.13\n",
            " 1.671 1.823 0.548 1.281 1.495 0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.   ]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.105 0.203\n",
            " 0.13  0.113 0.108 0.09  0.09  0.257 0.124 0.103 0.107 0.081 0.078 0.094\n",
            " 0.092 0.082 0.066 0.1   0.107 0.097 0.289 0.096 0.088 0.124 0.09  0.098\n",
            " 0.126 0.063 0.096 0.075 0.082 0.082 0.124 0.077 0.079 0.082 0.109 0.033\n",
            " 0.1   0.035 0.106 0.095 0.124 0.122 0.087 0.102 0.094 0.115 0.099 0.194\n",
            " 0.092 0.09  0.107 0.091 0.117 0.167 0.106 0.121 0.119 0.081 0.085 0.095\n",
            " 0.155 0.073 0.099 0.098 0.122 0.107 0.082 0.1   0.108 0.115 0.105 0.116\n",
            " 0.074 0.114 0.112 0.137 0.103 0.069 0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.   ]\n",
            "(100,)\n",
            "<class 'int'>\n",
            "300\n",
            "/content/gdrive/My Drive/thesis/Data/S03/S03_TEX06.txt\n",
            "3\n",
            "[ 1.069  0.893  0.872  1.078  0.241  5.3    1.353  3.763  0.72   2.463\n",
            "  2.914  0.6    1.686  0.886  3.594  1.433  1.065  1.268  3.015  1.133\n",
            "  0.912  0.8    0.179  2.522  2.201  2.424  1.109  1.651  0.443  1.139\n",
            "  1.056  0.268  6.28   0.742  0.715  0.716  0.207  2.561  0.877  1.028\n",
            "  0.214  2.827  1.392  0.956  1.411  0.886  7.42   2.966 10.627  1.259\n",
            "  1.449  1.921  3.908  0.88   0.88   1.447  0.275  4.407  1.76   1.267\n",
            "  1.459  3.877  0.22   0.241  3.851  0.879  1.474  2.744  1.894  1.343\n",
            "  0.297]\n",
            "[1.069 0.893 0.872 1.078 0.241 0.    1.353 0.    0.72  2.463 2.914 0.6\n",
            " 1.686 0.886 0.    1.433 1.065 1.268 0.    1.133 0.912 0.8   0.179 2.522\n",
            " 2.201 2.424 1.109 1.651 0.443 1.139 1.056 0.268 0.    0.742 0.715 0.716\n",
            " 0.207 2.561 0.877 1.028 0.214 2.827 1.392 0.956 1.411 0.886 0.    2.966\n",
            " 0.    1.259 1.449 1.921 0.    0.88  0.88  1.447 0.275 0.    1.76  1.267\n",
            " 1.459 0.    0.22  0.241 0.    0.879 1.474 2.744 1.894 1.343 0.297]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    1.069 0.893 0.872 1.078 0.241 0.    1.353 0.    0.72  2.463\n",
            " 2.914 0.6   1.686 0.886 0.    1.433 1.065 1.268 0.    1.133 0.912 0.8\n",
            " 0.179 2.522 2.201 2.424 1.109 1.651 0.443 1.139 1.056 0.268 0.    0.742\n",
            " 0.715 0.716 0.207 2.561 0.877 1.028 0.214 2.827 1.392 0.956 1.411 0.886\n",
            " 0.    2.966 0.    1.259 1.449 1.921 0.    0.88  0.88  1.447 0.275 0.\n",
            " 1.76  1.267 1.459 0.    0.22  0.241 0.    0.879 1.474 2.744 1.894 1.343\n",
            " 0.297 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.   ]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.105 0.101 0.138 0.055 0.082 0.066 0.1   0.086 0.104 0.083\n",
            " 0.095 0.124 0.089 0.055 0.075 0.138 0.085 0.118 0.08  0.14  0.111 0.072\n",
            " 0.077 0.099 0.094 0.105 0.108 0.07  0.098 0.112 0.069 0.126 0.084 0.121\n",
            " 0.117 0.084 0.107 0.107 0.09  0.062 0.124 0.099 0.039 0.027 0.059 0.154\n",
            " 0.097 0.074 0.082 0.108 0.095 0.076 0.109 0.116 0.1   0.109 0.134 0.116\n",
            " 0.055 0.082 0.087 0.123 0.077 0.073 0.025 0.097 0.107 0.061 0.094 0.052\n",
            " 0.139 0.015 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.   ]\n",
            "(100,)\n",
            "<class 'int'>\n",
            "301\n",
            "/content/gdrive/My Drive/thesis/Data/S03/S03_TEX07.txt\n",
            "3\n",
            "[0.834 3.945 0.237 1.278 0.919 0.636 0.266 4.45  0.57  1.234 4.39  0.845\n",
            " 0.718 0.225 7.177 0.934 0.843 5.999 6.223 2.497 0.552 1.186 0.957 3.913\n",
            " 0.866 1.082 0.538 1.371 0.791 1.173 0.855 0.214 6.272 3.585 0.641 0.865\n",
            " 0.231 5.368 0.863 0.544 0.299 6.555 2.205 0.696 3.708 2.446 9.318 2.042\n",
            " 1.864 3.856 0.897 0.205 5.145 0.373 0.967 2.429 0.368 4.075 3.439 3.317\n",
            " 1.405 7.393 2.681 3.882 1.853 1.589 0.861 0.231 6.897 2.101 0.727 1.019\n",
            " 2.993 3.201 1.125 0.609 2.219 2.401 1.559 0.394 3.212 0.561 9.521 0.938\n",
            " 1.697 0.397 3.296 0.178 3.388 1.077 2.175 1.032 1.186 1.382 1.475 0.893\n",
            " 0.216 3.414 1.001 2.366 0.198 1.214 1.669 1.895 2.473 0.769 2.954 1.864\n",
            " 0.184 3.069 0.971 0.462 1.217 0.548 3.372 0.643 0.647 0.225 4.447 0.573\n",
            " 1.282 0.729 1.088 1.47 ]\n",
            "[0.834 0.    0.237 1.278 0.919 0.636 0.266 0.    0.57  1.234 0.    0.845\n",
            " 0.718 0.225 0.    0.934 0.843 0.    0.    2.497 0.552 1.186 0.957 0.\n",
            " 0.866 1.082 0.538 1.371 0.791 1.173 0.855 0.214 0.    0.    0.641 0.865\n",
            " 0.231 0.    0.863 0.544 0.299 0.    2.205 0.696 0.    2.446 0.    2.042\n",
            " 1.864 0.    0.897 0.205 0.    0.373 0.967 2.429 0.368 0.    0.    0.\n",
            " 1.405 0.    2.681 0.    1.853 1.589 0.861 0.231 0.    2.101 0.727 1.019\n",
            " 2.993 0.    1.125 0.609 2.219 2.401 1.559 0.394 0.    0.561 0.    0.938\n",
            " 1.697 0.397 0.    0.178 0.    1.077 2.175 1.032 1.186 1.382 1.475 0.893\n",
            " 0.216 0.    1.001 2.366 0.198 1.214 1.669 1.895 2.473 0.769 2.954 1.864\n",
            " 0.184 0.    0.971 0.462 1.217 0.548 0.    0.643 0.647 0.225 0.    0.573\n",
            " 1.282 0.729 1.088 1.47 ]\n",
            "[0.834 0.    0.237 1.278 0.919 0.636 0.266 0.    0.57  1.234 0.    0.845\n",
            " 0.718 0.225 0.    0.934 0.843 0.    0.    2.497 0.552 1.186 0.957 0.\n",
            " 0.866 1.082 0.538 1.371 0.791 1.173 0.855 0.214 0.    0.    0.641 0.865\n",
            " 0.231 0.    0.863 0.544 0.299 0.    2.205 0.696 0.    2.446 0.    2.042\n",
            " 1.864 0.    0.897 0.205 0.    0.373 0.967 2.429 0.368 0.    0.    0.\n",
            " 1.405 0.    2.681 0.    1.853 1.589 0.861 0.231 0.    2.101 0.727 1.019\n",
            " 2.993 0.    1.125 0.609 2.219 2.401 1.559 0.394 0.    0.561 0.    0.938\n",
            " 1.697 0.397 0.    0.178 0.    1.077 2.175 1.032 1.186 1.382 1.475 0.893\n",
            " 0.216 0.    1.001 2.366]\n",
            "[0.228 0.108 0.094 0.09  0.134 0.088 0.114 0.082 0.103 0.082 0.071 0.093\n",
            " 0.12  0.133 0.108 0.114 0.094 0.082 0.073 0.101 0.082 0.104 0.141 0.106\n",
            " 0.066 0.073 0.355 0.133 0.091 0.073 0.09  0.105 0.107 0.127 0.05  0.073\n",
            " 0.131 0.1   0.084 0.102 0.139 0.057 0.1   0.131 0.099 0.115 0.077 0.104\n",
            " 0.098 0.095 0.078 0.103 0.106 0.106 0.065 0.062 0.119 0.183 0.097 0.074\n",
            " 0.065 0.124 0.067 0.139 0.049 0.08  0.053 0.089 0.091 0.092 0.11  0.098\n",
            " 0.092 0.082 0.098 0.076 0.09  0.09  0.125 0.276 0.115 0.127 0.123 0.116\n",
            " 0.133 0.09  0.092 0.103 0.107 0.118 0.121 0.124 0.114 0.099 0.088 0.091\n",
            " 0.097 0.097 0.066 0.086]\n",
            "(100,)\n",
            "<class 'int'>\n",
            "302\n",
            "/content/gdrive/My Drive/thesis/Data/S03/S03_TEX04.txt\n",
            "3\n",
            "[ 1.188  0.863  0.678  2.121  1.397  0.84   0.237 11.556  2.086  0.834\n",
            "  0.209  1.474  2.386  1.028  0.894  2.325  0.685  1.655  0.198  2.917\n",
            "  1.188  1.67   0.706  0.208 10.112  0.758  2.079  1.305  2.47   0.279\n",
            "  2.41   1.291  1.012  0.842  0.345  0.616  2.107  0.837  0.931  0.202\n",
            "  4.708  1.96   1.263  0.305  1.581  1.153  2.036  0.965  1.008  1.161\n",
            "  0.339  8.704  1.512  0.805  0.256  1.718  1.726  1.504  0.73   0.174\n",
            "  0.965  1.098  0.256  8.721  4.227  1.24   0.54   0.222  3.433  0.763\n",
            "  0.657  0.282  4.032  1.099  1.373  0.726  1.333  1.594  0.279  3.252\n",
            "  2.998  1.689  1.625  6.365  1.035  0.983  0.187  4.623  0.976  1.403\n",
            "  1.204  0.789  1.877  1.032  0.803  0.266  3.785  4.484  0.944  0.781\n",
            "  0.202  6.556  0.725  1.362  0.423  5.892  0.649  4.07   6.059  4.318\n",
            "  1.038  0.668]\n",
            "[1.188 0.863 0.678 2.121 1.397 0.84  0.237 0.    2.086 0.834 0.209 1.474\n",
            " 2.386 1.028 0.894 2.325 0.685 1.655 0.198 2.917 1.188 1.67  0.706 0.208\n",
            " 0.    0.758 2.079 1.305 2.47  0.279 2.41  1.291 1.012 0.842 0.345 0.616\n",
            " 2.107 0.837 0.931 0.202 0.    1.96  1.263 0.305 1.581 1.153 2.036 0.965\n",
            " 1.008 1.161 0.339 0.    1.512 0.805 0.256 1.718 1.726 1.504 0.73  0.174\n",
            " 0.965 1.098 0.256 0.    0.    1.24  0.54  0.222 0.    0.763 0.657 0.282\n",
            " 0.    1.099 1.373 0.726 1.333 1.594 0.279 0.    2.998 1.689 1.625 0.\n",
            " 1.035 0.983 0.187 0.    0.976 1.403 1.204 0.789 1.877 1.032 0.803 0.266\n",
            " 0.    0.    0.944 0.781 0.202 0.    0.725 1.362 0.423 0.    0.649 0.\n",
            " 0.    0.    1.038 0.668]\n",
            "[1.188 0.863 0.678 2.121 1.397 0.84  0.237 0.    2.086 0.834 0.209 1.474\n",
            " 2.386 1.028 0.894 2.325 0.685 1.655 0.198 2.917 1.188 1.67  0.706 0.208\n",
            " 0.    0.758 2.079 1.305 2.47  0.279 2.41  1.291 1.012 0.842 0.345 0.616\n",
            " 2.107 0.837 0.931 0.202 0.    1.96  1.263 0.305 1.581 1.153 2.036 0.965\n",
            " 1.008 1.161 0.339 0.    1.512 0.805 0.256 1.718 1.726 1.504 0.73  0.174\n",
            " 0.965 1.098 0.256 0.    0.    1.24  0.54  0.222 0.    0.763 0.657 0.282\n",
            " 0.    1.099 1.373 0.726 1.333 1.594 0.279 0.    2.998 1.689 1.625 0.\n",
            " 1.035 0.983 0.187 0.    0.976 1.403 1.204 0.789 1.877 1.032 0.803 0.266\n",
            " 0.    0.    0.944 0.781]\n",
            "[0.26  0.077 0.111 0.056 0.154 0.038 0.061 0.057 0.104 0.078 0.091 0.082\n",
            " 0.047 0.121 0.122 0.101 0.103 0.092 0.131 0.106 0.107 0.108 0.082 0.09\n",
            " 0.091 0.074 0.132 0.095 0.08  0.136 0.099 0.098 0.088 0.114 0.085 0.133\n",
            " 0.089 0.1   0.153 0.094 0.091 0.086 0.113 0.104 0.099 0.106 0.099 0.082\n",
            " 0.09  0.053 0.222 0.091 0.104 0.062 0.098 0.083 0.084 0.079 0.071 0.039\n",
            " 0.124 0.08  0.14  0.115 0.103 0.099 0.081 0.072 0.083 0.14  0.074 0.106\n",
            " 0.092 0.049 0.102 0.076 0.108 0.103 0.129 0.107 0.104 0.099 0.09  0.092\n",
            " 0.085 0.122 0.12  0.099 0.08  0.075 0.053 0.054 0.054 0.049 0.078 0.174\n",
            " 0.149 0.086 0.097 0.06 ]\n",
            "(100,)\n",
            "<class 'int'>\n",
            "303\n",
            "/content/gdrive/My Drive/thesis/Data/S03/S03_TEX11.txt\n",
            "3\n",
            "[ 0.316  1.266  6.543  2.705  1.644  2.431  2.444  1.282  2.025  1.08\n",
            " 55.638  2.598  0.941  1.137  5.168  0.69   0.827  0.211  4.132  1.296\n",
            "  1.154  0.826  2.537  0.667  1.732  3.384  0.942  2.246  0.853  1.244\n",
            "  0.767  0.748  0.481  2.346  2.253  3.963  2.311  4.146  0.988  0.179\n",
            "  3.911  1.202  1.582  4.243  1.153  1.189  0.912  7.269  1.314  1.013\n",
            "  0.598  0.194  3.931  3.424  1.917  5.215  0.712  0.799]\n",
            "[0.316 1.266 0.    2.705 1.644 2.431 2.444 1.282 2.025 1.08  0.    2.598\n",
            " 0.941 1.137 0.    0.69  0.827 0.211 0.    1.296 1.154 0.826 2.537 0.667\n",
            " 1.732 0.    0.942 2.246 0.853 1.244 0.767 0.748 0.481 2.346 2.253 0.\n",
            " 2.311 0.    0.988 0.179 0.    1.202 1.582 0.    1.153 1.189 0.912 0.\n",
            " 1.314 1.013 0.598 0.194 0.    0.    1.917 0.    0.712 0.799]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.316 1.266 0.\n",
            " 2.705 1.644 2.431 2.444 1.282 2.025 1.08  0.    2.598 0.941 1.137 0.\n",
            " 0.69  0.827 0.211 0.    1.296 1.154 0.826 2.537 0.667 1.732 0.    0.942\n",
            " 2.246 0.853 1.244 0.767 0.748 0.481 2.346 2.253 0.    2.311 0.    0.988\n",
            " 0.179 0.    1.202 1.582 0.    1.153 1.189 0.912 0.    1.314 1.013 0.598\n",
            " 0.194 0.    0.    1.917 0.    0.712 0.799 0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.   ]\n",
            "[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.074 0.09  0.095 0.084\n",
            " 0.107 0.133 0.12  0.102 0.1   0.095 0.089 0.121 0.148 0.071 0.074 0.123\n",
            " 0.115 0.084 0.09  0.106 0.081 0.123 0.086 0.093 0.072 0.078 0.101 0.089\n",
            " 0.16  0.064 0.109 0.098 0.122 0.098 0.103 0.111 0.104 0.087 0.095 0.096\n",
            " 0.099 0.103 0.097 0.101 0.097 0.108 0.106 0.082 0.086 0.093 0.111 0.068\n",
            " 0.083 0.082 0.123 0.101 0.087 0.108 0.098 0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            " 0.    0.    0.    0.   ]\n",
            "(100,)\n",
            "<class 'int'>\n",
            "304\n",
            "/content/gdrive/My Drive/thesis/Data/S03/S03_TEX02.txt\n",
            "3\n",
            "[ 0.298  0.972  2.827  1.621  1.598  2.289  4.985  1.137  0.782  1.12\n",
            "  4.076  0.845  2.192  0.287  4.106  0.59   1.626  0.2    2.857  1.165\n",
            "  1.188  1.54  19.401  0.194  2.393  0.891  0.885  0.792  2.365  0.932\n",
            "  1.376  6.507  9.081  1.113  1.28   0.937  3.336  1.613  6.051  2.912\n",
            "  2.08   0.289  1.887  6.073  1.671  2.187  0.721  3.668  0.855  1.294\n",
            "  0.505  4.581  5.032  1.877  2.99   2.137 15.11   1.027  0.925  1.797\n",
            "  7.678  3.75   5.383  1.818  9.678  1.752  0.387 13.191  1.738  1.239\n",
            "  0.603  0.973  0.272  2.016  0.593  2.123  3.894  2.555  2.949  4.268\n",
            "  0.634  0.842  3.004  1.099  0.828  1.062  0.841  4.706  2.002  0.294\n",
            "  1.217  1.076  0.577  0.432  1.7    1.172  1.015  0.554]\n",
            "[0.298 0.972 2.827 1.621 1.598 2.289 0.    1.137 0.782 1.12  0.    0.845\n",
            " 2.192 0.287 0.    0.59  1.626 0.2   2.857 1.165 1.188 1.54  0.    0.194\n",
            " 2.393 0.891 0.885 0.792 2.365 0.932 1.376 0.    0.    1.113 1.28  0.937\n",
            " 0.    1.613 0.    2.912 2.08  0.289 1.887 0.    1.671 2.187 0.721 0.\n",
            " 0.855 1.294 0.505 0.    0.    1.877 2.99  2.137 0.    1.027 0.925 1.797\n",
            " 0.    0.    0.    1.818 0.    1.752 0.387 0.    1.738 1.239 0.603 0.973\n",
            " 0.272 2.016 0.593 2.123 0.    2.555 2.949 0.    0.634 0.842 0.    1.099\n",
            " 0.828 1.062 0.841 0.    2.002 0.294 1.217 1.076 0.577 0.432 1.7   1.172\n",
            " 1.015 0.554]\n",
            "[0.    0.298 0.972 2.827 1.621 1.598 2.289 0.    1.137 0.782 1.12  0.\n",
            " 0.845 2.192 0.287 0.    0.59  1.626 0.2   2.857 1.165 1.188 1.54  0.\n",
            " 0.194 2.393 0.891 0.885 0.792 2.365 0.932 1.376 0.    0.    1.113 1.28\n",
            " 0.937 0.    1.613 0.    2.912 2.08  0.289 1.887 0.    1.671 2.187 0.721\n",
            " 0.    0.855 1.294 0.505 0.    0.    1.877 2.99  2.137 0.    1.027 0.925\n",
            " 1.797 0.    0.    0.    1.818 0.    1.752 0.387 0.    1.738 1.239 0.603\n",
            " 0.973 0.272 2.016 0.593 2.123 0.    2.555 2.949 0.    0.634 0.842 0.\n",
            " 1.099 0.828 1.062 0.841 0.    2.002 0.294 1.217 1.076 0.577 0.432 1.7\n",
            " 1.172 1.015 0.554 0.   ]\n",
            "[0.12  0.109 0.026 0.064 0.093 0.091 0.085 0.118 0.087 0.128 0.111 0.08\n",
            " 0.087 0.097 0.075 0.148 0.146 0.1   0.131 0.142 0.09  0.106 0.048 0.13\n",
            " 0.149 0.095 0.178 0.172 0.091 0.121 0.115 0.082 0.074 0.131 0.127 0.103\n",
            " 0.075 0.106 0.147 0.094 0.093 0.128 0.111 0.128 0.106 0.098 0.089 0.096\n",
            " 0.099 0.115 0.124 0.068 0.107 0.12  0.097 0.075 0.043 0.135 0.191 0.088\n",
            " 0.059 0.071 0.108 0.1   0.099 0.145 0.177 0.083 0.097 0.094 0.13  0.188\n",
            " 0.115 0.082 0.106 0.148 0.052 0.145 0.121 0.111 0.12  0.04  0.105 0.144\n",
            " 0.062 0.111 0.113 0.1   0.096 0.154 0.133 0.081 0.156 0.101 0.091 0.1\n",
            " 0.124 0.115 0.13  0.   ]\n",
            "(100,)\n",
            "<class 'int'>\n",
            "305\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "89QZ2rVkF03r"
      },
      "source": [
        "# data_full.reset_index(inplace=True)\n",
        "# data_full\n",
        "data_full['X_hold'].mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dI8d1iprF03v"
      },
      "source": [
        "df2['X_hold'].mean(\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gODyeZW_F03z"
      },
      "source": [
        "data_full.drop(columns={'index'},inplace=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jmYht-5cF032"
      },
      "source": [
        "temp=data_full[\"X_hold\"].apply(pd.Series)\n",
        "np.isnan(temp).sum().tail(50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rGiekOL5F035"
      },
      "source": [
        "data=pd.read_csv('C:/Users/pastal24/Desktop/thesis/test_data2.csv', sep=',')\n",
        "# pd.read_csv('file1.csv', delim_whitespace = True)\n",
        "data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "muOAnex2F037"
      },
      "source": [
        "from pathlib import Path\n",
        "from glob import glob\n",
        "x = [path(f).abspath() for f in glob(\"C:\\\\Users\\\\pastal24\\\\Desktop\\\\thesis\\\\Data\\\\S01*.txt\")]\n",
        "print(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nAwBp_WqF03_"
      },
      "source": [
        "import pathlib\n",
        "\n",
        "py = pathlib.Path().glob(\"C:\\\\Users\\\\pastal24\\\\Desktop\\\\thesis\\\\Data\\\\*.txt\")\n",
        "for file in py:\n",
        "    print(file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "noXJFpfWF04C"
      },
      "source": [
        "data_full=pd.read_csv(\"C:/Users/pastal24/Desktop/thesis/test_data.csv\") \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EeuOr0PXF04E"
      },
      "source": [
        "\n",
        "    \n",
        "temp1 = df2['X_hold'].apply(pd.Series)\n",
        "npa=temp1.to_numpy()\n",
        "npa.mean()\n",
        "# temp2 = data_full['X_hold'].apply(pd.Series)\n",
        "# npa=np.dstack((temp1,temp2))\n",
        "# npa = npa.swapaxes(1,2)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bhm69CrhF04G"
      },
      "source": [
        "\n",
        "ax=sns.distplot(npa,bins=bins, hist=True, kde=False, \n",
        "             \n",
        "           \n",
        "             )\n",
        "# ax.set_ylabel('Subjects', size =15)\n",
        "# ax.set_xlabel('MSE UPDRS22', size = 15)\n",
        "figure = ax.get_figure()    \n",
        "# figure.savefig('MSE_UPDRS22_cnn16_no_pdf.png', dpi=400)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RY-kpKe_F04J"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZUPIfS78F04L"
      },
      "source": [
        "from sklearn.preprocessing import MinMaxScaler ,StandardScaler"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_aob7D2-F04N"
      },
      "source": [
        "        temp1 = data_full['X_Flight'].apply(pd.Series)\n",
        "        temp2 = data_full['X_hold'].apply(pd.Series)\n",
        "        npa=np.dstack((temp1,temp2))\n",
        "        npa=npa[:,0:100,:]\n",
        "        print(npa.shape)\n",
        "        npa=np.nan_to_num(npa)\n",
        "        npa = npa.swapaxes(1,2)\n",
        "        npa=npa.reshape(-1 , 2, 30 , 1)\n",
        "        X=npa\n",
        "        temp4= np.asarray(data_dem['UPDRS 22'])\n",
        "        temp5= np.asarray(data_dem['UPDRS 23'])\n",
        "        temp6= np.asarray(data_dem['UPDRS 31'])\n",
        "        temp7=np.dstack((temp4,temp5,temp6))\n",
        "        temp7=np.nan_to_num(temp7)\n",
        "        temp7=temp7.swapaxes(0,1).swapaxes(1,2)\n",
        "        temp7=temp7.reshape(-1,3)\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gQtF7SibF04P"
      },
      "source": [
        "import json\n",
        "from tensorflow.keras.models import model_from_json\n",
        "json_file = open('/content/gdrive/My Drive/thesis/autoencoders/CNN100.json', 'r')\n",
        "loaded_model_json = json_file.read()\n",
        "json_file.close()\n",
        "loaded_model = model_from_json(loaded_model_json)\n",
        "# load weights into new model\n",
        "loaded_model.load_weights(\"/content/gdrive/My Drive/cnn100.h5\")\n",
        "print(\"Loaded model from disk\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XKf4cQFVxybd"
      },
      "source": [
        "df2f2=df2.sample(frac=0.5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Om2tidZxzF6"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "temp1 = df_tot['X_flight'].apply(pd.Series)\n",
        "temp2 = df_tot['X_hold'].apply(pd.Series)\n",
        "npa=np.dstack((temp1,temp2))\n",
        "npa = npa.swapaxes(1,2)\n",
        "npa=np.nan_to_num(npa)\n",
        "npa=npa.reshape(-1 , 2, 100 , 1)\n",
        "X=npa\n",
        "X = np.asarray(X).astype(np.float32)\n",
        "temp4= np.asarray(df_tot['UPDRS_22'])\n",
        "temp5= np.asarray(df_tot['UPDRS_23'])\n",
        "temp6= np.asarray(df_tot['UPDRS_31_E1_1_C1'])\n",
        "temp7=np.dstack((temp4,temp5,temp6))\n",
        "temp7=np.nan_to_num(temp7)\n",
        "temp7=temp7.swapaxes(0,1).swapaxes(1,2)\n",
        "print(temp7.shape)\n",
        "temp7=temp7.reshape(-1,3)\n",
        "\n",
        "#         temp7=temp7.reshape(-1,3,1,1)\n",
        "y=temp7\n",
        "y = np.asarray(y).astype(np.float32)\n",
        "\n",
        "       \n",
        "       \n",
        "\n",
        "       \n",
        "       \n",
        "       \n",
        "\n",
        "  # Encoder Layers\n",
        "\n",
        "        \n",
        "\n",
        "          \n",
        "          # autoencoder.add(MaxPooling2D((1, 2), padding='same'))\n",
        "                \n",
        " \n",
        " \n",
        " \n",
        " \n",
        " \n",
        " \n",
        " \n",
        " \n",
        " \n",
        " \n",
        " \n",
        " \n",
        " \n",
        " \n",
        " \n",
        " \n",
        " \n",
        "          # autoencoder.add(Conv2D(32, (2,3),strides=(1,2) , padding='same'))\n",
        "          # autoencoder.add(BatchNormalization())\n",
        "          # autoencoder.add(Activation('relu'))\n",
        "\n",
        "          # autoencoder.add(MaxPooling2D((1, 2), padding='same'))\n",
        "\n",
        "          # autoencoder.add(Conv2D(64, (2, 3),strides=(1,5) , padding='same'))\n",
        "          # autoencoder.add(BatchNormalization())\n",
        "          # autoencoder.add(Activation('relu'))\n",
        "\n",
        "          # autoencoder.add(Conv2D(48, (2, 2),strides=(1,5), activation='relu', padding='same'))\n",
        "\n",
        "\n",
        "  # autoencoder.add(BatchNormalization())\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  \n",
        "  \n",
        "  \n",
        "\n",
        "  \n",
        "  \n",
        "  \n",
        "  \n",
        "\n",
        "  \n",
        "  \n",
        "  \n",
        "  \n",
        "\n",
        "      \n",
        "      \n",
        "      \n",
        "\n",
        "      \n",
        "      \n",
        "      \n",
        "      \n",
        "      \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "      \n",
        "\n",
        "\n",
        "      \n",
        "      \n",
        "      \n",
        "model = Sequential()\n",
        "        # for layer in autoencoder.layers[:-13]: \n",
        "        #     model.add(layer)\n",
        "        #     layer.trainable=True\n",
        "model.add(Conv2D(64, (2, 3) , padding='same',input_shape=X.shape[1:]))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(Conv2D(64, (1, 1) , padding='same')) \n",
        "model.add(BatchNormalization())\n",
        "   \n",
        "model.add(Activation('relu'))   \n",
        "model.add(Conv2D(64, (2, 3) , padding='same'))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Activation('relu')) \n",
        "model.add(MaxPooling2D((1,2),padding='same'))\n",
        "model.add(Conv2D(32, (2, 3) , padding='same'))\n",
        "model.add(BatchNormalization())\n",
        "    \n",
        "model.add(Activation('relu'))    \n",
        "model.add(Conv2D(32, (1, 1) , padding='same'))\n",
        "model.add(BatchNormalization())\n",
        "    \n",
        "model.add(Activation('relu'))        \n",
        "model.add(Conv2D(32, (2, 3) , padding='same')) \n",
        "model.add(BatchNormalization())\n",
        "   \n",
        "model.add(Activation('relu'))  \n",
        "        # model.add(MaxPooling2D((1,2),padding='same'))\n",
        "        # model.add(Conv2D(32, (2, 3) , padding='same')) \n",
        "        # model.add(BatchNormalization())\n",
        "   \n",
        "        # model.add(Activation('relu'))    \n",
        "        # model.add(Conv2D(32, (1, 1) , padding='same')) \n",
        "        # model.add(BatchNormalization())\n",
        "   \n",
        "        # model.add(Activation('relu'))        \n",
        "        # model.add(Conv2D(32, (2, 3) , padding='same')) \n",
        "        # model.add(BatchNormalization())\n",
        "   \n",
        "        # model.add(Activation('relu'))\n",
        "         \n",
        "        # model.add(GaussianNoise(0.05))\n",
        "        # model.add(Dense(512,activation='relu'))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128))\n",
        "        # model.add(GaussianNoise(0.05))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Dense(64))\n",
        "        # model.add(GaussianNoise(0.05))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.4))\n",
        "\n",
        "        # model.add(Dense(64,use_bias='False'))\n",
        "\n",
        "        # model.add(BatchNormalization())\n",
        "        # model.add(Activation('relu'))\n",
        "        # model.add(Dropout(0.4))\n",
        "\n",
        "\n",
        "\n",
        "        # model.add(GaussianNoise(0.01))\n",
        "\n",
        "        # model.add(Dense(16,activation='relu'))\n",
        "        # model.add(Dropout(0.4))\n",
        "\n",
        "        # model.add(Dense(512,activation='relu'))\n",
        "\n",
        "        # model.add(GaussianNoise(0.01))\n",
        "\n",
        "        # model.add(Dense(25,activation='relu'))\n",
        "        # model.add(GaussianNoise(0.01))\n",
        "\n",
        "\n",
        "# kernel_initializer='he_normal',bias_initializer=initializer\n",
        "        # model.add(GaussianNoise(0.01))\n",
        "        # model.add(Dense(512, activation='relu'))\n",
        "\n",
        "        # model.add(Dense(96, activation='relu'))\n",
        "\n",
        "        # model.add(Dense(512, activation='relu'))\n",
        "\n",
        "        # model.add(Dense(2048,activation= 'relu'))\n",
        "        # model.add(Dense(2048,activation='relu'))\n",
        "#         model.add(Dense(512,activation='relu'))\n",
        "        # model.add(GaussianNoise(0.01))\n",
        "\n",
        "        # model.add(GaussianNoise(0.01))\n",
        "\n",
        "#         model.add(Dense(256,activation='relu'))\n",
        "#         model.add(Dense(256,activation='relu')) \n",
        "        # model.add(Dense(1024,activation='relu')) \n",
        "        # model.add(Dense(1024,activation='relu')) \n",
        "        # model.add(Dense(1024,activation='relu'))\n",
        "#         model.add(Dense(32,activation='relu'))\n",
        "\n",
        "model.add(Dense(3)) \n",
        "\n",
        "model.add(Activation('linear'))\n",
        "model.summary()\n",
        "\n",
        "X1=np.flip(X,2)\n",
        "kek=[X,X1]\n",
        "print(X1.shape)\n",
        "X=np.concatenate(kek,axis=0)\n",
        "X = np.asarray(X).astype(np.float32)\n",
        "kek2=[y,y]\n",
        "y=np.concatenate(kek2,axis=0)\n",
        "y = np.asarray(y).astype(np.float32)\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X, y, test_size=0.05, random_state=42)\n",
        "temp1 = df3['X_flight'].apply(pd.Series)\n",
        "temp2 = df3['X_hold'].apply(pd.Series)\n",
        "  \n",
        "  \n",
        "npa=np.dstack((temp1,temp2))\n",
        "print(npa.shape)\n",
        "npa = npa.swapaxes(1,2)\n",
        "npa=npa.reshape(-1 , 2, 100 , 1)\n",
        "X_test=npa\n",
        "temp4= np.asarray(df3['UPDRS 22'])\n",
        "temp5= np.asarray(df3['UPDRS 23'])\n",
        "temp6= np.asarray(df3['UPDRS 31'])\n",
        "temp7=np.dstack((temp4,temp5,temp6))\n",
        "temp7=np.nan_to_num(temp7)\n",
        "temp7=temp7.swapaxes(0,1).swapaxes(1,2)\n",
        "temp7=temp7.reshape(-1,3)\n",
        "temp7=temp7.reshape(-1,3,1,1)\n",
        "y_test=temp7\n",
        "print(y.shape)\n",
        "\n",
        "        # training_generator, steps_per_epoch = balanced_batch_generator((X_train.reshape(X_train.shape[0],-1)), y_train[:,], sampler=NearMiss(), batch_size=64, random_state=42)\n",
        "        \n",
        "        # my_generator = ((np.reshape(X_train, (-1, 2, 100,-1)),np.reshape( y_train,(-1,3))) for (X_train,y_train) in training_generator)\n",
        "        # history = model.fit_generator(generator=my_generator,\n",
        "        #                                 steps_per_epoch=steps_per_epoch,\n",
        "        #                                 epochs=50, verbose=0)\n",
        "        # model.fit(X_train1,y_train1,epochs=50,batch_size=16)\n",
        "opt3=tensorflow.keras.optimizers.Adam(learning_rate=1e-2,decay=1e-4,amsgrad='True')\n",
        "model.compile(loss='mse', optimizer=opt3, metrics=['mae'])\n",
        "history=model.fit(X_train,y_train,batch_size=96,\n",
        "                epochs=75,callbacks=[tensorflow.keras.callbacks.ModelCheckpoint(filepath=filepathh, monitor='val_loss', save_best_only=True)],verbose=2,validation_data=(X_test,y_test))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jIX_gERyF04R"
      },
      "source": [
        "        from tensorflow.keras.layers import GaussianNoise   \n",
        "        model = Sequential()\n",
        "\n",
        "        for layer in loaded_model.layers[:-4]: \n",
        "            model.add(layer)\n",
        "            layer.trainable=True\n",
        "            \n",
        "        model.add(Flatten()) \n",
        "        \n",
        "        model.add(Dense(512, activation='relu'))\n",
        "        # model.add(Dropout(0.2))\n",
        "#         model.add(GaussianNoise(0.05))\n",
        "\n",
        "        model.add(Dense(512, activation='relu'))\n",
        "        model.add(Dense(512, activation='relu'))\n",
        "        # model.add(Dense(256, activation='relu'))\n",
        "#         model.add(GaussianNoise(0.1))\n",
        "        # model.add(Dropout(0.2))\n",
        "        # model.add(Dense(,activation='relu'))\n",
        "        # model.add(Dropout(0.5))\n",
        "#         model.add(GaussianNoise(0.05))\n",
        "        # model.add(Dropout(0.2))\n",
        "#         model.add(Dense(64,activation= 'relu'))\n",
        "        #         model.add(Dense(512,activation= 'relu'))\n",
        "\n",
        "\n",
        "#      model.add(Dense(512,activation='relu'))\n",
        "#         model.add(Dense(256,activation='relu'))\n",
        "\n",
        "#         model.add(Dense(64,activation='relu'))\n",
        "#         model.add(Dense(64,activation='relu')) \n",
        "#         model.add(Dense(64,activation= 'relu'))\n",
        "#         model.add(Dense(64,activation='relu'))\n",
        "#         model.add(Dense(64,activation='relu')) \n",
        "#         model.add(Dense(64,activation='relu'))\n",
        "        model.add(Dense(3, activation=\"linear\")) \n",
        "        opt = tensorflow.keras.optimizers.Adam(learning_rate=1e-4,decay=1e-6 )\n",
        "        model.compile(loss='mse', optimizer=opt,metrics=['mae'])\n",
        "\n",
        "        \n",
        "        model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0uqxIAKyF04T"
      },
      "source": [
        "from sklearn.preprocessing import Normalizer,QuantileTransformer,PowerTransformer\n",
        "from sklearn import preprocessing\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i9jaEF8XF04V"
      },
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "Rl=ReduceLROnPlateau(monitor='loss',patience=1,verbose=1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QjSEWOxbSUJx"
      },
      "source": [
        "df2=df2.reset_index()\n",
        "df2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kUZJmfMKRvVV"
      },
      "source": [
        "a=np.zeros(25)\n",
        "df_tot=pd.DataFrame()\n",
        "for i in range (18848):\n",
        "    \n",
        "    j=25\n",
        "    \n",
        "    if (df2['X_flight'][i][0:j] != np.zeros(25)).any() :\n",
        "        print('True')\n",
        "        df_new=df2.loc[i]\n",
        "        df_tot=pd.concat([df_tot,df_new],axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S7wangtnSrPB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "126052aa-717d-48b3-974b-5a9a9fb95e26"
      },
      "source": [
        "df_tot=df2 \n",
        "# df_tot=df2[df2['UPDRS_22']<6]\n",
        "df_tot=df_tot.reset_index()\n",
        "df_tot.shape\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4624, 17)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 254
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LSK3L1BwF04W",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3bdaec87-1997-4771-9525-673fc23df465"
      },
      "source": [
        "\n",
        "temp1 = df_tot['X_flight'].apply(pd.Series)\n",
        "temp2 = df_tot['X_hold'].apply(pd.Series)\n",
        "\n",
        "\n",
        "#         scaler2 = preprocessing.MinMaxScaler().fit(temp2)\n",
        "#         temp2=scaler.transform(temp4)\n",
        "#         temp3=scaler.transform(temp1)\n",
        "#         print(temp3)\n",
        "#         temp4=scaler2.transform(temp2)\n",
        "npa=np.dstack((temp1,temp2))\n",
        "npa=np.nan_to_num(npa)\n",
        "#         npa=scaler.fit(npa)\n",
        "        \n",
        "# npa = npa.swapaxes(1,2)\n",
        "#         npa2=np.flip(npa,2)\n",
        "\n",
        "npa=npa.reshape(-1 , 100 , 2,1)\n",
        "npa.shape\n",
        "\n",
        "X=npa\n",
        "X = np.asarray(X).astype(np.float32)\n",
        "\n",
        "# X=np.concatenate(kek,axis=0)\n",
        "# X = np.asarray(X).astype(np.float32)\n",
        "# print(X_train.shape)        \n",
        "temp4= np.asarray(df_tot['UPDRS_22'])\n",
        "temp5= np.asarray(df_tot['UPDRS_23'])\n",
        "temp6= np.asarray(df_tot['UPDRS_31_E1_1_C1'])\n",
        "temp7=np.dstack((temp4,temp5,temp6))\n",
        "temp7=np.nan_to_num(temp7)\n",
        "temp7=temp7.swapaxes(0,1).swapaxes(1,2)\n",
        "temp7=temp7.reshape(-1,3)\n",
        "\n",
        "kek2=[temp7,temp7]\n",
        "# y=np.concatenate(kek2,axis=0)\n",
        "y=temp7\n",
        "y=np.asarray(y).astype(np.float32)\n",
        "# y=np.concatenate(kek2,axis=0)\n",
        "# y=np.asarray(y).astype(np.float32)\n",
        "print(y.shape)\n",
        "\n",
        "# temp1 = df_new['X_flight'].apply(pd.Series)\n",
        "# temp2 = df_new['X_hold'].apply(pd.Series)\n",
        "\n",
        "\n",
        "#         scaler2 = preprocessing.MinMaxScaler().fit(temp2)\n",
        "#         temp2=scaler.transform(temp4)\n",
        "#         temp3=scaler.transform(temp1)\n",
        "#         print(temp3)\n",
        "#         temp4=scaler2.transform(temp2)\n",
        "# npa=np.dstack((temp1,temp2))\n",
        "# npa=np.nan_to_num(npa)\n",
        "#         npa=scaler.fit(npa)\n",
        "        \n",
        "# npa = npa.swapaxes(1,2)\n",
        "#         npa2=np.flip(npa,2)\n",
        "\n",
        "# npa=npa.reshape(-1 , 100 , 2,1)\n",
        "# npa.shape\n",
        "\n",
        "# X_test=npa\n",
        "# X_test = np.asarray(X_test).astype(np.float32)\n",
        "\n",
        "# X=np.concatenate(kek,axis=0)\n",
        "# X = np.asarray(X).astype(np.float32)\n",
        "# print(X_train.shape)        \n",
        "# temp4= np.asarray(df_new['UPDRS_22'])\n",
        "# temp5= np.asarray(df_new['UPDRS_23'])\n",
        "# temp6= np.asarray(df_new['UPDRS_31_E1_1_C1'])\n",
        "# temp7=np.dstack((temp4,temp5,temp6))\n",
        "# temp7=np.nan_to_num(temp7)\n",
        "# temp7=temp7.swapaxes(0,1).swapaxes(1,2)\n",
        "# temp7=temp7.reshape(-1,3)\n",
        "# y_test=temp7\n",
        "# y_test=np.asarray(y_test).astype(np.float32)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(4624, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MfP-OCNdCZce"
      },
      "source": [
        "Visible=Input((100,2,1))\n",
        "                           \n",
        "x=((Conv2D(16,(3,2),padding='same')))(Visible)\n",
        "\n",
        "x=(BatchNormalization())(x)                \n",
        "        \n",
        "x=(Activation('relu'))(x)\n",
        "        \n",
        "x=((MaxPooling2D( (2,1) ,padding='same')))(x)  \n",
        "x=((Conv2D(12, (3,2), padding='same')))(x)\n",
        "x=(BatchNormalization())(x)\n",
        "\n",
        "encoded=(Activation('relu'))(x)\n",
        "        \n",
        "\n",
        "\n",
        "\n",
        "x=((UpSampling2D((2,1))))(x)\n",
        "x=((Conv2D(16, (3,2), padding='same')))(x)\n",
        "x=(BatchNormalization())(x)\n",
        "\n",
        "x=Activation('relu')(x)\n",
        "\n",
        "\n",
        "decoded=((Conv2D(1, (3,2), activation='linear', padding='same')))(x)\n",
        "        \n",
        "\n",
        "#                 # Encoder Layers\n",
        "# #                 # autoencoder.add(LSTM(25,return_sequences=False)\n",
        "autoencoder = Model(Visible, decoded)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e6GBd-iKXUl5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "099fb856-31ac-492c-f72a-ae6f758988b9"
      },
      "source": [
        "Visible=Input((100,2))\n",
        "x=(LSTM(16, input_shape= (100,2) ,activation='relu',return_sequences=True ))(Visible)\n",
        "x=LSTM(8,activation='relu', return_sequences=False )(x)\n",
        "encoded=(RepeatVector(X.shape[1]))(x)\n",
        "\n",
        "x=LSTM(8,activation='relu' ,return_sequences=True )(encoded)\n",
        "x=LSTM(16,activation='relu', return_sequences=True )(x)\n",
        "decoded=TimeDistributed(Dense(X.shape[2],activation='linear'))(x)\n",
        "autoencoder = Model( Visible , decoded )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer lstm_6 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "WARNING:tensorflow:Layer lstm_7 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "WARNING:tensorflow:Layer lstm_8 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "WARNING:tensorflow:Layer lstm_9 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "srwr3q85F04Z",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 757
        },
        "outputId": "7f4097d2-18d8-4b70-a86f-aebb0f221a47"
      },
      "source": [
        "opt = tensorflow.keras.optimizers.Adam(learning_rate=1e-2 )\n",
        "autoencoder.compile(loss='mse',optimizer=opt,metrics=['mae'])\n",
        "autoencoder.summary()\n",
        "autoencoder.fit(X,X,epochs=30,batch_size=32,shuffle=True,verbose=2)\n",
        "# from tensorflow.keras.models import model_from_json,load_model\n",
        "# model= load_model('/content/gdrive/My Drive/models/best_model_11_10.h5')\n",
        "# tensorflow.keras.utils.plot_model(autoencoder,show_shapes=True)\n",
        "# tensorflow.keras.models.save_model(model,\"model.h5\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"functional_27\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_8 (InputLayer)         [(None, 100, 2)]          0         \n",
            "_________________________________________________________________\n",
            "lstm_6 (LSTM)                (None, 100, 16)           1216      \n",
            "_________________________________________________________________\n",
            "lstm_7 (LSTM)                (None, 8)                 800       \n",
            "_________________________________________________________________\n",
            "repeat_vector_1 (RepeatVecto (None, 100, 8)            0         \n",
            "_________________________________________________________________\n",
            "lstm_8 (LSTM)                (None, 100, 8)            544       \n",
            "_________________________________________________________________\n",
            "lstm_9 (LSTM)                (None, 100, 16)           1600      \n",
            "_________________________________________________________________\n",
            "time_distributed_1 (TimeDist (None, 100, 2)            34        \n",
            "=================================================================\n",
            "Total params: 4,194\n",
            "Trainable params: 4,194\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/30\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-103-6a1155d8a881>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mautoencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'mse'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mae'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mautoencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mautoencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;31m# from tensorflow.keras.models import model_from_json,load_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# model= load_model('/content/gdrive/My Drive/models/best_model_11_10.h5')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1099\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    805\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 807\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    808\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2829\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2831\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1846\u001b[0m                            resource_variable_ops.BaseResourceVariable))],\n\u001b[1;32m   1847\u001b[0m         \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m         cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1922\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1923\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1924\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1926\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    548\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nq_LJg5EF04f"
      },
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EHO5hotC_Z1s"
      },
      "source": [
        "H=Flatten()(encoded)\n",
        "output=(Dense(16))(H)\n",
        "output=BatchNormalization()(output)\n",
        "output=(Activation('relu'))(output)\n",
        "output=Dropout(0.5)(output)\n",
        "        # output=(Dense(16))(output)\n",
        "        # output=BatchNormalization()(output)\n",
        "        # output=(Activation('relu'))(output)\n",
        "        # output=Dropout(0.5)(output)\n",
        "# attention=Dense(16)(H)\n",
        "# attention=BatchNormalization()(attention)\n",
        "# attention=Activation('tanh')(attention)\n",
        "#         # attention=Dropout(0.5)(attention)\n",
        "# attention=Dense(3)(attention)\n",
        "# attention=tensorflow.transpose(attention,perm=[0, 2, 1])\n",
        "# attention=Softmax()(attention)\n",
        "\n",
        "# output=tensorflow.matmul(attention,output)\n",
        "# output=Flatten()(output)\n",
        "\n",
        "output2=(Dense(3))(output)\n",
        "output2=(Activation('linear'))(output2)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        \n",
        "model = Model(Visible,outputs=output2)\n",
        "\n",
        "        # steps_per_epoch=np.floor(X_train.shape[0]/256).astype('int')\n",
        "        # model=load_model('model.h5')\n",
        "opt3=tensorflow.keras.optimizers.Adam(learning_rate=0.001)\n",
        "\n",
        "model.compile(loss=['mse'], optimizer=opt3, metrics=['mae'])\n",
        "model.summary()\n",
        "history=model.fit(X,y,batch_size=32,steps_per_epoch=np.floor(X.shape[0]/32).astype('int'),\n",
        "                epochs=60,verbose=2)\n",
        "# from tensorflow.keras.models import model_from_json,load_model\n",
        "# model= load_model('/content/gdrive/My Drive/models/best_model_11_10.h5')\n",
        "# tensorflow.keras.utils.plot_model(model,show_shapes=True)\n",
        "# tensorflow.keras.models.save_model(model,\"model.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FBHH0y-LF04g",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7ec8b66c-b5d3-4148-bded-8132fa8e596c"
      },
      "source": [
        "for i in range (1,11):\n",
        "  tensorflow.random.set_seed(\n",
        "    i)\n",
        "  Visible=Input((100,2,1))\n",
        "                           \n",
        "  x=((Conv2D(16,(3,2),padding='same')))(Visible)\n",
        "\n",
        "  x=(BatchNormalization())(x)                \n",
        "          \n",
        "  x=(Activation('relu'))(x)\n",
        "          \n",
        "  x=((MaxPooling2D( (2,1) ,padding='same')))(x)  \n",
        "  x=((Conv2D(12, (3,2), padding='same')))(x)\n",
        "  x=(BatchNormalization())(x)\n",
        "\n",
        "  encoded=(Activation('relu'))(x)\n",
        "          \n",
        "\n",
        "\n",
        "\n",
        "  x=((UpSampling2D((2,1))))(x)\n",
        "  x=((Conv2D(16, (3,2), padding='same')))(x)\n",
        "  x=(BatchNormalization())(x)\n",
        "\n",
        "  x=Activation('relu')(x)\n",
        "\n",
        "\n",
        "  decoded=((Conv2D(1, (3,2), activation='linear', padding='same')))(x)\n",
        "          \n",
        "\n",
        "  #                 # Encoder Layers\n",
        "  # #                 # autoencoder.add(LSTM(25,return_sequences=False)\n",
        "  autoencoder = Model(Visible, decoded) \n",
        "  opt = tensorflow.keras.optimizers.Adam(learning_rate=1e-2 )\n",
        "  autoencoder.compile(loss='mse',optimizer=opt,metrics=['mae'])\n",
        "  autoencoder.summary()\n",
        "  autoencoder.fit(X,X,epochs=30,batch_size=32,shuffle=True,verbose=2)\n",
        "\n",
        "  H=Flatten()(encoded)\n",
        "  output=(Dense(16))(H)\n",
        "  output=BatchNormalization()(output)\n",
        "  output=(Activation('relu'))(output)\n",
        "  output=Dropout(0.5)(output)\n",
        "          # output=(Dense(16))(output)\n",
        "          # output=BatchNormalization()(output)\n",
        "          # output=(Activation('relu'))(output)\n",
        "          # output=Dropout(0.5)(output)\n",
        "  # attention=Dense(16)(H)\n",
        "  # attention=BatchNormalization()(attention)\n",
        "  # attention=Activation('tanh')(attention)\n",
        "  #         # attention=Dropout(0.5)(attention)\n",
        "  # attention=Dense(3)(attention)\n",
        "  # attention=tensorflow.transpose(attention,perm=[0, 2, 1])\n",
        "  # attention=Softmax()(attention)\n",
        "\n",
        "  # output=tensorflow.matmul(attention,output)\n",
        "  # output=Flatten()(output)\n",
        "\n",
        "  output2=(Dense(3))(output)\n",
        "  output2=(Activation('linear'))(output2)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "          \n",
        "  model = Model(Visible,outputs=output2)\n",
        "\n",
        "          # steps_per_epoch=np.floor(X_train.shape[0]/256).astype('int')\n",
        "          # model=load_model('model.h5')\n",
        "  opt3=tensorflow.keras.optimizers.Adam(learning_rate=0.01)\n",
        "\n",
        "  model.compile(loss=['mse'], optimizer=opt3, metrics=['mae'])\n",
        "  model.summary()\n",
        "  history=model.fit(X,y,batch_size=32,steps_per_epoch=np.floor(X.shape[0]/32).astype('int'),\n",
        "                  epochs=60,verbose=2)\n",
        "  # from tensorflow.keras.models import model_from_json,load_model\n",
        "  # model= load_model('/content/gdrive/My Drive/models/best_model_11_10.h5')\n",
        "  # tensorflow.keras.utils.plot_model(model,show_shapes=True)\n",
        "  tensorflow.keras.models.save_model(model,\"model_%d.h5\"%(i))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"functional_1593\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_399 (InputLayer)       [(None, 100, 2, 1)]       0         \n",
            "_________________________________________________________________\n",
            "conv2d_1592 (Conv2D)         (None, 100, 2, 16)        112       \n",
            "_________________________________________________________________\n",
            "batch_normalization_1592 (Ba (None, 100, 2, 16)        64        \n",
            "_________________________________________________________________\n",
            "activation_1990 (Activation) (None, 100, 2, 16)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_398 (MaxPoolin (None, 50, 2, 16)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_1593 (Conv2D)         (None, 50, 2, 12)         1164      \n",
            "_________________________________________________________________\n",
            "batch_normalization_1593 (Ba (None, 50, 2, 12)         48        \n",
            "_________________________________________________________________\n",
            "up_sampling2d_398 (UpSamplin (None, 100, 2, 12)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1594 (Conv2D)         (None, 100, 2, 16)        1168      \n",
            "_________________________________________________________________\n",
            "batch_normalization_1594 (Ba (None, 100, 2, 16)        64        \n",
            "_________________________________________________________________\n",
            "activation_1992 (Activation) (None, 100, 2, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1595 (Conv2D)         (None, 100, 2, 1)         97        \n",
            "=================================================================\n",
            "Total params: 2,717\n",
            "Trainable params: 2,629\n",
            "Non-trainable params: 88\n",
            "_________________________________________________________________\n",
            "Epoch 1/30\n",
            "145/145 - 0s - loss: 0.0265 - mae: 0.0822\n",
            "Epoch 2/30\n",
            "145/145 - 0s - loss: 0.0092 - mae: 0.0597\n",
            "Epoch 3/30\n",
            "145/145 - 0s - loss: 0.0061 - mae: 0.0492\n",
            "Epoch 4/30\n",
            "145/145 - 0s - loss: 0.0044 - mae: 0.0399\n",
            "Epoch 5/30\n",
            "145/145 - 0s - loss: 0.0043 - mae: 0.0392\n",
            "Epoch 6/30\n",
            "145/145 - 0s - loss: 0.0039 - mae: 0.0377\n",
            "Epoch 7/30\n",
            "145/145 - 0s - loss: 0.0036 - mae: 0.0361\n",
            "Epoch 8/30\n",
            "145/145 - 0s - loss: 0.0040 - mae: 0.0366\n",
            "Epoch 9/30\n",
            "145/145 - 0s - loss: 0.0038 - mae: 0.0355\n",
            "Epoch 10/30\n",
            "145/145 - 0s - loss: 0.0040 - mae: 0.0361\n",
            "Epoch 11/30\n",
            "145/145 - 0s - loss: 0.0027 - mae: 0.0306\n",
            "Epoch 12/30\n",
            "145/145 - 0s - loss: 0.0034 - mae: 0.0334\n",
            "Epoch 13/30\n",
            "145/145 - 0s - loss: 0.0037 - mae: 0.0346\n",
            "Epoch 14/30\n",
            "145/145 - 0s - loss: 0.0035 - mae: 0.0345\n",
            "Epoch 15/30\n",
            "145/145 - 0s - loss: 0.0027 - mae: 0.0303\n",
            "Epoch 16/30\n",
            "145/145 - 0s - loss: 0.0029 - mae: 0.0317\n",
            "Epoch 17/30\n",
            "145/145 - 0s - loss: 0.0024 - mae: 0.0280\n",
            "Epoch 18/30\n",
            "145/145 - 0s - loss: 0.0028 - mae: 0.0303\n",
            "Epoch 19/30\n",
            "145/145 - 0s - loss: 0.0025 - mae: 0.0275\n",
            "Epoch 20/30\n",
            "145/145 - 1s - loss: 0.0032 - mae: 0.0324\n",
            "Epoch 21/30\n",
            "145/145 - 0s - loss: 0.0023 - mae: 0.0273\n",
            "Epoch 22/30\n",
            "145/145 - 0s - loss: 0.0024 - mae: 0.0277\n",
            "Epoch 23/30\n",
            "145/145 - 0s - loss: 0.0027 - mae: 0.0299\n",
            "Epoch 24/30\n",
            "145/145 - 0s - loss: 0.0028 - mae: 0.0303\n",
            "Epoch 25/30\n",
            "145/145 - 0s - loss: 0.0031 - mae: 0.0319\n",
            "Epoch 26/30\n",
            "145/145 - 0s - loss: 0.0028 - mae: 0.0303\n",
            "Epoch 27/30\n",
            "145/145 - 0s - loss: 0.0027 - mae: 0.0300\n",
            "Epoch 28/30\n",
            "145/145 - 0s - loss: 0.0021 - mae: 0.0263\n",
            "Epoch 29/30\n",
            "145/145 - 0s - loss: 0.0027 - mae: 0.0299\n",
            "Epoch 30/30\n",
            "145/145 - 0s - loss: 0.0023 - mae: 0.0264\n",
            "Model: \"functional_1595\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_399 (InputLayer)       [(None, 100, 2, 1)]       0         \n",
            "_________________________________________________________________\n",
            "conv2d_1592 (Conv2D)         (None, 100, 2, 16)        112       \n",
            "_________________________________________________________________\n",
            "batch_normalization_1592 (Ba (None, 100, 2, 16)        64        \n",
            "_________________________________________________________________\n",
            "activation_1990 (Activation) (None, 100, 2, 16)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_398 (MaxPoolin (None, 50, 2, 16)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_1593 (Conv2D)         (None, 50, 2, 12)         1164      \n",
            "_________________________________________________________________\n",
            "batch_normalization_1593 (Ba (None, 50, 2, 12)         48        \n",
            "_________________________________________________________________\n",
            "activation_1991 (Activation) (None, 50, 2, 12)         0         \n",
            "_________________________________________________________________\n",
            "flatten_398 (Flatten)        (None, 1200)              0         \n",
            "_________________________________________________________________\n",
            "dense_796 (Dense)            (None, 16)                19216     \n",
            "_________________________________________________________________\n",
            "batch_normalization_1595 (Ba (None, 16)                64        \n",
            "_________________________________________________________________\n",
            "activation_1993 (Activation) (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dropout_398 (Dropout)        (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_797 (Dense)            (None, 3)                 51        \n",
            "_________________________________________________________________\n",
            "activation_1994 (Activation) (None, 3)                 0         \n",
            "=================================================================\n",
            "Total params: 20,719\n",
            "Trainable params: 20,631\n",
            "Non-trainable params: 88\n",
            "_________________________________________________________________\n",
            "Epoch 1/60\n",
            "144/144 - 0s - loss: 1.8128 - mae: 0.9327\n",
            "Epoch 2/60\n",
            "144/144 - 1s - loss: 1.4201 - mae: 0.8417\n",
            "Epoch 3/60\n",
            "144/144 - 0s - loss: 1.3353 - mae: 0.8150\n",
            "Epoch 4/60\n",
            "144/144 - 0s - loss: 1.2872 - mae: 0.8009\n",
            "Epoch 5/60\n",
            "144/144 - 0s - loss: 1.2391 - mae: 0.7856\n",
            "Epoch 6/60\n",
            "144/144 - 0s - loss: 1.2137 - mae: 0.7778\n",
            "Epoch 7/60\n",
            "144/144 - 0s - loss: 1.2245 - mae: 0.7748\n",
            "Epoch 8/60\n",
            "144/144 - 0s - loss: 1.2496 - mae: 0.7800\n",
            "Epoch 9/60\n",
            "144/144 - 0s - loss: 1.1628 - mae: 0.7614\n",
            "Epoch 10/60\n",
            "144/144 - 0s - loss: 1.1594 - mae: 0.7628\n",
            "Epoch 11/60\n",
            "144/144 - 0s - loss: 1.1018 - mae: 0.7511\n",
            "Epoch 12/60\n",
            "144/144 - 0s - loss: 1.1160 - mae: 0.7449\n",
            "Epoch 13/60\n",
            "144/144 - 0s - loss: 1.1327 - mae: 0.7503\n",
            "Epoch 14/60\n",
            "144/144 - 0s - loss: 1.1280 - mae: 0.7526\n",
            "Epoch 15/60\n",
            "144/144 - 0s - loss: 1.0977 - mae: 0.7399\n",
            "Epoch 16/60\n",
            "144/144 - 0s - loss: 1.0573 - mae: 0.7318\n",
            "Epoch 17/60\n",
            "144/144 - 0s - loss: 1.0950 - mae: 0.7428\n",
            "Epoch 18/60\n",
            "144/144 - 0s - loss: 1.0414 - mae: 0.7296\n",
            "Epoch 19/60\n",
            "144/144 - 0s - loss: 1.0264 - mae: 0.7211\n",
            "Epoch 20/60\n",
            "144/144 - 0s - loss: 1.0676 - mae: 0.7338\n",
            "Epoch 21/60\n",
            "144/144 - 0s - loss: 1.0229 - mae: 0.7210\n",
            "Epoch 22/60\n",
            "144/144 - 0s - loss: 1.0042 - mae: 0.7152\n",
            "Epoch 23/60\n",
            "144/144 - 0s - loss: 1.0225 - mae: 0.7163\n",
            "Epoch 24/60\n",
            "144/144 - 1s - loss: 0.9759 - mae: 0.7081\n",
            "Epoch 25/60\n",
            "144/144 - 0s - loss: 1.0085 - mae: 0.7164\n",
            "Epoch 26/60\n",
            "144/144 - 0s - loss: 0.9997 - mae: 0.7137\n",
            "Epoch 27/60\n",
            "144/144 - 0s - loss: 0.9776 - mae: 0.7065\n",
            "Epoch 28/60\n",
            "144/144 - 0s - loss: 0.9963 - mae: 0.7137\n",
            "Epoch 29/60\n",
            "144/144 - 0s - loss: 0.9962 - mae: 0.7115\n",
            "Epoch 30/60\n",
            "144/144 - 0s - loss: 0.9832 - mae: 0.7048\n",
            "Epoch 31/60\n",
            "144/144 - 0s - loss: 1.0229 - mae: 0.7189\n",
            "Epoch 32/60\n",
            "144/144 - 1s - loss: 0.9631 - mae: 0.7038\n",
            "Epoch 33/60\n",
            "144/144 - 0s - loss: 0.9059 - mae: 0.6851\n",
            "Epoch 34/60\n",
            "144/144 - 1s - loss: 0.9880 - mae: 0.7105\n",
            "Epoch 35/60\n",
            "144/144 - 1s - loss: 0.9694 - mae: 0.6968\n",
            "Epoch 36/60\n",
            "144/144 - 0s - loss: 0.9543 - mae: 0.6970\n",
            "Epoch 37/60\n",
            "144/144 - 0s - loss: 0.9050 - mae: 0.6799\n",
            "Epoch 38/60\n",
            "144/144 - 0s - loss: 0.9072 - mae: 0.6776\n",
            "Epoch 39/60\n",
            "144/144 - 0s - loss: 0.9772 - mae: 0.7005\n",
            "Epoch 40/60\n",
            "144/144 - 0s - loss: 0.9162 - mae: 0.6863\n",
            "Epoch 41/60\n",
            "144/144 - 0s - loss: 0.9571 - mae: 0.6974\n",
            "Epoch 42/60\n",
            "144/144 - 0s - loss: 0.9046 - mae: 0.6803\n",
            "Epoch 43/60\n",
            "144/144 - 0s - loss: 0.9551 - mae: 0.6888\n",
            "Epoch 44/60\n",
            "144/144 - 0s - loss: 0.9275 - mae: 0.6877\n",
            "Epoch 45/60\n",
            "144/144 - 1s - loss: 0.9104 - mae: 0.6813\n",
            "Epoch 46/60\n",
            "144/144 - 0s - loss: 0.9453 - mae: 0.6840\n",
            "Epoch 47/60\n",
            "144/144 - 0s - loss: 0.9440 - mae: 0.6896\n",
            "Epoch 48/60\n",
            "144/144 - 0s - loss: 0.9245 - mae: 0.6843\n",
            "Epoch 49/60\n",
            "144/144 - 1s - loss: 0.9392 - mae: 0.6856\n",
            "Epoch 50/60\n",
            "144/144 - 0s - loss: 0.9228 - mae: 0.6830\n",
            "Epoch 51/60\n",
            "144/144 - 1s - loss: 0.9039 - mae: 0.6752\n",
            "Epoch 52/60\n",
            "144/144 - 0s - loss: 0.8878 - mae: 0.6739\n",
            "Epoch 53/60\n",
            "144/144 - 0s - loss: 0.9017 - mae: 0.6724\n",
            "Epoch 54/60\n",
            "144/144 - 1s - loss: 0.9256 - mae: 0.6838\n",
            "Epoch 55/60\n",
            "144/144 - 1s - loss: 0.8573 - mae: 0.6634\n",
            "Epoch 56/60\n",
            "144/144 - 0s - loss: 0.9574 - mae: 0.6887\n",
            "Epoch 57/60\n",
            "144/144 - 1s - loss: 0.9398 - mae: 0.6916\n",
            "Epoch 58/60\n",
            "144/144 - 1s - loss: 0.9043 - mae: 0.6746\n",
            "Epoch 59/60\n",
            "144/144 - 0s - loss: 0.9070 - mae: 0.6755\n",
            "Epoch 60/60\n",
            "144/144 - 1s - loss: 0.8667 - mae: 0.6658\n",
            "Model: \"functional_1597\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_400 (InputLayer)       [(None, 100, 2, 1)]       0         \n",
            "_________________________________________________________________\n",
            "conv2d_1596 (Conv2D)         (None, 100, 2, 16)        112       \n",
            "_________________________________________________________________\n",
            "batch_normalization_1596 (Ba (None, 100, 2, 16)        64        \n",
            "_________________________________________________________________\n",
            "activation_1995 (Activation) (None, 100, 2, 16)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_399 (MaxPoolin (None, 50, 2, 16)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_1597 (Conv2D)         (None, 50, 2, 12)         1164      \n",
            "_________________________________________________________________\n",
            "batch_normalization_1597 (Ba (None, 50, 2, 12)         48        \n",
            "_________________________________________________________________\n",
            "up_sampling2d_399 (UpSamplin (None, 100, 2, 12)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1598 (Conv2D)         (None, 100, 2, 16)        1168      \n",
            "_________________________________________________________________\n",
            "batch_normalization_1598 (Ba (None, 100, 2, 16)        64        \n",
            "_________________________________________________________________\n",
            "activation_1997 (Activation) (None, 100, 2, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1599 (Conv2D)         (None, 100, 2, 1)         97        \n",
            "=================================================================\n",
            "Total params: 2,717\n",
            "Trainable params: 2,629\n",
            "Non-trainable params: 88\n",
            "_________________________________________________________________\n",
            "Epoch 1/30\n",
            "145/145 - 0s - loss: 0.0319 - mae: 0.0893\n",
            "Epoch 2/30\n",
            "145/145 - 0s - loss: 0.0095 - mae: 0.0552\n",
            "Epoch 3/30\n",
            "145/145 - 0s - loss: 0.0068 - mae: 0.0462\n",
            "Epoch 4/30\n",
            "145/145 - 0s - loss: 0.0058 - mae: 0.0442\n",
            "Epoch 5/30\n",
            "145/145 - 0s - loss: 0.0047 - mae: 0.0392\n",
            "Epoch 6/30\n",
            "145/145 - 0s - loss: 0.0048 - mae: 0.0386\n",
            "Epoch 7/30\n",
            "145/145 - 0s - loss: 0.0037 - mae: 0.0356\n",
            "Epoch 8/30\n",
            "145/145 - 0s - loss: 0.0033 - mae: 0.0337\n",
            "Epoch 9/30\n",
            "145/145 - 0s - loss: 0.0032 - mae: 0.0328\n",
            "Epoch 10/30\n",
            "145/145 - 0s - loss: 0.0043 - mae: 0.0377\n",
            "Epoch 11/30\n",
            "145/145 - 0s - loss: 0.0030 - mae: 0.0315\n",
            "Epoch 12/30\n",
            "145/145 - 0s - loss: 0.0029 - mae: 0.0312\n",
            "Epoch 13/30\n",
            "145/145 - 0s - loss: 0.0026 - mae: 0.0295\n",
            "Epoch 14/30\n",
            "145/145 - 0s - loss: 0.0030 - mae: 0.0314\n",
            "Epoch 15/30\n",
            "145/145 - 0s - loss: 0.0031 - mae: 0.0315\n",
            "Epoch 16/30\n",
            "145/145 - 0s - loss: 0.0031 - mae: 0.0325\n",
            "Epoch 17/30\n",
            "145/145 - 0s - loss: 0.0025 - mae: 0.0282\n",
            "Epoch 18/30\n",
            "145/145 - 0s - loss: 0.0025 - mae: 0.0283\n",
            "Epoch 19/30\n",
            "145/145 - 0s - loss: 0.0021 - mae: 0.0267\n",
            "Epoch 20/30\n",
            "145/145 - 0s - loss: 0.0027 - mae: 0.0304\n",
            "Epoch 21/30\n",
            "145/145 - 0s - loss: 0.0026 - mae: 0.0292\n",
            "Epoch 22/30\n",
            "145/145 - 0s - loss: 0.0029 - mae: 0.0308\n",
            "Epoch 23/30\n",
            "145/145 - 0s - loss: 0.0022 - mae: 0.0271\n",
            "Epoch 24/30\n",
            "145/145 - 0s - loss: 0.0025 - mae: 0.0275\n",
            "Epoch 25/30\n",
            "145/145 - 0s - loss: 0.0023 - mae: 0.0269\n",
            "Epoch 26/30\n",
            "145/145 - 0s - loss: 0.0022 - mae: 0.0265\n",
            "Epoch 27/30\n",
            "145/145 - 0s - loss: 0.0021 - mae: 0.0266\n",
            "Epoch 28/30\n",
            "145/145 - 0s - loss: 0.0026 - mae: 0.0296\n",
            "Epoch 29/30\n",
            "145/145 - 0s - loss: 0.0023 - mae: 0.0274\n",
            "Epoch 30/30\n",
            "145/145 - 0s - loss: 0.0023 - mae: 0.0264\n",
            "Model: \"functional_1599\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_400 (InputLayer)       [(None, 100, 2, 1)]       0         \n",
            "_________________________________________________________________\n",
            "conv2d_1596 (Conv2D)         (None, 100, 2, 16)        112       \n",
            "_________________________________________________________________\n",
            "batch_normalization_1596 (Ba (None, 100, 2, 16)        64        \n",
            "_________________________________________________________________\n",
            "activation_1995 (Activation) (None, 100, 2, 16)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_399 (MaxPoolin (None, 50, 2, 16)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_1597 (Conv2D)         (None, 50, 2, 12)         1164      \n",
            "_________________________________________________________________\n",
            "batch_normalization_1597 (Ba (None, 50, 2, 12)         48        \n",
            "_________________________________________________________________\n",
            "activation_1996 (Activation) (None, 50, 2, 12)         0         \n",
            "_________________________________________________________________\n",
            "flatten_399 (Flatten)        (None, 1200)              0         \n",
            "_________________________________________________________________\n",
            "dense_798 (Dense)            (None, 16)                19216     \n",
            "_________________________________________________________________\n",
            "batch_normalization_1599 (Ba (None, 16)                64        \n",
            "_________________________________________________________________\n",
            "activation_1998 (Activation) (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dropout_399 (Dropout)        (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_799 (Dense)            (None, 3)                 51        \n",
            "_________________________________________________________________\n",
            "activation_1999 (Activation) (None, 3)                 0         \n",
            "=================================================================\n",
            "Total params: 20,719\n",
            "Trainable params: 20,631\n",
            "Non-trainable params: 88\n",
            "_________________________________________________________________\n",
            "Epoch 1/60\n",
            "144/144 - 0s - loss: 1.8703 - mae: 0.9511\n",
            "Epoch 2/60\n",
            "144/144 - 0s - loss: 1.3751 - mae: 0.8275\n",
            "Epoch 3/60\n",
            "144/144 - 0s - loss: 1.2953 - mae: 0.7986\n",
            "Epoch 4/60\n",
            "144/144 - 1s - loss: 1.2676 - mae: 0.7878\n",
            "Epoch 5/60\n",
            "144/144 - 0s - loss: 1.1546 - mae: 0.7560\n",
            "Epoch 6/60\n",
            "144/144 - 0s - loss: 1.1248 - mae: 0.7445\n",
            "Epoch 7/60\n",
            "144/144 - 0s - loss: 1.1658 - mae: 0.7542\n",
            "Epoch 8/60\n",
            "144/144 - 0s - loss: 1.1012 - mae: 0.7401\n",
            "Epoch 9/60\n",
            "144/144 - 0s - loss: 1.1176 - mae: 0.7521\n",
            "Epoch 10/60\n",
            "144/144 - 0s - loss: 1.1357 - mae: 0.7463\n",
            "Epoch 11/60\n",
            "144/144 - 0s - loss: 1.0608 - mae: 0.7310\n",
            "Epoch 12/60\n",
            "144/144 - 0s - loss: 1.0563 - mae: 0.7235\n",
            "Epoch 13/60\n",
            "144/144 - 0s - loss: 1.0677 - mae: 0.7265\n",
            "Epoch 14/60\n",
            "144/144 - 0s - loss: 1.0142 - mae: 0.7113\n",
            "Epoch 15/60\n",
            "144/144 - 0s - loss: 1.0620 - mae: 0.7274\n",
            "Epoch 16/60\n",
            "144/144 - 1s - loss: 1.0127 - mae: 0.7127\n",
            "Epoch 17/60\n",
            "144/144 - 1s - loss: 1.0574 - mae: 0.7239\n",
            "Epoch 18/60\n",
            "144/144 - 0s - loss: 0.9655 - mae: 0.6987\n",
            "Epoch 19/60\n",
            "144/144 - 0s - loss: 0.9803 - mae: 0.6928\n",
            "Epoch 20/60\n",
            "144/144 - 0s - loss: 0.9937 - mae: 0.7105\n",
            "Epoch 21/60\n",
            "144/144 - 0s - loss: 1.0108 - mae: 0.7105\n",
            "Epoch 22/60\n",
            "144/144 - 0s - loss: 1.0299 - mae: 0.7154\n",
            "Epoch 23/60\n",
            "144/144 - 0s - loss: 1.0058 - mae: 0.7104\n",
            "Epoch 24/60\n",
            "144/144 - 0s - loss: 0.9692 - mae: 0.7000\n",
            "Epoch 25/60\n",
            "144/144 - 0s - loss: 0.9497 - mae: 0.6903\n",
            "Epoch 26/60\n",
            "144/144 - 0s - loss: 1.0261 - mae: 0.7099\n",
            "Epoch 27/60\n",
            "144/144 - 0s - loss: 0.9635 - mae: 0.6955\n",
            "Epoch 28/60\n",
            "144/144 - 0s - loss: 0.9539 - mae: 0.6933\n",
            "Epoch 29/60\n",
            "144/144 - 0s - loss: 0.9682 - mae: 0.7020\n",
            "Epoch 30/60\n",
            "144/144 - 0s - loss: 0.9512 - mae: 0.6838\n",
            "Epoch 31/60\n",
            "144/144 - 0s - loss: 0.9956 - mae: 0.7038\n",
            "Epoch 32/60\n",
            "144/144 - 0s - loss: 0.9616 - mae: 0.6947\n",
            "Epoch 33/60\n",
            "144/144 - 0s - loss: 0.9583 - mae: 0.6878\n",
            "Epoch 34/60\n",
            "144/144 - 0s - loss: 0.9482 - mae: 0.6907\n",
            "Epoch 35/60\n",
            "144/144 - 0s - loss: 0.9121 - mae: 0.6770\n",
            "Epoch 36/60\n",
            "144/144 - 0s - loss: 0.9444 - mae: 0.6957\n",
            "Epoch 37/60\n",
            "144/144 - 0s - loss: 0.9115 - mae: 0.6706\n",
            "Epoch 38/60\n",
            "144/144 - 0s - loss: 0.9217 - mae: 0.6780\n",
            "Epoch 39/60\n",
            "144/144 - 0s - loss: 0.8740 - mae: 0.6645\n",
            "Epoch 40/60\n",
            "144/144 - 0s - loss: 0.9349 - mae: 0.6793\n",
            "Epoch 41/60\n",
            "144/144 - 0s - loss: 0.9077 - mae: 0.6771\n",
            "Epoch 42/60\n",
            "144/144 - 0s - loss: 0.9488 - mae: 0.6875\n",
            "Epoch 43/60\n",
            "144/144 - 1s - loss: 0.9074 - mae: 0.6815\n",
            "Epoch 44/60\n",
            "144/144 - 1s - loss: 0.9100 - mae: 0.6718\n",
            "Epoch 45/60\n",
            "144/144 - 0s - loss: 0.9052 - mae: 0.6735\n",
            "Epoch 46/60\n",
            "144/144 - 0s - loss: 0.9378 - mae: 0.6800\n",
            "Epoch 47/60\n",
            "144/144 - 0s - loss: 0.8995 - mae: 0.6664\n",
            "Epoch 48/60\n",
            "144/144 - 0s - loss: 0.9210 - mae: 0.6825\n",
            "Epoch 49/60\n",
            "144/144 - 0s - loss: 0.8911 - mae: 0.6678\n",
            "Epoch 50/60\n",
            "144/144 - 0s - loss: 0.9189 - mae: 0.6787\n",
            "Epoch 51/60\n",
            "144/144 - 0s - loss: 0.9262 - mae: 0.6805\n",
            "Epoch 52/60\n",
            "144/144 - 0s - loss: 0.8657 - mae: 0.6655\n",
            "Epoch 53/60\n",
            "144/144 - 0s - loss: 0.8983 - mae: 0.6690\n",
            "Epoch 54/60\n",
            "144/144 - 0s - loss: 0.8314 - mae: 0.6484\n",
            "Epoch 55/60\n",
            "144/144 - 0s - loss: 0.9085 - mae: 0.6705\n",
            "Epoch 56/60\n",
            "144/144 - 0s - loss: 0.8747 - mae: 0.6600\n",
            "Epoch 57/60\n",
            "144/144 - 0s - loss: 0.9247 - mae: 0.6796\n",
            "Epoch 58/60\n",
            "144/144 - 0s - loss: 0.9050 - mae: 0.6703\n",
            "Epoch 59/60\n",
            "144/144 - 0s - loss: 0.8843 - mae: 0.6680\n",
            "Epoch 60/60\n",
            "144/144 - 0s - loss: 0.8560 - mae: 0.6584\n",
            "Model: \"functional_1601\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_401 (InputLayer)       [(None, 100, 2, 1)]       0         \n",
            "_________________________________________________________________\n",
            "conv2d_1600 (Conv2D)         (None, 100, 2, 16)        112       \n",
            "_________________________________________________________________\n",
            "batch_normalization_1600 (Ba (None, 100, 2, 16)        64        \n",
            "_________________________________________________________________\n",
            "activation_2000 (Activation) (None, 100, 2, 16)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_400 (MaxPoolin (None, 50, 2, 16)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_1601 (Conv2D)         (None, 50, 2, 12)         1164      \n",
            "_________________________________________________________________\n",
            "batch_normalization_1601 (Ba (None, 50, 2, 12)         48        \n",
            "_________________________________________________________________\n",
            "up_sampling2d_400 (UpSamplin (None, 100, 2, 12)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1602 (Conv2D)         (None, 100, 2, 16)        1168      \n",
            "_________________________________________________________________\n",
            "batch_normalization_1602 (Ba (None, 100, 2, 16)        64        \n",
            "_________________________________________________________________\n",
            "activation_2002 (Activation) (None, 100, 2, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1603 (Conv2D)         (None, 100, 2, 1)         97        \n",
            "=================================================================\n",
            "Total params: 2,717\n",
            "Trainable params: 2,629\n",
            "Non-trainable params: 88\n",
            "_________________________________________________________________\n",
            "Epoch 1/30\n",
            "145/145 - 0s - loss: 0.0327 - mae: 0.0910\n",
            "Epoch 2/30\n",
            "145/145 - 0s - loss: 0.0083 - mae: 0.0521\n",
            "Epoch 3/30\n",
            "145/145 - 0s - loss: 0.0061 - mae: 0.0449\n",
            "Epoch 4/30\n",
            "145/145 - 0s - loss: 0.0044 - mae: 0.0387\n",
            "Epoch 5/30\n",
            "145/145 - 0s - loss: 0.0039 - mae: 0.0359\n",
            "Epoch 6/30\n",
            "145/145 - 0s - loss: 0.0045 - mae: 0.0397\n",
            "Epoch 7/30\n",
            "145/145 - 0s - loss: 0.0042 - mae: 0.0378\n",
            "Epoch 8/30\n",
            "145/145 - 0s - loss: 0.0041 - mae: 0.0373\n",
            "Epoch 9/30\n",
            "145/145 - 0s - loss: 0.0035 - mae: 0.0347\n",
            "Epoch 10/30\n",
            "145/145 - 0s - loss: 0.0025 - mae: 0.0289\n",
            "Epoch 11/30\n",
            "145/145 - 0s - loss: 0.0032 - mae: 0.0329\n",
            "Epoch 12/30\n",
            "145/145 - 0s - loss: 0.0032 - mae: 0.0327\n",
            "Epoch 13/30\n",
            "145/145 - 1s - loss: 0.0027 - mae: 0.0294\n",
            "Epoch 14/30\n",
            "145/145 - 0s - loss: 0.0029 - mae: 0.0306\n",
            "Epoch 15/30\n",
            "145/145 - 0s - loss: 0.0023 - mae: 0.0279\n",
            "Epoch 16/30\n",
            "145/145 - 0s - loss: 0.0026 - mae: 0.0298\n",
            "Epoch 17/30\n",
            "145/145 - 0s - loss: 0.0024 - mae: 0.0277\n",
            "Epoch 18/30\n",
            "145/145 - 0s - loss: 0.0027 - mae: 0.0307\n",
            "Epoch 19/30\n",
            "145/145 - 0s - loss: 0.0022 - mae: 0.0268\n",
            "Epoch 20/30\n",
            "145/145 - 0s - loss: 0.0020 - mae: 0.0258\n",
            "Epoch 21/30\n",
            "145/145 - 0s - loss: 0.0026 - mae: 0.0288\n",
            "Epoch 22/30\n",
            "145/145 - 0s - loss: 0.0024 - mae: 0.0280\n",
            "Epoch 23/30\n",
            "145/145 - 0s - loss: 0.0027 - mae: 0.0303\n",
            "Epoch 24/30\n",
            "145/145 - 0s - loss: 0.0020 - mae: 0.0258\n",
            "Epoch 25/30\n",
            "145/145 - 0s - loss: 0.0021 - mae: 0.0258\n",
            "Epoch 26/30\n",
            "145/145 - 0s - loss: 0.0024 - mae: 0.0285\n",
            "Epoch 27/30\n",
            "145/145 - 0s - loss: 0.0022 - mae: 0.0262\n",
            "Epoch 28/30\n",
            "145/145 - 0s - loss: 0.0019 - mae: 0.0257\n",
            "Epoch 29/30\n",
            "145/145 - 0s - loss: 0.0021 - mae: 0.0259\n",
            "Epoch 30/30\n",
            "145/145 - 0s - loss: 0.0025 - mae: 0.0273\n",
            "Model: \"functional_1603\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_401 (InputLayer)       [(None, 100, 2, 1)]       0         \n",
            "_________________________________________________________________\n",
            "conv2d_1600 (Conv2D)         (None, 100, 2, 16)        112       \n",
            "_________________________________________________________________\n",
            "batch_normalization_1600 (Ba (None, 100, 2, 16)        64        \n",
            "_________________________________________________________________\n",
            "activation_2000 (Activation) (None, 100, 2, 16)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_400 (MaxPoolin (None, 50, 2, 16)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_1601 (Conv2D)         (None, 50, 2, 12)         1164      \n",
            "_________________________________________________________________\n",
            "batch_normalization_1601 (Ba (None, 50, 2, 12)         48        \n",
            "_________________________________________________________________\n",
            "activation_2001 (Activation) (None, 50, 2, 12)         0         \n",
            "_________________________________________________________________\n",
            "flatten_400 (Flatten)        (None, 1200)              0         \n",
            "_________________________________________________________________\n",
            "dense_800 (Dense)            (None, 16)                19216     \n",
            "_________________________________________________________________\n",
            "batch_normalization_1603 (Ba (None, 16)                64        \n",
            "_________________________________________________________________\n",
            "activation_2003 (Activation) (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dropout_400 (Dropout)        (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_801 (Dense)            (None, 3)                 51        \n",
            "_________________________________________________________________\n",
            "activation_2004 (Activation) (None, 3)                 0         \n",
            "=================================================================\n",
            "Total params: 20,719\n",
            "Trainable params: 20,631\n",
            "Non-trainable params: 88\n",
            "_________________________________________________________________\n",
            "Epoch 1/60\n",
            "144/144 - 0s - loss: 1.8449 - mae: 0.9272\n",
            "Epoch 2/60\n",
            "144/144 - 0s - loss: 1.4193 - mae: 0.8342\n",
            "Epoch 3/60\n",
            "144/144 - 1s - loss: 1.2902 - mae: 0.7940\n",
            "Epoch 4/60\n",
            "144/144 - 1s - loss: 1.2303 - mae: 0.7776\n",
            "Epoch 5/60\n",
            "144/144 - 0s - loss: 1.2519 - mae: 0.7764\n",
            "Epoch 6/60\n",
            "144/144 - 0s - loss: 1.1773 - mae: 0.7605\n",
            "Epoch 7/60\n",
            "144/144 - 0s - loss: 1.1427 - mae: 0.7524\n",
            "Epoch 8/60\n",
            "144/144 - 0s - loss: 1.1243 - mae: 0.7447\n",
            "Epoch 9/60\n",
            "144/144 - 0s - loss: 1.1374 - mae: 0.7526\n",
            "Epoch 10/60\n",
            "144/144 - 0s - loss: 1.1198 - mae: 0.7449\n",
            "Epoch 11/60\n",
            "144/144 - 0s - loss: 1.1254 - mae: 0.7471\n",
            "Epoch 12/60\n",
            "144/144 - 0s - loss: 1.0834 - mae: 0.7345\n",
            "Epoch 13/60\n",
            "144/144 - 0s - loss: 1.0689 - mae: 0.7354\n",
            "Epoch 14/60\n",
            "144/144 - 0s - loss: 1.0589 - mae: 0.7257\n",
            "Epoch 15/60\n",
            "144/144 - 0s - loss: 1.0270 - mae: 0.7224\n",
            "Epoch 16/60\n",
            "144/144 - 0s - loss: 1.0676 - mae: 0.7299\n",
            "Epoch 17/60\n",
            "144/144 - 0s - loss: 1.0355 - mae: 0.7201\n",
            "Epoch 18/60\n",
            "144/144 - 0s - loss: 0.9996 - mae: 0.7139\n",
            "Epoch 19/60\n",
            "144/144 - 0s - loss: 0.9867 - mae: 0.7070\n",
            "Epoch 20/60\n",
            "144/144 - 0s - loss: 1.0247 - mae: 0.7131\n",
            "Epoch 21/60\n",
            "144/144 - 0s - loss: 1.0119 - mae: 0.7098\n",
            "Epoch 22/60\n",
            "144/144 - 0s - loss: 1.0165 - mae: 0.7145\n",
            "Epoch 23/60\n",
            "144/144 - 0s - loss: 0.9747 - mae: 0.7114\n",
            "Epoch 24/60\n",
            "144/144 - 0s - loss: 0.9531 - mae: 0.6949\n",
            "Epoch 25/60\n",
            "144/144 - 0s - loss: 1.0039 - mae: 0.7131\n",
            "Epoch 26/60\n",
            "144/144 - 0s - loss: 0.9590 - mae: 0.7008\n",
            "Epoch 27/60\n",
            "144/144 - 0s - loss: 0.9823 - mae: 0.7034\n",
            "Epoch 28/60\n",
            "144/144 - 0s - loss: 0.9425 - mae: 0.6957\n",
            "Epoch 29/60\n",
            "144/144 - 1s - loss: 0.9799 - mae: 0.7017\n",
            "Epoch 30/60\n",
            "144/144 - 0s - loss: 0.9479 - mae: 0.6979\n",
            "Epoch 31/60\n",
            "144/144 - 0s - loss: 0.9493 - mae: 0.6905\n",
            "Epoch 32/60\n",
            "144/144 - 0s - loss: 0.9250 - mae: 0.6868\n",
            "Epoch 33/60\n",
            "144/144 - 1s - loss: 0.9351 - mae: 0.6904\n",
            "Epoch 34/60\n",
            "144/144 - 1s - loss: 0.9262 - mae: 0.6784\n",
            "Epoch 35/60\n",
            "144/144 - 0s - loss: 0.9143 - mae: 0.6836\n",
            "Epoch 36/60\n",
            "144/144 - 0s - loss: 0.9504 - mae: 0.6867\n",
            "Epoch 37/60\n",
            "144/144 - 0s - loss: 0.8931 - mae: 0.6696\n",
            "Epoch 38/60\n",
            "144/144 - 0s - loss: 0.8759 - mae: 0.6680\n",
            "Epoch 39/60\n",
            "144/144 - 0s - loss: 0.9281 - mae: 0.6870\n",
            "Epoch 40/60\n",
            "144/144 - 0s - loss: 0.9393 - mae: 0.6865\n",
            "Epoch 41/60\n",
            "144/144 - 0s - loss: 0.9151 - mae: 0.6755\n",
            "Epoch 42/60\n",
            "144/144 - 1s - loss: 0.8834 - mae: 0.6703\n",
            "Epoch 43/60\n",
            "144/144 - 0s - loss: 0.9444 - mae: 0.6890\n",
            "Epoch 44/60\n",
            "144/144 - 0s - loss: 0.9047 - mae: 0.6773\n",
            "Epoch 45/60\n",
            "144/144 - 0s - loss: 0.8997 - mae: 0.6691\n",
            "Epoch 46/60\n",
            "144/144 - 0s - loss: 0.8859 - mae: 0.6671\n",
            "Epoch 47/60\n",
            "144/144 - 0s - loss: 0.8695 - mae: 0.6668\n",
            "Epoch 48/60\n",
            "144/144 - 0s - loss: 0.8908 - mae: 0.6663\n",
            "Epoch 49/60\n",
            "144/144 - 0s - loss: 0.8481 - mae: 0.6554\n",
            "Epoch 50/60\n",
            "144/144 - 0s - loss: 0.9128 - mae: 0.6803\n",
            "Epoch 51/60\n",
            "144/144 - 1s - loss: 0.8311 - mae: 0.6516\n",
            "Epoch 52/60\n",
            "144/144 - 0s - loss: 0.8638 - mae: 0.6588\n",
            "Epoch 53/60\n",
            "144/144 - 0s - loss: 0.8657 - mae: 0.6615\n",
            "Epoch 54/60\n",
            "144/144 - 1s - loss: 0.9182 - mae: 0.6749\n",
            "Epoch 55/60\n",
            "144/144 - 0s - loss: 0.8617 - mae: 0.6643\n",
            "Epoch 56/60\n",
            "144/144 - 0s - loss: 0.8803 - mae: 0.6649\n",
            "Epoch 57/60\n",
            "144/144 - 0s - loss: 0.8405 - mae: 0.6503\n",
            "Epoch 58/60\n",
            "144/144 - 0s - loss: 0.8349 - mae: 0.6443\n",
            "Epoch 59/60\n",
            "144/144 - 0s - loss: 0.9031 - mae: 0.6730\n",
            "Epoch 60/60\n",
            "144/144 - 0s - loss: 0.8274 - mae: 0.6471\n",
            "Model: \"functional_1605\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_402 (InputLayer)       [(None, 100, 2, 1)]       0         \n",
            "_________________________________________________________________\n",
            "conv2d_1604 (Conv2D)         (None, 100, 2, 16)        112       \n",
            "_________________________________________________________________\n",
            "batch_normalization_1604 (Ba (None, 100, 2, 16)        64        \n",
            "_________________________________________________________________\n",
            "activation_2005 (Activation) (None, 100, 2, 16)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_401 (MaxPoolin (None, 50, 2, 16)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_1605 (Conv2D)         (None, 50, 2, 12)         1164      \n",
            "_________________________________________________________________\n",
            "batch_normalization_1605 (Ba (None, 50, 2, 12)         48        \n",
            "_________________________________________________________________\n",
            "up_sampling2d_401 (UpSamplin (None, 100, 2, 12)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1606 (Conv2D)         (None, 100, 2, 16)        1168      \n",
            "_________________________________________________________________\n",
            "batch_normalization_1606 (Ba (None, 100, 2, 16)        64        \n",
            "_________________________________________________________________\n",
            "activation_2007 (Activation) (None, 100, 2, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1607 (Conv2D)         (None, 100, 2, 1)         97        \n",
            "=================================================================\n",
            "Total params: 2,717\n",
            "Trainable params: 2,629\n",
            "Non-trainable params: 88\n",
            "_________________________________________________________________\n",
            "Epoch 1/30\n",
            "145/145 - 0s - loss: 0.0202 - mae: 0.0743\n",
            "Epoch 2/30\n",
            "145/145 - 0s - loss: 0.0069 - mae: 0.0485\n",
            "Epoch 3/30\n",
            "145/145 - 0s - loss: 0.0098 - mae: 0.0565\n",
            "Epoch 4/30\n",
            "145/145 - 0s - loss: 0.0051 - mae: 0.0428\n",
            "Epoch 5/30\n",
            "145/145 - 0s - loss: 0.0040 - mae: 0.0364\n",
            "Epoch 6/30\n",
            "145/145 - 0s - loss: 0.0037 - mae: 0.0349\n",
            "Epoch 7/30\n",
            "145/145 - 1s - loss: 0.0040 - mae: 0.0361\n",
            "Epoch 8/30\n",
            "145/145 - 0s - loss: 0.0035 - mae: 0.0341\n",
            "Epoch 9/30\n",
            "145/145 - 0s - loss: 0.0036 - mae: 0.0350\n",
            "Epoch 10/30\n",
            "145/145 - 0s - loss: 0.0030 - mae: 0.0312\n",
            "Epoch 11/30\n",
            "145/145 - 0s - loss: 0.0038 - mae: 0.0358\n",
            "Epoch 12/30\n",
            "145/145 - 0s - loss: 0.0034 - mae: 0.0337\n",
            "Epoch 13/30\n",
            "145/145 - 0s - loss: 0.0029 - mae: 0.0321\n",
            "Epoch 14/30\n",
            "145/145 - 0s - loss: 0.0029 - mae: 0.0300\n",
            "Epoch 15/30\n",
            "145/145 - 1s - loss: 0.0031 - mae: 0.0318\n",
            "Epoch 16/30\n",
            "145/145 - 0s - loss: 0.0027 - mae: 0.0305\n",
            "Epoch 17/30\n",
            "145/145 - 0s - loss: 0.0031 - mae: 0.0322\n",
            "Epoch 18/30\n",
            "145/145 - 0s - loss: 0.0029 - mae: 0.0300\n",
            "Epoch 19/30\n",
            "145/145 - 0s - loss: 0.0030 - mae: 0.0319\n",
            "Epoch 20/30\n",
            "145/145 - 0s - loss: 0.0028 - mae: 0.0305\n",
            "Epoch 21/30\n",
            "145/145 - 0s - loss: 0.0024 - mae: 0.0284\n",
            "Epoch 22/30\n",
            "145/145 - 0s - loss: 0.0030 - mae: 0.0318\n",
            "Epoch 23/30\n",
            "145/145 - 0s - loss: 0.0024 - mae: 0.0289\n",
            "Epoch 24/30\n",
            "145/145 - 0s - loss: 0.0024 - mae: 0.0286\n",
            "Epoch 25/30\n",
            "145/145 - 0s - loss: 0.0028 - mae: 0.0299\n",
            "Epoch 26/30\n",
            "145/145 - 0s - loss: 0.0023 - mae: 0.0280\n",
            "Epoch 27/30\n",
            "145/145 - 0s - loss: 0.0023 - mae: 0.0277\n",
            "Epoch 28/30\n",
            "145/145 - 0s - loss: 0.0026 - mae: 0.0295\n",
            "Epoch 29/30\n",
            "145/145 - 0s - loss: 0.0027 - mae: 0.0296\n",
            "Epoch 30/30\n",
            "145/145 - 0s - loss: 0.0023 - mae: 0.0276\n",
            "Model: \"functional_1607\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_402 (InputLayer)       [(None, 100, 2, 1)]       0         \n",
            "_________________________________________________________________\n",
            "conv2d_1604 (Conv2D)         (None, 100, 2, 16)        112       \n",
            "_________________________________________________________________\n",
            "batch_normalization_1604 (Ba (None, 100, 2, 16)        64        \n",
            "_________________________________________________________________\n",
            "activation_2005 (Activation) (None, 100, 2, 16)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_401 (MaxPoolin (None, 50, 2, 16)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_1605 (Conv2D)         (None, 50, 2, 12)         1164      \n",
            "_________________________________________________________________\n",
            "batch_normalization_1605 (Ba (None, 50, 2, 12)         48        \n",
            "_________________________________________________________________\n",
            "activation_2006 (Activation) (None, 50, 2, 12)         0         \n",
            "_________________________________________________________________\n",
            "flatten_401 (Flatten)        (None, 1200)              0         \n",
            "_________________________________________________________________\n",
            "dense_802 (Dense)            (None, 16)                19216     \n",
            "_________________________________________________________________\n",
            "batch_normalization_1607 (Ba (None, 16)                64        \n",
            "_________________________________________________________________\n",
            "activation_2008 (Activation) (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dropout_401 (Dropout)        (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_803 (Dense)            (None, 3)                 51        \n",
            "_________________________________________________________________\n",
            "activation_2009 (Activation) (None, 3)                 0         \n",
            "=================================================================\n",
            "Total params: 20,719\n",
            "Trainable params: 20,631\n",
            "Non-trainable params: 88\n",
            "_________________________________________________________________\n",
            "Epoch 1/60\n",
            "144/144 - 0s - loss: 1.9557 - mae: 0.9646\n",
            "Epoch 2/60\n",
            "144/144 - 0s - loss: 1.4233 - mae: 0.8389\n",
            "Epoch 3/60\n",
            "144/144 - 0s - loss: 1.3622 - mae: 0.8230\n",
            "Epoch 4/60\n",
            "144/144 - 1s - loss: 1.2603 - mae: 0.7800\n",
            "Epoch 5/60\n",
            "144/144 - 0s - loss: 1.2283 - mae: 0.7736\n",
            "Epoch 6/60\n",
            "144/144 - 0s - loss: 1.1953 - mae: 0.7716\n",
            "Epoch 7/60\n",
            "144/144 - 1s - loss: 1.1928 - mae: 0.7641\n",
            "Epoch 8/60\n",
            "144/144 - 0s - loss: 1.1480 - mae: 0.7519\n",
            "Epoch 9/60\n",
            "144/144 - 0s - loss: 1.1328 - mae: 0.7574\n",
            "Epoch 10/60\n",
            "144/144 - 0s - loss: 1.0637 - mae: 0.7309\n",
            "Epoch 11/60\n",
            "144/144 - 0s - loss: 1.1140 - mae: 0.7486\n",
            "Epoch 12/60\n",
            "144/144 - 1s - loss: 1.0975 - mae: 0.7389\n",
            "Epoch 13/60\n",
            "144/144 - 1s - loss: 1.0628 - mae: 0.7278\n",
            "Epoch 14/60\n",
            "144/144 - 0s - loss: 1.0640 - mae: 0.7322\n",
            "Epoch 15/60\n",
            "144/144 - 0s - loss: 1.1078 - mae: 0.7451\n",
            "Epoch 16/60\n",
            "144/144 - 1s - loss: 1.0172 - mae: 0.7183\n",
            "Epoch 17/60\n",
            "144/144 - 0s - loss: 1.0359 - mae: 0.7190\n",
            "Epoch 18/60\n",
            "144/144 - 0s - loss: 1.0098 - mae: 0.7104\n",
            "Epoch 19/60\n",
            "144/144 - 0s - loss: 1.0374 - mae: 0.7254\n",
            "Epoch 20/60\n",
            "144/144 - 1s - loss: 1.0681 - mae: 0.7295\n",
            "Epoch 21/60\n",
            "144/144 - 0s - loss: 0.9922 - mae: 0.7025\n",
            "Epoch 22/60\n",
            "144/144 - 0s - loss: 0.9964 - mae: 0.7088\n",
            "Epoch 23/60\n",
            "144/144 - 0s - loss: 0.9684 - mae: 0.7014\n",
            "Epoch 24/60\n",
            "144/144 - 0s - loss: 1.0275 - mae: 0.7161\n",
            "Epoch 25/60\n",
            "144/144 - 0s - loss: 1.0160 - mae: 0.7071\n",
            "Epoch 26/60\n",
            "144/144 - 0s - loss: 0.9902 - mae: 0.7060\n",
            "Epoch 27/60\n",
            "144/144 - 0s - loss: 0.9920 - mae: 0.7017\n",
            "Epoch 28/60\n",
            "144/144 - 1s - loss: 1.0082 - mae: 0.7105\n",
            "Epoch 29/60\n",
            "144/144 - 1s - loss: 0.9681 - mae: 0.6999\n",
            "Epoch 30/60\n",
            "144/144 - 0s - loss: 0.9863 - mae: 0.7038\n",
            "Epoch 31/60\n",
            "144/144 - 0s - loss: 0.9579 - mae: 0.6960\n",
            "Epoch 32/60\n",
            "144/144 - 0s - loss: 0.9600 - mae: 0.6945\n",
            "Epoch 33/60\n",
            "144/144 - 1s - loss: 0.9445 - mae: 0.6866\n",
            "Epoch 34/60\n",
            "144/144 - 1s - loss: 0.9153 - mae: 0.6879\n",
            "Epoch 35/60\n",
            "144/144 - 0s - loss: 0.9654 - mae: 0.6928\n",
            "Epoch 36/60\n",
            "144/144 - 1s - loss: 0.9311 - mae: 0.6880\n",
            "Epoch 37/60\n",
            "144/144 - 1s - loss: 0.9265 - mae: 0.6818\n",
            "Epoch 38/60\n",
            "144/144 - 1s - loss: 0.9608 - mae: 0.6959\n",
            "Epoch 39/60\n",
            "144/144 - 0s - loss: 0.8720 - mae: 0.6658\n",
            "Epoch 40/60\n",
            "144/144 - 1s - loss: 0.9032 - mae: 0.6724\n",
            "Epoch 41/60\n",
            "144/144 - 1s - loss: 0.9144 - mae: 0.6773\n",
            "Epoch 42/60\n",
            "144/144 - 0s - loss: 0.9333 - mae: 0.6897\n",
            "Epoch 43/60\n",
            "144/144 - 0s - loss: 0.8786 - mae: 0.6647\n",
            "Epoch 44/60\n",
            "144/144 - 1s - loss: 0.8870 - mae: 0.6684\n",
            "Epoch 45/60\n",
            "144/144 - 1s - loss: 0.9218 - mae: 0.6806\n",
            "Epoch 46/60\n",
            "144/144 - 1s - loss: 0.9307 - mae: 0.6854\n",
            "Epoch 47/60\n",
            "144/144 - 0s - loss: 0.9032 - mae: 0.6725\n",
            "Epoch 48/60\n",
            "144/144 - 0s - loss: 0.9228 - mae: 0.6818\n",
            "Epoch 49/60\n",
            "144/144 - 1s - loss: 0.8890 - mae: 0.6686\n",
            "Epoch 50/60\n",
            "144/144 - 0s - loss: 0.9101 - mae: 0.6808\n",
            "Epoch 51/60\n",
            "144/144 - 0s - loss: 0.8740 - mae: 0.6669\n",
            "Epoch 52/60\n",
            "144/144 - 0s - loss: 0.8902 - mae: 0.6699\n",
            "Epoch 53/60\n",
            "144/144 - 0s - loss: 0.8878 - mae: 0.6724\n",
            "Epoch 54/60\n",
            "144/144 - 0s - loss: 0.8912 - mae: 0.6732\n",
            "Epoch 55/60\n",
            "144/144 - 0s - loss: 0.8769 - mae: 0.6650\n",
            "Epoch 56/60\n",
            "144/144 - 0s - loss: 0.8879 - mae: 0.6681\n",
            "Epoch 57/60\n",
            "144/144 - 0s - loss: 0.9110 - mae: 0.6760\n",
            "Epoch 58/60\n",
            "144/144 - 0s - loss: 0.8681 - mae: 0.6653\n",
            "Epoch 59/60\n",
            "144/144 - 0s - loss: 0.8877 - mae: 0.6678\n",
            "Epoch 60/60\n",
            "144/144 - 0s - loss: 0.8867 - mae: 0.6706\n",
            "Model: \"functional_1609\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_403 (InputLayer)       [(None, 100, 2, 1)]       0         \n",
            "_________________________________________________________________\n",
            "conv2d_1608 (Conv2D)         (None, 100, 2, 16)        112       \n",
            "_________________________________________________________________\n",
            "batch_normalization_1608 (Ba (None, 100, 2, 16)        64        \n",
            "_________________________________________________________________\n",
            "activation_2010 (Activation) (None, 100, 2, 16)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_402 (MaxPoolin (None, 50, 2, 16)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_1609 (Conv2D)         (None, 50, 2, 12)         1164      \n",
            "_________________________________________________________________\n",
            "batch_normalization_1609 (Ba (None, 50, 2, 12)         48        \n",
            "_________________________________________________________________\n",
            "up_sampling2d_402 (UpSamplin (None, 100, 2, 12)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1610 (Conv2D)         (None, 100, 2, 16)        1168      \n",
            "_________________________________________________________________\n",
            "batch_normalization_1610 (Ba (None, 100, 2, 16)        64        \n",
            "_________________________________________________________________\n",
            "activation_2012 (Activation) (None, 100, 2, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1611 (Conv2D)         (None, 100, 2, 1)         97        \n",
            "=================================================================\n",
            "Total params: 2,717\n",
            "Trainable params: 2,629\n",
            "Non-trainable params: 88\n",
            "_________________________________________________________________\n",
            "Epoch 1/30\n",
            "145/145 - 0s - loss: 0.0245 - mae: 0.0842\n",
            "Epoch 2/30\n",
            "145/145 - 0s - loss: 0.0073 - mae: 0.0527\n",
            "Epoch 3/30\n",
            "145/145 - 0s - loss: 0.0081 - mae: 0.0558\n",
            "Epoch 4/30\n",
            "145/145 - 0s - loss: 0.0049 - mae: 0.0439\n",
            "Epoch 5/30\n",
            "145/145 - 0s - loss: 0.0045 - mae: 0.0405\n",
            "Epoch 6/30\n",
            "145/145 - 0s - loss: 0.0040 - mae: 0.0387\n",
            "Epoch 7/30\n",
            "145/145 - 0s - loss: 0.0035 - mae: 0.0348\n",
            "Epoch 8/30\n",
            "145/145 - 0s - loss: 0.0042 - mae: 0.0373\n",
            "Epoch 9/30\n",
            "145/145 - 0s - loss: 0.0030 - mae: 0.0309\n",
            "Epoch 10/30\n",
            "145/145 - 0s - loss: 0.0029 - mae: 0.0311\n",
            "Epoch 11/30\n",
            "145/145 - 0s - loss: 0.0033 - mae: 0.0325\n",
            "Epoch 12/30\n",
            "145/145 - 0s - loss: 0.0027 - mae: 0.0294\n",
            "Epoch 13/30\n",
            "145/145 - 0s - loss: 0.0029 - mae: 0.0309\n",
            "Epoch 14/30\n",
            "145/145 - 1s - loss: 0.0025 - mae: 0.0281\n",
            "Epoch 15/30\n",
            "145/145 - 0s - loss: 0.0024 - mae: 0.0283\n",
            "Epoch 16/30\n",
            "145/145 - 0s - loss: 0.0026 - mae: 0.0285\n",
            "Epoch 17/30\n",
            "145/145 - 0s - loss: 0.0026 - mae: 0.0281\n",
            "Epoch 18/30\n",
            "145/145 - 0s - loss: 0.0025 - mae: 0.0274\n",
            "Epoch 19/30\n",
            "145/145 - 0s - loss: 0.0029 - mae: 0.0302\n",
            "Epoch 20/30\n",
            "145/145 - 0s - loss: 0.0031 - mae: 0.0306\n",
            "Epoch 21/30\n",
            "145/145 - 0s - loss: 0.0021 - mae: 0.0259\n",
            "Epoch 22/30\n",
            "145/145 - 0s - loss: 0.0026 - mae: 0.0291\n",
            "Epoch 23/30\n",
            "145/145 - 1s - loss: 0.0028 - mae: 0.0297\n",
            "Epoch 24/30\n",
            "145/145 - 0s - loss: 0.0026 - mae: 0.0293\n",
            "Epoch 25/30\n",
            "145/145 - 0s - loss: 0.0024 - mae: 0.0280\n",
            "Epoch 26/30\n",
            "145/145 - 0s - loss: 0.0023 - mae: 0.0273\n",
            "Epoch 27/30\n",
            "145/145 - 0s - loss: 0.0026 - mae: 0.0293\n",
            "Epoch 28/30\n",
            "145/145 - 0s - loss: 0.0024 - mae: 0.0275\n",
            "Epoch 29/30\n",
            "145/145 - 0s - loss: 0.0023 - mae: 0.0272\n",
            "Epoch 30/30\n",
            "145/145 - 0s - loss: 0.0025 - mae: 0.0288\n",
            "Model: \"functional_1611\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_403 (InputLayer)       [(None, 100, 2, 1)]       0         \n",
            "_________________________________________________________________\n",
            "conv2d_1608 (Conv2D)         (None, 100, 2, 16)        112       \n",
            "_________________________________________________________________\n",
            "batch_normalization_1608 (Ba (None, 100, 2, 16)        64        \n",
            "_________________________________________________________________\n",
            "activation_2010 (Activation) (None, 100, 2, 16)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_402 (MaxPoolin (None, 50, 2, 16)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_1609 (Conv2D)         (None, 50, 2, 12)         1164      \n",
            "_________________________________________________________________\n",
            "batch_normalization_1609 (Ba (None, 50, 2, 12)         48        \n",
            "_________________________________________________________________\n",
            "activation_2011 (Activation) (None, 50, 2, 12)         0         \n",
            "_________________________________________________________________\n",
            "flatten_402 (Flatten)        (None, 1200)              0         \n",
            "_________________________________________________________________\n",
            "dense_804 (Dense)            (None, 16)                19216     \n",
            "_________________________________________________________________\n",
            "batch_normalization_1611 (Ba (None, 16)                64        \n",
            "_________________________________________________________________\n",
            "activation_2013 (Activation) (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dropout_402 (Dropout)        (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_805 (Dense)            (None, 3)                 51        \n",
            "_________________________________________________________________\n",
            "activation_2014 (Activation) (None, 3)                 0         \n",
            "=================================================================\n",
            "Total params: 20,719\n",
            "Trainable params: 20,631\n",
            "Non-trainable params: 88\n",
            "_________________________________________________________________\n",
            "Epoch 1/60\n",
            "144/144 - 0s - loss: 1.7361 - mae: 0.9063\n",
            "Epoch 2/60\n",
            "144/144 - 0s - loss: 1.3399 - mae: 0.8180\n",
            "Epoch 3/60\n",
            "144/144 - 0s - loss: 1.2782 - mae: 0.8015\n",
            "Epoch 4/60\n",
            "144/144 - 0s - loss: 1.2493 - mae: 0.7923\n",
            "Epoch 5/60\n",
            "144/144 - 0s - loss: 1.1987 - mae: 0.7795\n",
            "Epoch 6/60\n",
            "144/144 - 0s - loss: 1.1816 - mae: 0.7740\n",
            "Epoch 7/60\n",
            "144/144 - 1s - loss: 1.1223 - mae: 0.7596\n",
            "Epoch 8/60\n",
            "144/144 - 0s - loss: 1.1226 - mae: 0.7517\n",
            "Epoch 9/60\n",
            "144/144 - 0s - loss: 1.0780 - mae: 0.7366\n",
            "Epoch 10/60\n",
            "144/144 - 0s - loss: 1.0847 - mae: 0.7392\n",
            "Epoch 11/60\n",
            "144/144 - 0s - loss: 1.0698 - mae: 0.7378\n",
            "Epoch 12/60\n",
            "144/144 - 0s - loss: 1.0457 - mae: 0.7305\n",
            "Epoch 13/60\n",
            "144/144 - 0s - loss: 1.0595 - mae: 0.7375\n",
            "Epoch 14/60\n",
            "144/144 - 0s - loss: 1.0459 - mae: 0.7338\n",
            "Epoch 15/60\n",
            "144/144 - 0s - loss: 1.0506 - mae: 0.7293\n",
            "Epoch 16/60\n",
            "144/144 - 0s - loss: 0.9782 - mae: 0.7127\n",
            "Epoch 17/60\n",
            "144/144 - 0s - loss: 1.0300 - mae: 0.7228\n",
            "Epoch 18/60\n",
            "144/144 - 0s - loss: 0.9857 - mae: 0.7100\n",
            "Epoch 19/60\n",
            "144/144 - 0s - loss: 1.0125 - mae: 0.7192\n",
            "Epoch 20/60\n",
            "144/144 - 0s - loss: 1.0140 - mae: 0.7163\n",
            "Epoch 21/60\n",
            "144/144 - 1s - loss: 1.0206 - mae: 0.7238\n",
            "Epoch 22/60\n",
            "144/144 - 1s - loss: 0.9686 - mae: 0.7059\n",
            "Epoch 23/60\n",
            "144/144 - 0s - loss: 0.9481 - mae: 0.7023\n",
            "Epoch 24/60\n",
            "144/144 - 0s - loss: 0.9728 - mae: 0.7105\n",
            "Epoch 25/60\n",
            "144/144 - 0s - loss: 0.9721 - mae: 0.7062\n",
            "Epoch 26/60\n",
            "144/144 - 0s - loss: 0.9908 - mae: 0.7154\n",
            "Epoch 27/60\n",
            "144/144 - 0s - loss: 0.9549 - mae: 0.7036\n",
            "Epoch 28/60\n",
            "144/144 - 0s - loss: 0.9521 - mae: 0.6978\n",
            "Epoch 29/60\n",
            "144/144 - 0s - loss: 0.9419 - mae: 0.6878\n",
            "Epoch 30/60\n",
            "144/144 - 0s - loss: 0.9379 - mae: 0.6972\n",
            "Epoch 31/60\n",
            "144/144 - 0s - loss: 0.9423 - mae: 0.6903\n",
            "Epoch 32/60\n",
            "144/144 - 0s - loss: 0.9539 - mae: 0.6997\n",
            "Epoch 33/60\n",
            "144/144 - 1s - loss: 0.9106 - mae: 0.6893\n",
            "Epoch 34/60\n",
            "144/144 - 0s - loss: 0.8762 - mae: 0.6686\n",
            "Epoch 35/60\n",
            "144/144 - 0s - loss: 0.9504 - mae: 0.7007\n",
            "Epoch 36/60\n",
            "144/144 - 0s - loss: 0.9152 - mae: 0.6862\n",
            "Epoch 37/60\n",
            "144/144 - 0s - loss: 0.9109 - mae: 0.6792\n",
            "Epoch 38/60\n",
            "144/144 - 0s - loss: 0.9065 - mae: 0.6831\n",
            "Epoch 39/60\n",
            "144/144 - 0s - loss: 0.8871 - mae: 0.6706\n",
            "Epoch 40/60\n",
            "144/144 - 0s - loss: 0.8919 - mae: 0.6760\n",
            "Epoch 41/60\n",
            "144/144 - 0s - loss: 0.9451 - mae: 0.6910\n",
            "Epoch 42/60\n",
            "144/144 - 0s - loss: 0.8515 - mae: 0.6673\n",
            "Epoch 43/60\n",
            "144/144 - 0s - loss: 0.9461 - mae: 0.6911\n",
            "Epoch 44/60\n",
            "144/144 - 0s - loss: 0.8723 - mae: 0.6688\n",
            "Epoch 45/60\n",
            "144/144 - 0s - loss: 0.8574 - mae: 0.6669\n",
            "Epoch 46/60\n",
            "144/144 - 1s - loss: 0.8995 - mae: 0.6740\n",
            "Epoch 47/60\n",
            "144/144 - 0s - loss: 0.8974 - mae: 0.6774\n",
            "Epoch 48/60\n",
            "144/144 - 0s - loss: 0.9616 - mae: 0.6949\n",
            "Epoch 49/60\n",
            "144/144 - 0s - loss: 0.8996 - mae: 0.6766\n",
            "Epoch 50/60\n",
            "144/144 - 1s - loss: 0.8757 - mae: 0.6693\n",
            "Epoch 51/60\n",
            "144/144 - 0s - loss: 0.8682 - mae: 0.6613\n",
            "Epoch 52/60\n",
            "144/144 - 0s - loss: 0.8791 - mae: 0.6696\n",
            "Epoch 53/60\n",
            "144/144 - 0s - loss: 0.9202 - mae: 0.6802\n",
            "Epoch 54/60\n",
            "144/144 - 1s - loss: 0.8838 - mae: 0.6676\n",
            "Epoch 55/60\n",
            "144/144 - 0s - loss: 0.8864 - mae: 0.6706\n",
            "Epoch 56/60\n",
            "144/144 - 0s - loss: 0.8276 - mae: 0.6533\n",
            "Epoch 57/60\n",
            "144/144 - 0s - loss: 0.8742 - mae: 0.6697\n",
            "Epoch 58/60\n",
            "144/144 - 1s - loss: 0.8481 - mae: 0.6571\n",
            "Epoch 59/60\n",
            "144/144 - 1s - loss: 0.8509 - mae: 0.6628\n",
            "Epoch 60/60\n",
            "144/144 - 1s - loss: 0.8389 - mae: 0.6554\n",
            "Model: \"functional_1613\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_404 (InputLayer)       [(None, 100, 2, 1)]       0         \n",
            "_________________________________________________________________\n",
            "conv2d_1612 (Conv2D)         (None, 100, 2, 16)        112       \n",
            "_________________________________________________________________\n",
            "batch_normalization_1612 (Ba (None, 100, 2, 16)        64        \n",
            "_________________________________________________________________\n",
            "activation_2015 (Activation) (None, 100, 2, 16)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_403 (MaxPoolin (None, 50, 2, 16)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_1613 (Conv2D)         (None, 50, 2, 12)         1164      \n",
            "_________________________________________________________________\n",
            "batch_normalization_1613 (Ba (None, 50, 2, 12)         48        \n",
            "_________________________________________________________________\n",
            "up_sampling2d_403 (UpSamplin (None, 100, 2, 12)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1614 (Conv2D)         (None, 100, 2, 16)        1168      \n",
            "_________________________________________________________________\n",
            "batch_normalization_1614 (Ba (None, 100, 2, 16)        64        \n",
            "_________________________________________________________________\n",
            "activation_2017 (Activation) (None, 100, 2, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1615 (Conv2D)         (None, 100, 2, 1)         97        \n",
            "=================================================================\n",
            "Total params: 2,717\n",
            "Trainable params: 2,629\n",
            "Non-trainable params: 88\n",
            "_________________________________________________________________\n",
            "Epoch 1/30\n",
            "145/145 - 0s - loss: 0.0266 - mae: 0.0859\n",
            "Epoch 2/30\n",
            "145/145 - 0s - loss: 0.0080 - mae: 0.0494\n",
            "Epoch 3/30\n",
            "145/145 - 0s - loss: 0.0053 - mae: 0.0426\n",
            "Epoch 4/30\n",
            "145/145 - 0s - loss: 0.0058 - mae: 0.0446\n",
            "Epoch 5/30\n",
            "145/145 - 0s - loss: 0.0051 - mae: 0.0412\n",
            "Epoch 6/30\n",
            "145/145 - 0s - loss: 0.0077 - mae: 0.0477\n",
            "Epoch 7/30\n",
            "145/145 - 0s - loss: 0.0038 - mae: 0.0350\n",
            "Epoch 8/30\n",
            "145/145 - 0s - loss: 0.0040 - mae: 0.0356\n",
            "Epoch 9/30\n",
            "145/145 - 0s - loss: 0.0032 - mae: 0.0319\n",
            "Epoch 10/30\n",
            "145/145 - 0s - loss: 0.0033 - mae: 0.0324\n",
            "Epoch 11/30\n",
            "145/145 - 0s - loss: 0.0038 - mae: 0.0353\n",
            "Epoch 12/30\n",
            "145/145 - 0s - loss: 0.0036 - mae: 0.0340\n",
            "Epoch 13/30\n",
            "145/145 - 0s - loss: 0.0029 - mae: 0.0298\n",
            "Epoch 14/30\n",
            "145/145 - 0s - loss: 0.0036 - mae: 0.0325\n",
            "Epoch 15/30\n",
            "145/145 - 0s - loss: 0.0029 - mae: 0.0306\n",
            "Epoch 16/30\n",
            "145/145 - 0s - loss: 0.0028 - mae: 0.0295\n",
            "Epoch 17/30\n",
            "145/145 - 0s - loss: 0.0038 - mae: 0.0324\n",
            "Epoch 18/30\n",
            "145/145 - 0s - loss: 0.0032 - mae: 0.0311\n",
            "Epoch 19/30\n",
            "145/145 - 0s - loss: 0.0027 - mae: 0.0298\n",
            "Epoch 20/30\n",
            "145/145 - 0s - loss: 0.0025 - mae: 0.0280\n",
            "Epoch 21/30\n",
            "145/145 - 0s - loss: 0.0027 - mae: 0.0299\n",
            "Epoch 22/30\n",
            "145/145 - 0s - loss: 0.0021 - mae: 0.0270\n",
            "Epoch 23/30\n",
            "145/145 - 0s - loss: 0.0020 - mae: 0.0260\n",
            "Epoch 24/30\n",
            "145/145 - 0s - loss: 0.0023 - mae: 0.0277\n",
            "Epoch 25/30\n",
            "145/145 - 0s - loss: 0.0022 - mae: 0.0269\n",
            "Epoch 26/30\n",
            "145/145 - 0s - loss: 0.0023 - mae: 0.0274\n",
            "Epoch 27/30\n",
            "145/145 - 0s - loss: 0.0023 - mae: 0.0280\n",
            "Epoch 28/30\n",
            "145/145 - 0s - loss: 0.0021 - mae: 0.0270\n",
            "Epoch 29/30\n",
            "145/145 - 0s - loss: 0.0022 - mae: 0.0264\n",
            "Epoch 30/30\n",
            "145/145 - 0s - loss: 0.0020 - mae: 0.0253\n",
            "Model: \"functional_1615\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_404 (InputLayer)       [(None, 100, 2, 1)]       0         \n",
            "_________________________________________________________________\n",
            "conv2d_1612 (Conv2D)         (None, 100, 2, 16)        112       \n",
            "_________________________________________________________________\n",
            "batch_normalization_1612 (Ba (None, 100, 2, 16)        64        \n",
            "_________________________________________________________________\n",
            "activation_2015 (Activation) (None, 100, 2, 16)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_403 (MaxPoolin (None, 50, 2, 16)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_1613 (Conv2D)         (None, 50, 2, 12)         1164      \n",
            "_________________________________________________________________\n",
            "batch_normalization_1613 (Ba (None, 50, 2, 12)         48        \n",
            "_________________________________________________________________\n",
            "activation_2016 (Activation) (None, 50, 2, 12)         0         \n",
            "_________________________________________________________________\n",
            "flatten_403 (Flatten)        (None, 1200)              0         \n",
            "_________________________________________________________________\n",
            "dense_806 (Dense)            (None, 16)                19216     \n",
            "_________________________________________________________________\n",
            "batch_normalization_1615 (Ba (None, 16)                64        \n",
            "_________________________________________________________________\n",
            "activation_2018 (Activation) (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dropout_403 (Dropout)        (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_807 (Dense)            (None, 3)                 51        \n",
            "_________________________________________________________________\n",
            "activation_2019 (Activation) (None, 3)                 0         \n",
            "=================================================================\n",
            "Total params: 20,719\n",
            "Trainable params: 20,631\n",
            "Non-trainable params: 88\n",
            "_________________________________________________________________\n",
            "Epoch 1/60\n",
            "144/144 - 0s - loss: 1.8009 - mae: 0.9326\n",
            "Epoch 2/60\n",
            "144/144 - 0s - loss: 1.3611 - mae: 0.8279\n",
            "Epoch 3/60\n",
            "144/144 - 1s - loss: 1.2784 - mae: 0.8051\n",
            "Epoch 4/60\n",
            "144/144 - 0s - loss: 1.2039 - mae: 0.7784\n",
            "Epoch 5/60\n",
            "144/144 - 1s - loss: 1.2128 - mae: 0.7767\n",
            "Epoch 6/60\n",
            "144/144 - 1s - loss: 1.1814 - mae: 0.7669\n",
            "Epoch 7/60\n",
            "144/144 - 0s - loss: 1.1360 - mae: 0.7543\n",
            "Epoch 8/60\n",
            "144/144 - 0s - loss: 1.1482 - mae: 0.7507\n",
            "Epoch 9/60\n",
            "144/144 - 0s - loss: 1.1027 - mae: 0.7450\n",
            "Epoch 10/60\n",
            "144/144 - 0s - loss: 1.0767 - mae: 0.7363\n",
            "Epoch 11/60\n",
            "144/144 - 0s - loss: 1.0380 - mae: 0.7259\n",
            "Epoch 12/60\n",
            "144/144 - 1s - loss: 1.0280 - mae: 0.7264\n",
            "Epoch 13/60\n",
            "144/144 - 0s - loss: 1.0448 - mae: 0.7270\n",
            "Epoch 14/60\n",
            "144/144 - 0s - loss: 1.0528 - mae: 0.7300\n",
            "Epoch 15/60\n",
            "144/144 - 0s - loss: 1.0125 - mae: 0.7148\n",
            "Epoch 16/60\n",
            "144/144 - 0s - loss: 0.9870 - mae: 0.7059\n",
            "Epoch 17/60\n",
            "144/144 - 0s - loss: 1.0201 - mae: 0.7173\n",
            "Epoch 18/60\n",
            "144/144 - 1s - loss: 0.9931 - mae: 0.7072\n",
            "Epoch 19/60\n",
            "144/144 - 0s - loss: 0.9797 - mae: 0.7045\n",
            "Epoch 20/60\n",
            "144/144 - 0s - loss: 0.9838 - mae: 0.7074\n",
            "Epoch 21/60\n",
            "144/144 - 0s - loss: 0.9749 - mae: 0.7036\n",
            "Epoch 22/60\n",
            "144/144 - 0s - loss: 0.9528 - mae: 0.6921\n",
            "Epoch 23/60\n",
            "144/144 - 0s - loss: 0.9438 - mae: 0.6881\n",
            "Epoch 24/60\n",
            "144/144 - 0s - loss: 0.9147 - mae: 0.6794\n",
            "Epoch 25/60\n",
            "144/144 - 0s - loss: 0.9676 - mae: 0.6993\n",
            "Epoch 26/60\n",
            "144/144 - 0s - loss: 0.9530 - mae: 0.6937\n",
            "Epoch 27/60\n",
            "144/144 - 0s - loss: 0.9448 - mae: 0.6925\n",
            "Epoch 28/60\n",
            "144/144 - 0s - loss: 0.9301 - mae: 0.6847\n",
            "Epoch 29/60\n",
            "144/144 - 1s - loss: 0.9130 - mae: 0.6847\n",
            "Epoch 30/60\n",
            "144/144 - 0s - loss: 0.9075 - mae: 0.6783\n",
            "Epoch 31/60\n",
            "144/144 - 0s - loss: 0.8959 - mae: 0.6697\n",
            "Epoch 32/60\n",
            "144/144 - 1s - loss: 0.9322 - mae: 0.6858\n",
            "Epoch 33/60\n",
            "144/144 - 1s - loss: 0.9283 - mae: 0.6860\n",
            "Epoch 34/60\n",
            "144/144 - 0s - loss: 0.9119 - mae: 0.6709\n",
            "Epoch 35/60\n",
            "144/144 - 0s - loss: 0.9148 - mae: 0.6732\n",
            "Epoch 36/60\n",
            "144/144 - 0s - loss: 0.8977 - mae: 0.6776\n",
            "Epoch 37/60\n",
            "144/144 - 0s - loss: 0.8631 - mae: 0.6584\n",
            "Epoch 38/60\n",
            "144/144 - 0s - loss: 0.9048 - mae: 0.6713\n",
            "Epoch 39/60\n",
            "144/144 - 0s - loss: 0.9002 - mae: 0.6758\n",
            "Epoch 40/60\n",
            "144/144 - 0s - loss: 0.9404 - mae: 0.6812\n",
            "Epoch 41/60\n",
            "144/144 - 0s - loss: 0.8870 - mae: 0.6731\n",
            "Epoch 42/60\n",
            "144/144 - 0s - loss: 0.8972 - mae: 0.6711\n",
            "Epoch 43/60\n",
            "144/144 - 0s - loss: 0.8515 - mae: 0.6553\n",
            "Epoch 44/60\n",
            "144/144 - 0s - loss: 0.9121 - mae: 0.6722\n",
            "Epoch 45/60\n",
            "144/144 - 0s - loss: 0.8922 - mae: 0.6650\n",
            "Epoch 46/60\n",
            "144/144 - 0s - loss: 0.8880 - mae: 0.6657\n",
            "Epoch 47/60\n",
            "144/144 - 0s - loss: 0.8638 - mae: 0.6603\n",
            "Epoch 48/60\n",
            "144/144 - 0s - loss: 0.8976 - mae: 0.6694\n",
            "Epoch 49/60\n",
            "144/144 - 0s - loss: 0.9060 - mae: 0.6752\n",
            "Epoch 50/60\n",
            "144/144 - 0s - loss: 0.8377 - mae: 0.6523\n",
            "Epoch 51/60\n",
            "144/144 - 0s - loss: 0.8082 - mae: 0.6437\n",
            "Epoch 52/60\n",
            "144/144 - 1s - loss: 0.8321 - mae: 0.6416\n",
            "Epoch 53/60\n",
            "144/144 - 0s - loss: 0.8424 - mae: 0.6510\n",
            "Epoch 54/60\n",
            "144/144 - 0s - loss: 0.8615 - mae: 0.6552\n",
            "Epoch 55/60\n",
            "144/144 - 0s - loss: 0.8275 - mae: 0.6467\n",
            "Epoch 56/60\n",
            "144/144 - 0s - loss: 0.8831 - mae: 0.6629\n",
            "Epoch 57/60\n",
            "144/144 - 0s - loss: 0.8314 - mae: 0.6465\n",
            "Epoch 58/60\n",
            "144/144 - 0s - loss: 0.8584 - mae: 0.6553\n",
            "Epoch 59/60\n",
            "144/144 - 0s - loss: 0.8500 - mae: 0.6501\n",
            "Epoch 60/60\n",
            "144/144 - 0s - loss: 0.8643 - mae: 0.6541\n",
            "Model: \"functional_1617\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_405 (InputLayer)       [(None, 100, 2, 1)]       0         \n",
            "_________________________________________________________________\n",
            "conv2d_1616 (Conv2D)         (None, 100, 2, 16)        112       \n",
            "_________________________________________________________________\n",
            "batch_normalization_1616 (Ba (None, 100, 2, 16)        64        \n",
            "_________________________________________________________________\n",
            "activation_2020 (Activation) (None, 100, 2, 16)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_404 (MaxPoolin (None, 50, 2, 16)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_1617 (Conv2D)         (None, 50, 2, 12)         1164      \n",
            "_________________________________________________________________\n",
            "batch_normalization_1617 (Ba (None, 50, 2, 12)         48        \n",
            "_________________________________________________________________\n",
            "up_sampling2d_404 (UpSamplin (None, 100, 2, 12)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1618 (Conv2D)         (None, 100, 2, 16)        1168      \n",
            "_________________________________________________________________\n",
            "batch_normalization_1618 (Ba (None, 100, 2, 16)        64        \n",
            "_________________________________________________________________\n",
            "activation_2022 (Activation) (None, 100, 2, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1619 (Conv2D)         (None, 100, 2, 1)         97        \n",
            "=================================================================\n",
            "Total params: 2,717\n",
            "Trainable params: 2,629\n",
            "Non-trainable params: 88\n",
            "_________________________________________________________________\n",
            "Epoch 1/30\n",
            "145/145 - 0s - loss: 0.0286 - mae: 0.0843\n",
            "Epoch 2/30\n",
            "145/145 - 0s - loss: 0.0084 - mae: 0.0506\n",
            "Epoch 3/30\n",
            "145/145 - 0s - loss: 0.0065 - mae: 0.0442\n",
            "Epoch 4/30\n",
            "145/145 - 0s - loss: 0.0056 - mae: 0.0432\n",
            "Epoch 5/30\n",
            "145/145 - 0s - loss: 0.0053 - mae: 0.0401\n",
            "Epoch 6/30\n",
            "145/145 - 0s - loss: 0.0041 - mae: 0.0353\n",
            "Epoch 7/30\n",
            "145/145 - 0s - loss: 0.0039 - mae: 0.0348\n",
            "Epoch 8/30\n",
            "145/145 - 0s - loss: 0.0034 - mae: 0.0330\n",
            "Epoch 9/30\n",
            "145/145 - 0s - loss: 0.0033 - mae: 0.0314\n",
            "Epoch 10/30\n",
            "145/145 - 0s - loss: 0.0037 - mae: 0.0338\n",
            "Epoch 11/30\n",
            "145/145 - 0s - loss: 0.0028 - mae: 0.0299\n",
            "Epoch 12/30\n",
            "145/145 - 0s - loss: 0.0029 - mae: 0.0304\n",
            "Epoch 13/30\n",
            "145/145 - 0s - loss: 0.0026 - mae: 0.0288\n",
            "Epoch 14/30\n",
            "145/145 - 0s - loss: 0.0031 - mae: 0.0327\n",
            "Epoch 15/30\n",
            "145/145 - 0s - loss: 0.0024 - mae: 0.0280\n",
            "Epoch 16/30\n",
            "145/145 - 0s - loss: 0.0028 - mae: 0.0304\n",
            "Epoch 17/30\n",
            "145/145 - 0s - loss: 0.0024 - mae: 0.0281\n",
            "Epoch 18/30\n",
            "145/145 - 0s - loss: 0.0026 - mae: 0.0283\n",
            "Epoch 19/30\n",
            "145/145 - 0s - loss: 0.0027 - mae: 0.0298\n",
            "Epoch 20/30\n",
            "145/145 - 0s - loss: 0.0027 - mae: 0.0298\n",
            "Epoch 21/30\n",
            "145/145 - 0s - loss: 0.0031 - mae: 0.0322\n",
            "Epoch 22/30\n",
            "145/145 - 0s - loss: 0.0025 - mae: 0.0285\n",
            "Epoch 23/30\n",
            "145/145 - 0s - loss: 0.0022 - mae: 0.0271\n",
            "Epoch 24/30\n",
            "145/145 - 0s - loss: 0.0025 - mae: 0.0283\n",
            "Epoch 25/30\n",
            "145/145 - 0s - loss: 0.0026 - mae: 0.0305\n",
            "Epoch 26/30\n",
            "145/145 - 0s - loss: 0.0033 - mae: 0.0327\n",
            "Epoch 27/30\n",
            "145/145 - 0s - loss: 0.0028 - mae: 0.0298\n",
            "Epoch 28/30\n",
            "145/145 - 0s - loss: 0.0024 - mae: 0.0282\n",
            "Epoch 29/30\n",
            "145/145 - 0s - loss: 0.0025 - mae: 0.0277\n",
            "Epoch 30/30\n",
            "145/145 - 0s - loss: 0.0022 - mae: 0.0272\n",
            "Model: \"functional_1619\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_405 (InputLayer)       [(None, 100, 2, 1)]       0         \n",
            "_________________________________________________________________\n",
            "conv2d_1616 (Conv2D)         (None, 100, 2, 16)        112       \n",
            "_________________________________________________________________\n",
            "batch_normalization_1616 (Ba (None, 100, 2, 16)        64        \n",
            "_________________________________________________________________\n",
            "activation_2020 (Activation) (None, 100, 2, 16)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_404 (MaxPoolin (None, 50, 2, 16)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_1617 (Conv2D)         (None, 50, 2, 12)         1164      \n",
            "_________________________________________________________________\n",
            "batch_normalization_1617 (Ba (None, 50, 2, 12)         48        \n",
            "_________________________________________________________________\n",
            "activation_2021 (Activation) (None, 50, 2, 12)         0         \n",
            "_________________________________________________________________\n",
            "flatten_404 (Flatten)        (None, 1200)              0         \n",
            "_________________________________________________________________\n",
            "dense_808 (Dense)            (None, 16)                19216     \n",
            "_________________________________________________________________\n",
            "batch_normalization_1619 (Ba (None, 16)                64        \n",
            "_________________________________________________________________\n",
            "activation_2023 (Activation) (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dropout_404 (Dropout)        (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_809 (Dense)            (None, 3)                 51        \n",
            "_________________________________________________________________\n",
            "activation_2024 (Activation) (None, 3)                 0         \n",
            "=================================================================\n",
            "Total params: 20,719\n",
            "Trainable params: 20,631\n",
            "Non-trainable params: 88\n",
            "_________________________________________________________________\n",
            "Epoch 1/60\n",
            "144/144 - 0s - loss: 1.7746 - mae: 0.9068\n",
            "Epoch 2/60\n",
            "144/144 - 0s - loss: 1.3892 - mae: 0.8394\n",
            "Epoch 3/60\n",
            "144/144 - 0s - loss: 1.3534 - mae: 0.8185\n",
            "Epoch 4/60\n",
            "144/144 - 0s - loss: 1.2552 - mae: 0.7969\n",
            "Epoch 5/60\n",
            "144/144 - 0s - loss: 1.2697 - mae: 0.7945\n",
            "Epoch 6/60\n",
            "144/144 - 0s - loss: 1.2613 - mae: 0.7889\n",
            "Epoch 7/60\n",
            "144/144 - 0s - loss: 1.1283 - mae: 0.7602\n",
            "Epoch 8/60\n",
            "144/144 - 0s - loss: 1.1309 - mae: 0.7560\n",
            "Epoch 9/60\n",
            "144/144 - 0s - loss: 1.1499 - mae: 0.7648\n",
            "Epoch 10/60\n",
            "144/144 - 0s - loss: 1.1011 - mae: 0.7516\n",
            "Epoch 11/60\n",
            "144/144 - 0s - loss: 1.0660 - mae: 0.7362\n",
            "Epoch 12/60\n",
            "144/144 - 0s - loss: 1.1289 - mae: 0.7579\n",
            "Epoch 13/60\n",
            "144/144 - 1s - loss: 1.0808 - mae: 0.7445\n",
            "Epoch 14/60\n",
            "144/144 - 0s - loss: 1.0709 - mae: 0.7356\n",
            "Epoch 15/60\n",
            "144/144 - 0s - loss: 1.0151 - mae: 0.7210\n",
            "Epoch 16/60\n",
            "144/144 - 0s - loss: 1.0800 - mae: 0.7409\n",
            "Epoch 17/60\n",
            "144/144 - 0s - loss: 1.0833 - mae: 0.7435\n",
            "Epoch 18/60\n",
            "144/144 - 0s - loss: 1.0506 - mae: 0.7278\n",
            "Epoch 19/60\n",
            "144/144 - 0s - loss: 1.0033 - mae: 0.7173\n",
            "Epoch 20/60\n",
            "144/144 - 1s - loss: 0.9946 - mae: 0.7140\n",
            "Epoch 21/60\n",
            "144/144 - 0s - loss: 1.0289 - mae: 0.7254\n",
            "Epoch 22/60\n",
            "144/144 - 0s - loss: 1.0162 - mae: 0.7155\n",
            "Epoch 23/60\n",
            "144/144 - 0s - loss: 1.0039 - mae: 0.7167\n",
            "Epoch 24/60\n",
            "144/144 - 0s - loss: 0.9692 - mae: 0.7082\n",
            "Epoch 25/60\n",
            "144/144 - 0s - loss: 1.0049 - mae: 0.7173\n",
            "Epoch 26/60\n",
            "144/144 - 0s - loss: 1.0016 - mae: 0.7155\n",
            "Epoch 27/60\n",
            "144/144 - 0s - loss: 0.9744 - mae: 0.7050\n",
            "Epoch 28/60\n",
            "144/144 - 0s - loss: 0.9563 - mae: 0.7006\n",
            "Epoch 29/60\n",
            "144/144 - 0s - loss: 0.9852 - mae: 0.7116\n",
            "Epoch 30/60\n",
            "144/144 - 1s - loss: 0.9455 - mae: 0.6987\n",
            "Epoch 31/60\n",
            "144/144 - 0s - loss: 0.9175 - mae: 0.6925\n",
            "Epoch 32/60\n",
            "144/144 - 1s - loss: 0.8956 - mae: 0.6823\n",
            "Epoch 33/60\n",
            "144/144 - 1s - loss: 0.9889 - mae: 0.7145\n",
            "Epoch 34/60\n",
            "144/144 - 0s - loss: 0.9154 - mae: 0.6901\n",
            "Epoch 35/60\n",
            "144/144 - 0s - loss: 0.9344 - mae: 0.6930\n",
            "Epoch 36/60\n",
            "144/144 - 0s - loss: 0.9727 - mae: 0.7065\n",
            "Epoch 37/60\n",
            "144/144 - 0s - loss: 0.8994 - mae: 0.6802\n",
            "Epoch 38/60\n",
            "144/144 - 0s - loss: 0.9788 - mae: 0.7124\n",
            "Epoch 39/60\n",
            "144/144 - 0s - loss: 0.9751 - mae: 0.7094\n",
            "Epoch 40/60\n",
            "144/144 - 0s - loss: 0.9031 - mae: 0.6866\n",
            "Epoch 41/60\n",
            "144/144 - 0s - loss: 0.9099 - mae: 0.6903\n",
            "Epoch 42/60\n",
            "144/144 - 0s - loss: 0.9225 - mae: 0.6907\n",
            "Epoch 43/60\n",
            "144/144 - 0s - loss: 0.9315 - mae: 0.6904\n",
            "Epoch 44/60\n",
            "144/144 - 0s - loss: 0.9025 - mae: 0.6866\n",
            "Epoch 45/60\n",
            "144/144 - 0s - loss: 0.9320 - mae: 0.6904\n",
            "Epoch 46/60\n",
            "144/144 - 0s - loss: 0.8911 - mae: 0.6768\n",
            "Epoch 47/60\n",
            "144/144 - 0s - loss: 0.9048 - mae: 0.6809\n",
            "Epoch 48/60\n",
            "144/144 - 0s - loss: 0.9204 - mae: 0.6899\n",
            "Epoch 49/60\n",
            "144/144 - 0s - loss: 0.8818 - mae: 0.6794\n",
            "Epoch 50/60\n",
            "144/144 - 0s - loss: 0.9180 - mae: 0.6911\n",
            "Epoch 51/60\n",
            "144/144 - 0s - loss: 0.8555 - mae: 0.6660\n",
            "Epoch 52/60\n",
            "144/144 - 0s - loss: 0.9089 - mae: 0.6826\n",
            "Epoch 53/60\n",
            "144/144 - 0s - loss: 0.9056 - mae: 0.6752\n",
            "Epoch 54/60\n",
            "144/144 - 1s - loss: 0.8467 - mae: 0.6681\n",
            "Epoch 55/60\n",
            "144/144 - 1s - loss: 0.9096 - mae: 0.6850\n",
            "Epoch 56/60\n",
            "144/144 - 0s - loss: 0.8817 - mae: 0.6740\n",
            "Epoch 57/60\n",
            "144/144 - 0s - loss: 0.8605 - mae: 0.6706\n",
            "Epoch 58/60\n",
            "144/144 - 0s - loss: 0.8820 - mae: 0.6753\n",
            "Epoch 59/60\n",
            "144/144 - 1s - loss: 0.8926 - mae: 0.6768\n",
            "Epoch 60/60\n",
            "144/144 - 0s - loss: 0.8629 - mae: 0.6672\n",
            "Model: \"functional_1621\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_406 (InputLayer)       [(None, 100, 2, 1)]       0         \n",
            "_________________________________________________________________\n",
            "conv2d_1620 (Conv2D)         (None, 100, 2, 16)        112       \n",
            "_________________________________________________________________\n",
            "batch_normalization_1620 (Ba (None, 100, 2, 16)        64        \n",
            "_________________________________________________________________\n",
            "activation_2025 (Activation) (None, 100, 2, 16)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_405 (MaxPoolin (None, 50, 2, 16)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_1621 (Conv2D)         (None, 50, 2, 12)         1164      \n",
            "_________________________________________________________________\n",
            "batch_normalization_1621 (Ba (None, 50, 2, 12)         48        \n",
            "_________________________________________________________________\n",
            "up_sampling2d_405 (UpSamplin (None, 100, 2, 12)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1622 (Conv2D)         (None, 100, 2, 16)        1168      \n",
            "_________________________________________________________________\n",
            "batch_normalization_1622 (Ba (None, 100, 2, 16)        64        \n",
            "_________________________________________________________________\n",
            "activation_2027 (Activation) (None, 100, 2, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1623 (Conv2D)         (None, 100, 2, 1)         97        \n",
            "=================================================================\n",
            "Total params: 2,717\n",
            "Trainable params: 2,629\n",
            "Non-trainable params: 88\n",
            "_________________________________________________________________\n",
            "Epoch 1/30\n",
            "145/145 - 0s - loss: 0.0234 - mae: 0.0807\n",
            "Epoch 2/30\n",
            "145/145 - 0s - loss: 0.0078 - mae: 0.0530\n",
            "Epoch 3/30\n",
            "145/145 - 0s - loss: 0.0050 - mae: 0.0425\n",
            "Epoch 4/30\n",
            "145/145 - 0s - loss: 0.0047 - mae: 0.0415\n",
            "Epoch 5/30\n",
            "145/145 - 0s - loss: 0.0037 - mae: 0.0357\n",
            "Epoch 6/30\n",
            "145/145 - 0s - loss: 0.0044 - mae: 0.0398\n",
            "Epoch 7/30\n",
            "145/145 - 0s - loss: 0.0040 - mae: 0.0360\n",
            "Epoch 8/30\n",
            "145/145 - 0s - loss: 0.0039 - mae: 0.0372\n",
            "Epoch 9/30\n",
            "145/145 - 0s - loss: 0.0032 - mae: 0.0323\n",
            "Epoch 10/30\n",
            "145/145 - 0s - loss: 0.0031 - mae: 0.0311\n",
            "Epoch 11/30\n",
            "145/145 - 0s - loss: 0.0030 - mae: 0.0315\n",
            "Epoch 12/30\n",
            "145/145 - 0s - loss: 0.0031 - mae: 0.0310\n",
            "Epoch 13/30\n",
            "145/145 - 0s - loss: 0.0027 - mae: 0.0299\n",
            "Epoch 14/30\n",
            "145/145 - 0s - loss: 0.0029 - mae: 0.0299\n",
            "Epoch 15/30\n",
            "145/145 - 0s - loss: 0.0028 - mae: 0.0298\n",
            "Epoch 16/30\n",
            "145/145 - 0s - loss: 0.0028 - mae: 0.0292\n",
            "Epoch 17/30\n",
            "145/145 - 0s - loss: 0.0024 - mae: 0.0276\n",
            "Epoch 18/30\n",
            "145/145 - 0s - loss: 0.0025 - mae: 0.0272\n",
            "Epoch 19/30\n",
            "145/145 - 0s - loss: 0.0027 - mae: 0.0286\n",
            "Epoch 20/30\n",
            "145/145 - 0s - loss: 0.0029 - mae: 0.0303\n",
            "Epoch 21/30\n",
            "145/145 - 0s - loss: 0.0023 - mae: 0.0264\n",
            "Epoch 22/30\n",
            "145/145 - 0s - loss: 0.0025 - mae: 0.0283\n",
            "Epoch 23/30\n",
            "145/145 - 0s - loss: 0.0024 - mae: 0.0276\n",
            "Epoch 24/30\n",
            "145/145 - 1s - loss: 0.0023 - mae: 0.0267\n",
            "Epoch 25/30\n",
            "145/145 - 0s - loss: 0.0025 - mae: 0.0287\n",
            "Epoch 26/30\n",
            "145/145 - 0s - loss: 0.0027 - mae: 0.0305\n",
            "Epoch 27/30\n",
            "145/145 - 0s - loss: 0.0023 - mae: 0.0267\n",
            "Epoch 28/30\n",
            "145/145 - 0s - loss: 0.0025 - mae: 0.0285\n",
            "Epoch 29/30\n",
            "145/145 - 0s - loss: 0.0021 - mae: 0.0255\n",
            "Epoch 30/30\n",
            "145/145 - 0s - loss: 0.0029 - mae: 0.0303\n",
            "Model: \"functional_1623\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_406 (InputLayer)       [(None, 100, 2, 1)]       0         \n",
            "_________________________________________________________________\n",
            "conv2d_1620 (Conv2D)         (None, 100, 2, 16)        112       \n",
            "_________________________________________________________________\n",
            "batch_normalization_1620 (Ba (None, 100, 2, 16)        64        \n",
            "_________________________________________________________________\n",
            "activation_2025 (Activation) (None, 100, 2, 16)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_405 (MaxPoolin (None, 50, 2, 16)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_1621 (Conv2D)         (None, 50, 2, 12)         1164      \n",
            "_________________________________________________________________\n",
            "batch_normalization_1621 (Ba (None, 50, 2, 12)         48        \n",
            "_________________________________________________________________\n",
            "activation_2026 (Activation) (None, 50, 2, 12)         0         \n",
            "_________________________________________________________________\n",
            "flatten_405 (Flatten)        (None, 1200)              0         \n",
            "_________________________________________________________________\n",
            "dense_810 (Dense)            (None, 16)                19216     \n",
            "_________________________________________________________________\n",
            "batch_normalization_1623 (Ba (None, 16)                64        \n",
            "_________________________________________________________________\n",
            "activation_2028 (Activation) (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dropout_405 (Dropout)        (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_811 (Dense)            (None, 3)                 51        \n",
            "_________________________________________________________________\n",
            "activation_2029 (Activation) (None, 3)                 0         \n",
            "=================================================================\n",
            "Total params: 20,719\n",
            "Trainable params: 20,631\n",
            "Non-trainable params: 88\n",
            "_________________________________________________________________\n",
            "Epoch 1/60\n",
            "144/144 - 0s - loss: 2.1152 - mae: 0.9924\n",
            "Epoch 2/60\n",
            "144/144 - 0s - loss: 1.4549 - mae: 0.8473\n",
            "Epoch 3/60\n",
            "144/144 - 1s - loss: 1.3517 - mae: 0.8175\n",
            "Epoch 4/60\n",
            "144/144 - 0s - loss: 1.3700 - mae: 0.8222\n",
            "Epoch 5/60\n",
            "144/144 - 1s - loss: 1.2488 - mae: 0.7821\n",
            "Epoch 6/60\n",
            "144/144 - 1s - loss: 1.1925 - mae: 0.7725\n",
            "Epoch 7/60\n",
            "144/144 - 0s - loss: 1.1998 - mae: 0.7746\n",
            "Epoch 8/60\n",
            "144/144 - 0s - loss: 1.1527 - mae: 0.7545\n",
            "Epoch 9/60\n",
            "144/144 - 0s - loss: 1.1311 - mae: 0.7573\n",
            "Epoch 10/60\n",
            "144/144 - 0s - loss: 1.1361 - mae: 0.7590\n",
            "Epoch 11/60\n",
            "144/144 - 0s - loss: 1.1327 - mae: 0.7568\n",
            "Epoch 12/60\n",
            "144/144 - 0s - loss: 1.1068 - mae: 0.7411\n",
            "Epoch 13/60\n",
            "144/144 - 1s - loss: 1.1514 - mae: 0.7607\n",
            "Epoch 14/60\n",
            "144/144 - 1s - loss: 1.0779 - mae: 0.7368\n",
            "Epoch 15/60\n",
            "144/144 - 0s - loss: 1.0875 - mae: 0.7417\n",
            "Epoch 16/60\n",
            "144/144 - 0s - loss: 0.9999 - mae: 0.7119\n",
            "Epoch 17/60\n",
            "144/144 - 0s - loss: 1.0606 - mae: 0.7281\n",
            "Epoch 18/60\n",
            "144/144 - 0s - loss: 1.0125 - mae: 0.7217\n",
            "Epoch 19/60\n",
            "144/144 - 0s - loss: 1.0605 - mae: 0.7309\n",
            "Epoch 20/60\n",
            "144/144 - 0s - loss: 1.0465 - mae: 0.7307\n",
            "Epoch 21/60\n",
            "144/144 - 0s - loss: 1.0066 - mae: 0.7140\n",
            "Epoch 22/60\n",
            "144/144 - 1s - loss: 0.9989 - mae: 0.7122\n",
            "Epoch 23/60\n",
            "144/144 - 0s - loss: 0.9706 - mae: 0.7005\n",
            "Epoch 24/60\n",
            "144/144 - 0s - loss: 0.9527 - mae: 0.6987\n",
            "Epoch 25/60\n",
            "144/144 - 0s - loss: 0.9576 - mae: 0.7015\n",
            "Epoch 26/60\n",
            "144/144 - 0s - loss: 1.0384 - mae: 0.7173\n",
            "Epoch 27/60\n",
            "144/144 - 0s - loss: 0.9843 - mae: 0.7067\n",
            "Epoch 28/60\n",
            "144/144 - 1s - loss: 0.9932 - mae: 0.7073\n",
            "Epoch 29/60\n",
            "144/144 - 1s - loss: 0.9662 - mae: 0.7009\n",
            "Epoch 30/60\n",
            "144/144 - 0s - loss: 0.9454 - mae: 0.6896\n",
            "Epoch 31/60\n",
            "144/144 - 0s - loss: 1.0286 - mae: 0.7203\n",
            "Epoch 32/60\n",
            "144/144 - 0s - loss: 0.9326 - mae: 0.6899\n",
            "Epoch 33/60\n",
            "144/144 - 0s - loss: 0.9533 - mae: 0.6966\n",
            "Epoch 34/60\n",
            "144/144 - 0s - loss: 0.8989 - mae: 0.6811\n",
            "Epoch 35/60\n",
            "144/144 - 0s - loss: 0.9542 - mae: 0.6959\n",
            "Epoch 36/60\n",
            "144/144 - 0s - loss: 0.9621 - mae: 0.6971\n",
            "Epoch 37/60\n",
            "144/144 - 1s - loss: 0.9331 - mae: 0.6885\n",
            "Epoch 38/60\n",
            "144/144 - 0s - loss: 0.9355 - mae: 0.6934\n",
            "Epoch 39/60\n",
            "144/144 - 0s - loss: 0.9885 - mae: 0.7062\n",
            "Epoch 40/60\n",
            "144/144 - 0s - loss: 0.9386 - mae: 0.6895\n",
            "Epoch 41/60\n",
            "144/144 - 1s - loss: 0.9350 - mae: 0.6869\n",
            "Epoch 42/60\n",
            "144/144 - 0s - loss: 0.8870 - mae: 0.6732\n",
            "Epoch 43/60\n",
            "144/144 - 0s - loss: 0.9985 - mae: 0.7061\n",
            "Epoch 44/60\n",
            "144/144 - 0s - loss: 0.9064 - mae: 0.6828\n",
            "Epoch 45/60\n",
            "144/144 - 1s - loss: 0.9147 - mae: 0.6806\n",
            "Epoch 46/60\n",
            "144/144 - 0s - loss: 0.8953 - mae: 0.6745\n",
            "Epoch 47/60\n",
            "144/144 - 0s - loss: 0.9182 - mae: 0.6843\n",
            "Epoch 48/60\n",
            "144/144 - 0s - loss: 0.9076 - mae: 0.6737\n",
            "Epoch 49/60\n",
            "144/144 - 0s - loss: 0.8665 - mae: 0.6634\n",
            "Epoch 50/60\n",
            "144/144 - 0s - loss: 0.9010 - mae: 0.6739\n",
            "Epoch 51/60\n",
            "144/144 - 1s - loss: 0.8841 - mae: 0.6735\n",
            "Epoch 52/60\n",
            "144/144 - 1s - loss: 0.8890 - mae: 0.6662\n",
            "Epoch 53/60\n",
            "144/144 - 0s - loss: 0.8549 - mae: 0.6596\n",
            "Epoch 54/60\n",
            "144/144 - 0s - loss: 0.9141 - mae: 0.6772\n",
            "Epoch 55/60\n",
            "144/144 - 0s - loss: 0.8583 - mae: 0.6591\n",
            "Epoch 56/60\n",
            "144/144 - 0s - loss: 0.8859 - mae: 0.6698\n",
            "Epoch 57/60\n",
            "144/144 - 0s - loss: 0.8664 - mae: 0.6626\n",
            "Epoch 58/60\n",
            "144/144 - 0s - loss: 0.8445 - mae: 0.6554\n",
            "Epoch 59/60\n",
            "144/144 - 1s - loss: 0.9206 - mae: 0.6807\n",
            "Epoch 60/60\n",
            "144/144 - 0s - loss: 0.9134 - mae: 0.6745\n",
            "Model: \"functional_1625\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_407 (InputLayer)       [(None, 100, 2, 1)]       0         \n",
            "_________________________________________________________________\n",
            "conv2d_1624 (Conv2D)         (None, 100, 2, 16)        112       \n",
            "_________________________________________________________________\n",
            "batch_normalization_1624 (Ba (None, 100, 2, 16)        64        \n",
            "_________________________________________________________________\n",
            "activation_2030 (Activation) (None, 100, 2, 16)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_406 (MaxPoolin (None, 50, 2, 16)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_1625 (Conv2D)         (None, 50, 2, 12)         1164      \n",
            "_________________________________________________________________\n",
            "batch_normalization_1625 (Ba (None, 50, 2, 12)         48        \n",
            "_________________________________________________________________\n",
            "up_sampling2d_406 (UpSamplin (None, 100, 2, 12)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1626 (Conv2D)         (None, 100, 2, 16)        1168      \n",
            "_________________________________________________________________\n",
            "batch_normalization_1626 (Ba (None, 100, 2, 16)        64        \n",
            "_________________________________________________________________\n",
            "activation_2032 (Activation) (None, 100, 2, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1627 (Conv2D)         (None, 100, 2, 1)         97        \n",
            "=================================================================\n",
            "Total params: 2,717\n",
            "Trainable params: 2,629\n",
            "Non-trainable params: 88\n",
            "_________________________________________________________________\n",
            "Epoch 1/30\n",
            "145/145 - 0s - loss: 0.0341 - mae: 0.0910\n",
            "Epoch 2/30\n",
            "145/145 - 0s - loss: 0.0097 - mae: 0.0569\n",
            "Epoch 3/30\n",
            "145/145 - 0s - loss: 0.0070 - mae: 0.0483\n",
            "Epoch 4/30\n",
            "145/145 - 0s - loss: 0.0051 - mae: 0.0416\n",
            "Epoch 5/30\n",
            "145/145 - 0s - loss: 0.0057 - mae: 0.0436\n",
            "Epoch 6/30\n",
            "145/145 - 0s - loss: 0.0047 - mae: 0.0397\n",
            "Epoch 7/30\n",
            "145/145 - 0s - loss: 0.0051 - mae: 0.0401\n",
            "Epoch 8/30\n",
            "145/145 - 0s - loss: 0.0039 - mae: 0.0377\n",
            "Epoch 9/30\n",
            "145/145 - 0s - loss: 0.0038 - mae: 0.0356\n",
            "Epoch 10/30\n",
            "145/145 - 0s - loss: 0.0048 - mae: 0.0395\n",
            "Epoch 11/30\n",
            "145/145 - 0s - loss: 0.0036 - mae: 0.0344\n",
            "Epoch 12/30\n",
            "145/145 - 0s - loss: 0.0028 - mae: 0.0307\n",
            "Epoch 13/30\n",
            "145/145 - 0s - loss: 0.0032 - mae: 0.0331\n",
            "Epoch 14/30\n",
            "145/145 - 0s - loss: 0.0028 - mae: 0.0309\n",
            "Epoch 15/30\n",
            "145/145 - 0s - loss: 0.0030 - mae: 0.0316\n",
            "Epoch 16/30\n",
            "145/145 - 0s - loss: 0.0027 - mae: 0.0305\n",
            "Epoch 17/30\n",
            "145/145 - 0s - loss: 0.0029 - mae: 0.0312\n",
            "Epoch 18/30\n",
            "145/145 - 1s - loss: 0.0028 - mae: 0.0301\n",
            "Epoch 19/30\n",
            "145/145 - 1s - loss: 0.0029 - mae: 0.0312\n",
            "Epoch 20/30\n",
            "145/145 - 0s - loss: 0.0025 - mae: 0.0280\n",
            "Epoch 21/30\n",
            "145/145 - 0s - loss: 0.0026 - mae: 0.0291\n",
            "Epoch 22/30\n",
            "145/145 - 0s - loss: 0.0023 - mae: 0.0276\n",
            "Epoch 23/30\n",
            "145/145 - 0s - loss: 0.0022 - mae: 0.0268\n",
            "Epoch 24/30\n",
            "145/145 - 0s - loss: 0.0023 - mae: 0.0273\n",
            "Epoch 25/30\n",
            "145/145 - 0s - loss: 0.0025 - mae: 0.0288\n",
            "Epoch 26/30\n",
            "145/145 - 1s - loss: 0.0024 - mae: 0.0282\n",
            "Epoch 27/30\n",
            "145/145 - 1s - loss: 0.0022 - mae: 0.0267\n",
            "Epoch 28/30\n",
            "145/145 - 0s - loss: 0.0022 - mae: 0.0272\n",
            "Epoch 29/30\n",
            "145/145 - 0s - loss: 0.0026 - mae: 0.0299\n",
            "Epoch 30/30\n",
            "145/145 - 0s - loss: 0.0024 - mae: 0.0291\n",
            "Model: \"functional_1627\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_407 (InputLayer)       [(None, 100, 2, 1)]       0         \n",
            "_________________________________________________________________\n",
            "conv2d_1624 (Conv2D)         (None, 100, 2, 16)        112       \n",
            "_________________________________________________________________\n",
            "batch_normalization_1624 (Ba (None, 100, 2, 16)        64        \n",
            "_________________________________________________________________\n",
            "activation_2030 (Activation) (None, 100, 2, 16)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_406 (MaxPoolin (None, 50, 2, 16)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_1625 (Conv2D)         (None, 50, 2, 12)         1164      \n",
            "_________________________________________________________________\n",
            "batch_normalization_1625 (Ba (None, 50, 2, 12)         48        \n",
            "_________________________________________________________________\n",
            "activation_2031 (Activation) (None, 50, 2, 12)         0         \n",
            "_________________________________________________________________\n",
            "flatten_406 (Flatten)        (None, 1200)              0         \n",
            "_________________________________________________________________\n",
            "dense_812 (Dense)            (None, 16)                19216     \n",
            "_________________________________________________________________\n",
            "batch_normalization_1627 (Ba (None, 16)                64        \n",
            "_________________________________________________________________\n",
            "activation_2033 (Activation) (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dropout_406 (Dropout)        (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_813 (Dense)            (None, 3)                 51        \n",
            "_________________________________________________________________\n",
            "activation_2034 (Activation) (None, 3)                 0         \n",
            "=================================================================\n",
            "Total params: 20,719\n",
            "Trainable params: 20,631\n",
            "Non-trainable params: 88\n",
            "_________________________________________________________________\n",
            "Epoch 1/60\n",
            "144/144 - 1s - loss: 1.9431 - mae: 0.9489\n",
            "Epoch 2/60\n",
            "144/144 - 1s - loss: 1.3750 - mae: 0.8310\n",
            "Epoch 3/60\n",
            "144/144 - 1s - loss: 1.2825 - mae: 0.8047\n",
            "Epoch 4/60\n",
            "144/144 - 1s - loss: 1.2125 - mae: 0.7885\n",
            "Epoch 5/60\n",
            "144/144 - 0s - loss: 1.2006 - mae: 0.7835\n",
            "Epoch 6/60\n",
            "144/144 - 0s - loss: 1.1638 - mae: 0.7738\n",
            "Epoch 7/60\n",
            "144/144 - 1s - loss: 1.1377 - mae: 0.7682\n",
            "Epoch 8/60\n",
            "144/144 - 1s - loss: 1.1421 - mae: 0.7675\n",
            "Epoch 9/60\n",
            "144/144 - 1s - loss: 1.0659 - mae: 0.7418\n",
            "Epoch 10/60\n",
            "144/144 - 0s - loss: 1.0880 - mae: 0.7534\n",
            "Epoch 11/60\n",
            "144/144 - 0s - loss: 1.0779 - mae: 0.7443\n",
            "Epoch 12/60\n",
            "144/144 - 0s - loss: 1.1135 - mae: 0.7464\n",
            "Epoch 13/60\n",
            "144/144 - 0s - loss: 1.0171 - mae: 0.7245\n",
            "Epoch 14/60\n",
            "144/144 - 0s - loss: 1.0400 - mae: 0.7294\n",
            "Epoch 15/60\n",
            "144/144 - 0s - loss: 1.0185 - mae: 0.7220\n",
            "Epoch 16/60\n",
            "144/144 - 0s - loss: 1.0451 - mae: 0.7323\n",
            "Epoch 17/60\n",
            "144/144 - 0s - loss: 1.0077 - mae: 0.7255\n",
            "Epoch 18/60\n",
            "144/144 - 0s - loss: 0.9878 - mae: 0.7133\n",
            "Epoch 19/60\n",
            "144/144 - 0s - loss: 1.0572 - mae: 0.7337\n",
            "Epoch 20/60\n",
            "144/144 - 0s - loss: 0.9928 - mae: 0.7139\n",
            "Epoch 21/60\n",
            "144/144 - 0s - loss: 1.0068 - mae: 0.7147\n",
            "Epoch 22/60\n",
            "144/144 - 0s - loss: 0.9724 - mae: 0.7121\n",
            "Epoch 23/60\n",
            "144/144 - 0s - loss: 0.9744 - mae: 0.7094\n",
            "Epoch 24/60\n",
            "144/144 - 0s - loss: 0.9340 - mae: 0.6909\n",
            "Epoch 25/60\n",
            "144/144 - 0s - loss: 0.9175 - mae: 0.6910\n",
            "Epoch 26/60\n",
            "144/144 - 0s - loss: 0.9235 - mae: 0.6870\n",
            "Epoch 27/60\n",
            "144/144 - 0s - loss: 0.9280 - mae: 0.6976\n",
            "Epoch 28/60\n",
            "144/144 - 0s - loss: 0.9282 - mae: 0.6930\n",
            "Epoch 29/60\n",
            "144/144 - 0s - loss: 0.9915 - mae: 0.7093\n",
            "Epoch 30/60\n",
            "144/144 - 0s - loss: 0.9289 - mae: 0.6920\n",
            "Epoch 31/60\n",
            "144/144 - 0s - loss: 0.9768 - mae: 0.7089\n",
            "Epoch 32/60\n",
            "144/144 - 0s - loss: 0.9545 - mae: 0.6964\n",
            "Epoch 33/60\n",
            "144/144 - 1s - loss: 0.9715 - mae: 0.7038\n",
            "Epoch 34/60\n",
            "144/144 - 1s - loss: 0.9000 - mae: 0.6873\n",
            "Epoch 35/60\n",
            "144/144 - 0s - loss: 0.9066 - mae: 0.6827\n",
            "Epoch 36/60\n",
            "144/144 - 0s - loss: 0.8784 - mae: 0.6725\n",
            "Epoch 37/60\n",
            "144/144 - 1s - loss: 0.8986 - mae: 0.6816\n",
            "Epoch 38/60\n",
            "144/144 - 1s - loss: 0.9279 - mae: 0.6863\n",
            "Epoch 39/60\n",
            "144/144 - 0s - loss: 0.8769 - mae: 0.6751\n",
            "Epoch 40/60\n",
            "144/144 - 0s - loss: 0.8546 - mae: 0.6655\n",
            "Epoch 41/60\n",
            "144/144 - 1s - loss: 0.9165 - mae: 0.6847\n",
            "Epoch 42/60\n",
            "144/144 - 0s - loss: 0.9071 - mae: 0.6873\n",
            "Epoch 43/60\n",
            "144/144 - 0s - loss: 0.8504 - mae: 0.6671\n",
            "Epoch 44/60\n",
            "144/144 - 0s - loss: 0.9093 - mae: 0.6798\n",
            "Epoch 45/60\n",
            "144/144 - 0s - loss: 0.8664 - mae: 0.6717\n",
            "Epoch 46/60\n",
            "144/144 - 0s - loss: 0.9153 - mae: 0.6815\n",
            "Epoch 47/60\n",
            "144/144 - 0s - loss: 0.8732 - mae: 0.6670\n",
            "Epoch 48/60\n",
            "144/144 - 0s - loss: 0.8705 - mae: 0.6723\n",
            "Epoch 49/60\n",
            "144/144 - 0s - loss: 0.8635 - mae: 0.6676\n",
            "Epoch 50/60\n",
            "144/144 - 1s - loss: 0.9236 - mae: 0.6945\n",
            "Epoch 51/60\n",
            "144/144 - 0s - loss: 0.8538 - mae: 0.6647\n",
            "Epoch 52/60\n",
            "144/144 - 0s - loss: 0.8651 - mae: 0.6714\n",
            "Epoch 53/60\n",
            "144/144 - 0s - loss: 0.8202 - mae: 0.6547\n",
            "Epoch 54/60\n",
            "144/144 - 0s - loss: 0.8637 - mae: 0.6687\n",
            "Epoch 55/60\n",
            "144/144 - 0s - loss: 0.8426 - mae: 0.6574\n",
            "Epoch 56/60\n",
            "144/144 - 0s - loss: 0.8964 - mae: 0.6776\n",
            "Epoch 57/60\n",
            "144/144 - 0s - loss: 0.8217 - mae: 0.6575\n",
            "Epoch 58/60\n",
            "144/144 - 0s - loss: 0.8534 - mae: 0.6648\n",
            "Epoch 59/60\n",
            "144/144 - 0s - loss: 0.9088 - mae: 0.6825\n",
            "Epoch 60/60\n",
            "144/144 - 0s - loss: 0.8872 - mae: 0.6687\n",
            "Model: \"functional_1629\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_408 (InputLayer)       [(None, 100, 2, 1)]       0         \n",
            "_________________________________________________________________\n",
            "conv2d_1628 (Conv2D)         (None, 100, 2, 16)        112       \n",
            "_________________________________________________________________\n",
            "batch_normalization_1628 (Ba (None, 100, 2, 16)        64        \n",
            "_________________________________________________________________\n",
            "activation_2035 (Activation) (None, 100, 2, 16)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_407 (MaxPoolin (None, 50, 2, 16)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_1629 (Conv2D)         (None, 50, 2, 12)         1164      \n",
            "_________________________________________________________________\n",
            "batch_normalization_1629 (Ba (None, 50, 2, 12)         48        \n",
            "_________________________________________________________________\n",
            "up_sampling2d_407 (UpSamplin (None, 100, 2, 12)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1630 (Conv2D)         (None, 100, 2, 16)        1168      \n",
            "_________________________________________________________________\n",
            "batch_normalization_1630 (Ba (None, 100, 2, 16)        64        \n",
            "_________________________________________________________________\n",
            "activation_2037 (Activation) (None, 100, 2, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1631 (Conv2D)         (None, 100, 2, 1)         97        \n",
            "=================================================================\n",
            "Total params: 2,717\n",
            "Trainable params: 2,629\n",
            "Non-trainable params: 88\n",
            "_________________________________________________________________\n",
            "Epoch 1/30\n",
            "145/145 - 0s - loss: 0.0262 - mae: 0.0810\n",
            "Epoch 2/30\n",
            "145/145 - 0s - loss: 0.0077 - mae: 0.0501\n",
            "Epoch 3/30\n",
            "145/145 - 0s - loss: 0.0066 - mae: 0.0471\n",
            "Epoch 4/30\n",
            "145/145 - 0s - loss: 0.0049 - mae: 0.0432\n",
            "Epoch 5/30\n",
            "145/145 - 0s - loss: 0.0043 - mae: 0.0392\n",
            "Epoch 6/30\n",
            "145/145 - 0s - loss: 0.0052 - mae: 0.0451\n",
            "Epoch 7/30\n",
            "145/145 - 0s - loss: 0.0033 - mae: 0.0338\n",
            "Epoch 8/30\n",
            "145/145 - 0s - loss: 0.0036 - mae: 0.0351\n",
            "Epoch 9/30\n",
            "145/145 - 0s - loss: 0.0033 - mae: 0.0338\n",
            "Epoch 10/30\n",
            "145/145 - 0s - loss: 0.0034 - mae: 0.0337\n",
            "Epoch 11/30\n",
            "145/145 - 0s - loss: 0.0032 - mae: 0.0326\n",
            "Epoch 12/30\n",
            "145/145 - 0s - loss: 0.0028 - mae: 0.0306\n",
            "Epoch 13/30\n",
            "145/145 - 0s - loss: 0.0035 - mae: 0.0340\n",
            "Epoch 14/30\n",
            "145/145 - 0s - loss: 0.0038 - mae: 0.0354\n",
            "Epoch 15/30\n",
            "145/145 - 0s - loss: 0.0026 - mae: 0.0288\n",
            "Epoch 16/30\n",
            "145/145 - 0s - loss: 0.0024 - mae: 0.0283\n",
            "Epoch 17/30\n",
            "145/145 - 0s - loss: 0.0030 - mae: 0.0307\n",
            "Epoch 18/30\n",
            "145/145 - 0s - loss: 0.0025 - mae: 0.0288\n",
            "Epoch 19/30\n",
            "145/145 - 0s - loss: 0.0024 - mae: 0.0277\n",
            "Epoch 20/30\n",
            "145/145 - 0s - loss: 0.0025 - mae: 0.0277\n",
            "Epoch 21/30\n",
            "145/145 - 0s - loss: 0.0024 - mae: 0.0282\n",
            "Epoch 22/30\n",
            "145/145 - 0s - loss: 0.0028 - mae: 0.0299\n",
            "Epoch 23/30\n",
            "145/145 - 0s - loss: 0.0027 - mae: 0.0298\n",
            "Epoch 24/30\n",
            "145/145 - 0s - loss: 0.0028 - mae: 0.0307\n",
            "Epoch 25/30\n",
            "145/145 - 0s - loss: 0.0024 - mae: 0.0268\n",
            "Epoch 26/30\n",
            "145/145 - 0s - loss: 0.0020 - mae: 0.0254\n",
            "Epoch 27/30\n",
            "145/145 - 0s - loss: 0.0024 - mae: 0.0276\n",
            "Epoch 28/30\n",
            "145/145 - 0s - loss: 0.0023 - mae: 0.0276\n",
            "Epoch 29/30\n",
            "145/145 - 0s - loss: 0.0025 - mae: 0.0282\n",
            "Epoch 30/30\n",
            "145/145 - 0s - loss: 0.0025 - mae: 0.0283\n",
            "Model: \"functional_1631\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_408 (InputLayer)       [(None, 100, 2, 1)]       0         \n",
            "_________________________________________________________________\n",
            "conv2d_1628 (Conv2D)         (None, 100, 2, 16)        112       \n",
            "_________________________________________________________________\n",
            "batch_normalization_1628 (Ba (None, 100, 2, 16)        64        \n",
            "_________________________________________________________________\n",
            "activation_2035 (Activation) (None, 100, 2, 16)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_407 (MaxPoolin (None, 50, 2, 16)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_1629 (Conv2D)         (None, 50, 2, 12)         1164      \n",
            "_________________________________________________________________\n",
            "batch_normalization_1629 (Ba (None, 50, 2, 12)         48        \n",
            "_________________________________________________________________\n",
            "activation_2036 (Activation) (None, 50, 2, 12)         0         \n",
            "_________________________________________________________________\n",
            "flatten_407 (Flatten)        (None, 1200)              0         \n",
            "_________________________________________________________________\n",
            "dense_814 (Dense)            (None, 16)                19216     \n",
            "_________________________________________________________________\n",
            "batch_normalization_1631 (Ba (None, 16)                64        \n",
            "_________________________________________________________________\n",
            "activation_2038 (Activation) (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dropout_407 (Dropout)        (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_815 (Dense)            (None, 3)                 51        \n",
            "_________________________________________________________________\n",
            "activation_2039 (Activation) (None, 3)                 0         \n",
            "=================================================================\n",
            "Total params: 20,719\n",
            "Trainable params: 20,631\n",
            "Non-trainable params: 88\n",
            "_________________________________________________________________\n",
            "Epoch 1/60\n",
            "144/144 - 0s - loss: 1.8963 - mae: 0.9294\n",
            "Epoch 2/60\n",
            "144/144 - 0s - loss: 1.3934 - mae: 0.8286\n",
            "Epoch 3/60\n",
            "144/144 - 0s - loss: 1.2635 - mae: 0.7877\n",
            "Epoch 4/60\n",
            "144/144 - 0s - loss: 1.2553 - mae: 0.7782\n",
            "Epoch 5/60\n",
            "144/144 - 0s - loss: 1.2035 - mae: 0.7679\n",
            "Epoch 6/60\n",
            "144/144 - 0s - loss: 1.1985 - mae: 0.7637\n",
            "Epoch 7/60\n",
            "144/144 - 1s - loss: 1.1014 - mae: 0.7416\n",
            "Epoch 8/60\n",
            "144/144 - 0s - loss: 1.1353 - mae: 0.7507\n",
            "Epoch 9/60\n",
            "144/144 - 0s - loss: 1.1634 - mae: 0.7534\n",
            "Epoch 10/60\n",
            "144/144 - 0s - loss: 1.1539 - mae: 0.7590\n",
            "Epoch 11/60\n",
            "144/144 - 0s - loss: 1.0983 - mae: 0.7395\n",
            "Epoch 12/60\n",
            "144/144 - 0s - loss: 1.1111 - mae: 0.7420\n",
            "Epoch 13/60\n",
            "144/144 - 0s - loss: 1.0508 - mae: 0.7264\n",
            "Epoch 14/60\n",
            "144/144 - 0s - loss: 1.1134 - mae: 0.7405\n",
            "Epoch 15/60\n",
            "144/144 - 1s - loss: 1.0356 - mae: 0.7306\n",
            "Epoch 16/60\n",
            "144/144 - 0s - loss: 1.0524 - mae: 0.7275\n",
            "Epoch 17/60\n",
            "144/144 - 0s - loss: 1.0226 - mae: 0.7173\n",
            "Epoch 18/60\n",
            "144/144 - 0s - loss: 1.0319 - mae: 0.7221\n",
            "Epoch 19/60\n",
            "144/144 - 0s - loss: 1.0420 - mae: 0.7165\n",
            "Epoch 20/60\n",
            "144/144 - 0s - loss: 1.0537 - mae: 0.7288\n",
            "Epoch 21/60\n",
            "144/144 - 0s - loss: 0.9676 - mae: 0.7061\n",
            "Epoch 22/60\n",
            "144/144 - 0s - loss: 1.0241 - mae: 0.7131\n",
            "Epoch 23/60\n",
            "144/144 - 0s - loss: 1.0080 - mae: 0.7137\n",
            "Epoch 24/60\n",
            "144/144 - 0s - loss: 0.9984 - mae: 0.7067\n",
            "Epoch 25/60\n",
            "144/144 - 0s - loss: 0.9826 - mae: 0.7068\n",
            "Epoch 26/60\n",
            "144/144 - 0s - loss: 0.9455 - mae: 0.6907\n",
            "Epoch 27/60\n",
            "144/144 - 0s - loss: 0.9854 - mae: 0.7062\n",
            "Epoch 28/60\n",
            "144/144 - 0s - loss: 0.9515 - mae: 0.6931\n",
            "Epoch 29/60\n",
            "144/144 - 0s - loss: 1.0010 - mae: 0.7024\n",
            "Epoch 30/60\n",
            "144/144 - 0s - loss: 0.9427 - mae: 0.6889\n",
            "Epoch 31/60\n",
            "144/144 - 1s - loss: 0.9169 - mae: 0.6874\n",
            "Epoch 32/60\n",
            "144/144 - 1s - loss: 0.9619 - mae: 0.6961\n",
            "Epoch 33/60\n",
            "144/144 - 0s - loss: 1.0042 - mae: 0.7058\n",
            "Epoch 34/60\n",
            "144/144 - 0s - loss: 0.9496 - mae: 0.6934\n",
            "Epoch 35/60\n",
            "144/144 - 0s - loss: 0.9819 - mae: 0.6997\n",
            "Epoch 36/60\n",
            "144/144 - 0s - loss: 0.9390 - mae: 0.6887\n",
            "Epoch 37/60\n",
            "144/144 - 0s - loss: 0.9543 - mae: 0.6963\n",
            "Epoch 38/60\n",
            "144/144 - 0s - loss: 0.9360 - mae: 0.6860\n",
            "Epoch 39/60\n",
            "144/144 - 1s - loss: 0.9382 - mae: 0.6878\n",
            "Epoch 40/60\n",
            "144/144 - 1s - loss: 0.8696 - mae: 0.6660\n",
            "Epoch 41/60\n",
            "144/144 - 0s - loss: 0.9238 - mae: 0.6797\n",
            "Epoch 42/60\n",
            "144/144 - 0s - loss: 0.9640 - mae: 0.6985\n",
            "Epoch 43/60\n",
            "144/144 - 0s - loss: 0.9451 - mae: 0.6920\n",
            "Epoch 44/60\n",
            "144/144 - 0s - loss: 0.9082 - mae: 0.6816\n",
            "Epoch 45/60\n",
            "144/144 - 1s - loss: 0.8802 - mae: 0.6704\n",
            "Epoch 46/60\n",
            "144/144 - 0s - loss: 0.9211 - mae: 0.6796\n",
            "Epoch 47/60\n",
            "144/144 - 0s - loss: 0.8839 - mae: 0.6695\n",
            "Epoch 48/60\n",
            "144/144 - 0s - loss: 0.9234 - mae: 0.6845\n",
            "Epoch 49/60\n",
            "144/144 - 1s - loss: 0.8452 - mae: 0.6577\n",
            "Epoch 50/60\n",
            "144/144 - 0s - loss: 0.9052 - mae: 0.6754\n",
            "Epoch 51/60\n",
            "144/144 - 1s - loss: 0.9284 - mae: 0.6768\n",
            "Epoch 52/60\n",
            "144/144 - 0s - loss: 0.8443 - mae: 0.6569\n",
            "Epoch 53/60\n",
            "144/144 - 0s - loss: 0.8920 - mae: 0.6672\n",
            "Epoch 54/60\n",
            "144/144 - 0s - loss: 0.8668 - mae: 0.6618\n",
            "Epoch 55/60\n",
            "144/144 - 0s - loss: 0.8628 - mae: 0.6602\n",
            "Epoch 56/60\n",
            "144/144 - 0s - loss: 0.8921 - mae: 0.6701\n",
            "Epoch 57/60\n",
            "144/144 - 0s - loss: 0.8809 - mae: 0.6650\n",
            "Epoch 58/60\n",
            "144/144 - 1s - loss: 0.8726 - mae: 0.6617\n",
            "Epoch 59/60\n",
            "144/144 - 1s - loss: 0.8746 - mae: 0.6634\n",
            "Epoch 60/60\n",
            "144/144 - 0s - loss: 0.9221 - mae: 0.6796\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "boUysguDgXl5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "73e0f27e-7301-416f-b7e1-b118568df1c3"
      },
      "source": [
        "sum1=0\n",
        "sum2=0\n",
        "sum3=0\n",
        "        \n",
        "for i in range (1,11):\n",
        "  saved_model=load_model('model_%d.h5'%(i))\n",
        "  y_hat = saved_model.predict(X_test)\n",
        "  y_hat1=np.median(y_hat[:,0])\n",
        "  y_hat2=np.median(y_hat[:,1])\n",
        "  y_hat3=np.median(y_hat[:,2])\n",
        "  sum1=sum1+y_hat1\n",
        "  sum2=sum2+y_hat2\n",
        "  sum3=sum3+y_hat3\n",
        "\n",
        "y_hat1=sum1/10.0\n",
        "y_hat2=sum2/10.0\n",
        "y_hat3=sum3/10.0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f1722c53ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SjQs4Qo_3EK1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 876
        },
        "outputId": "8b98ac99-ad2d-47a1-ebf7-80083f2e1800"
      },
      "source": [
        "df10=df9.append({\"UPDRS22_Predicted\":y_hat1,\"UPDRS23_Predicted\":y_hat2,\"UPDRS31_Predicted\":y_hat3,\"UPDRS22_True\":7.0,\"UPDRS23_True\":5.0,\"UPDRS31_True\":2.0},ignore_index=True)\n",
        "df10"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>UPDRS22_Predicted</th>\n",
              "      <th>UPDRS23_Predicted</th>\n",
              "      <th>UPDRS31_Predicted</th>\n",
              "      <th>UPDRS22_True</th>\n",
              "      <th>UPDRS23_True</th>\n",
              "      <th>UPDRS31_True</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.931259</td>\n",
              "      <td>0.850512</td>\n",
              "      <td>0.619939</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.388335</td>\n",
              "      <td>0.957859</td>\n",
              "      <td>0.893668</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.978960</td>\n",
              "      <td>0.751282</td>\n",
              "      <td>0.652146</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.693754</td>\n",
              "      <td>0.586140</td>\n",
              "      <td>0.528320</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.682731</td>\n",
              "      <td>0.623512</td>\n",
              "      <td>0.511887</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.960438</td>\n",
              "      <td>0.638229</td>\n",
              "      <td>0.520282</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>1.189730</td>\n",
              "      <td>1.317596</td>\n",
              "      <td>0.711764</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.728550</td>\n",
              "      <td>0.896067</td>\n",
              "      <td>0.680871</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.817806</td>\n",
              "      <td>0.779411</td>\n",
              "      <td>0.626993</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.726610</td>\n",
              "      <td>0.603548</td>\n",
              "      <td>0.430715</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>1.398295</td>\n",
              "      <td>1.274212</td>\n",
              "      <td>0.724072</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0.632659</td>\n",
              "      <td>0.795341</td>\n",
              "      <td>0.581597</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>0.874880</td>\n",
              "      <td>0.645406</td>\n",
              "      <td>0.514364</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>1.261601</td>\n",
              "      <td>1.191196</td>\n",
              "      <td>0.723451</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>1.244123</td>\n",
              "      <td>1.197184</td>\n",
              "      <td>0.658160</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>0.592633</td>\n",
              "      <td>0.715875</td>\n",
              "      <td>0.527697</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>0.951574</td>\n",
              "      <td>0.557701</td>\n",
              "      <td>0.388493</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>1.315726</td>\n",
              "      <td>1.771767</td>\n",
              "      <td>0.865590</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>0.154446</td>\n",
              "      <td>0.464001</td>\n",
              "      <td>0.410356</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>1.787228</td>\n",
              "      <td>2.062056</td>\n",
              "      <td>0.902770</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>1.276794</td>\n",
              "      <td>1.507759</td>\n",
              "      <td>0.740243</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>0.550726</td>\n",
              "      <td>0.593494</td>\n",
              "      <td>0.532902</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>2.048120</td>\n",
              "      <td>2.215213</td>\n",
              "      <td>1.097466</td>\n",
              "      <td>2.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>2.066189</td>\n",
              "      <td>2.352506</td>\n",
              "      <td>0.910849</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>1.549838</td>\n",
              "      <td>1.927130</td>\n",
              "      <td>0.891622</td>\n",
              "      <td>4.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>1.865434</td>\n",
              "      <td>2.446856</td>\n",
              "      <td>0.979985</td>\n",
              "      <td>4.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>3.659236</td>\n",
              "      <td>4.285470</td>\n",
              "      <td>1.299288</td>\n",
              "      <td>5.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>1.055449</td>\n",
              "      <td>0.912653</td>\n",
              "      <td>0.582440</td>\n",
              "      <td>7.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    UPDRS22_Predicted  UPDRS23_Predicted  ...  UPDRS23_True  UPDRS31_True\n",
              "0            0.931259           0.850512  ...           0.0           0.0\n",
              "1            0.388335           0.957859  ...           0.0           0.0\n",
              "2            0.978960           0.751282  ...           0.0           0.0\n",
              "3            0.693754           0.586140  ...           0.0           0.0\n",
              "4            0.682731           0.623512  ...           0.0           0.0\n",
              "5            0.960438           0.638229  ...           0.0           0.0\n",
              "6            1.189730           1.317596  ...           0.0           1.0\n",
              "7            0.728550           0.896067  ...           0.0           0.0\n",
              "8            0.817806           0.779411  ...           0.0           0.0\n",
              "9            0.726610           0.603548  ...           0.0           0.0\n",
              "10           1.398295           1.274212  ...           0.0           1.0\n",
              "11           0.632659           0.795341  ...           0.0           1.0\n",
              "12           0.874880           0.645406  ...           0.0           0.0\n",
              "13           1.261601           1.191196  ...           1.0           1.0\n",
              "14           1.244123           1.197184  ...           1.0           1.0\n",
              "15           0.592633           0.715875  ...           1.0           0.0\n",
              "16           0.951574           0.557701  ...           1.0           1.0\n",
              "17           1.315726           1.771767  ...           1.0           1.0\n",
              "18           0.154446           0.464001  ...           1.0           1.0\n",
              "19           1.787228           2.062056  ...           1.0           1.0\n",
              "20           1.276794           1.507759  ...           2.0           1.0\n",
              "21           0.550726           0.593494  ...           3.0           1.0\n",
              "22           2.048120           2.215213  ...           3.0           1.0\n",
              "23           2.066189           2.352506  ...           3.0           2.0\n",
              "24           1.549838           1.927130  ...           4.0           1.0\n",
              "25           1.865434           2.446856  ...           4.0           1.0\n",
              "26           3.659236           4.285470  ...           5.0           2.0\n",
              "27           1.055449           0.912653  ...           5.0           2.0\n",
              "\n",
              "[28 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6EXEuOzrF04i",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9318d063-832f-4170-bd14-8786f51c28bc"
      },
      "source": [
        "opt = tensorflow.keras.optimizers.Adam(learning_rate=1e-3,nesterov=True,momentum=0.9 )\n",
        "model.compile(loss='mse', optimizer=opt,metrics=['mae'])\n",
        "model.fit(X, y, epochs=50,batch_size=32)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "173/173 [==============================] - 1s 5ms/step - loss: 0.4390 - mae: 0.4948\n",
            "Epoch 2/50\n",
            "173/173 [==============================] - 1s 5ms/step - loss: 0.4328 - mae: 0.4876\n",
            "Epoch 3/50\n",
            "173/173 [==============================] - 1s 5ms/step - loss: 0.4351 - mae: 0.4877\n",
            "Epoch 4/50\n",
            "173/173 [==============================] - 1s 5ms/step - loss: 0.4229 - mae: 0.4808\n",
            "Epoch 5/50\n",
            "173/173 [==============================] - 1s 6ms/step - loss: 0.4398 - mae: 0.4887\n",
            "Epoch 6/50\n",
            "173/173 [==============================] - 1s 5ms/step - loss: 0.4312 - mae: 0.4844\n",
            "Epoch 7/50\n",
            "173/173 [==============================] - 1s 6ms/step - loss: 0.4409 - mae: 0.4897\n",
            "Epoch 8/50\n",
            "173/173 [==============================] - 1s 5ms/step - loss: 0.4327 - mae: 0.4859\n",
            "Epoch 9/50\n",
            "173/173 [==============================] - 1s 5ms/step - loss: 0.4299 - mae: 0.4818\n",
            "Epoch 10/50\n",
            "173/173 [==============================] - 1s 5ms/step - loss: 0.4375 - mae: 0.4835\n",
            "Epoch 11/50\n",
            "173/173 [==============================] - 1s 5ms/step - loss: 0.4317 - mae: 0.4840\n",
            "Epoch 12/50\n",
            "173/173 [==============================] - 1s 5ms/step - loss: 0.4221 - mae: 0.4790\n",
            "Epoch 13/50\n",
            "173/173 [==============================] - 1s 5ms/step - loss: 0.4303 - mae: 0.4829\n",
            "Epoch 14/50\n",
            "173/173 [==============================] - 1s 5ms/step - loss: 0.4301 - mae: 0.4844\n",
            "Epoch 15/50\n",
            "173/173 [==============================] - 1s 6ms/step - loss: 0.4312 - mae: 0.4850\n",
            "Epoch 16/50\n",
            "173/173 [==============================] - 1s 5ms/step - loss: 0.4350 - mae: 0.4864\n",
            "Epoch 17/50\n",
            "173/173 [==============================] - 1s 5ms/step - loss: 0.4330 - mae: 0.4838\n",
            "Epoch 18/50\n",
            "173/173 [==============================] - 1s 5ms/step - loss: 0.4336 - mae: 0.4862\n",
            "Epoch 19/50\n",
            "173/173 [==============================] - 1s 6ms/step - loss: 0.4123 - mae: 0.4747\n",
            "Epoch 20/50\n",
            "173/173 [==============================] - 1s 5ms/step - loss: 0.4336 - mae: 0.4829\n",
            "Epoch 21/50\n",
            "173/173 [==============================] - 1s 5ms/step - loss: 0.4311 - mae: 0.4835\n",
            "Epoch 22/50\n",
            "173/173 [==============================] - 1s 5ms/step - loss: 0.4285 - mae: 0.4832\n",
            "Epoch 23/50\n",
            "173/173 [==============================] - 1s 5ms/step - loss: 0.4177 - mae: 0.4784\n",
            "Epoch 24/50\n",
            "173/173 [==============================] - 1s 5ms/step - loss: 0.4241 - mae: 0.4797\n",
            "Epoch 25/50\n",
            "173/173 [==============================] - 1s 5ms/step - loss: 0.4198 - mae: 0.4803\n",
            "Epoch 26/50\n",
            "173/173 [==============================] - 1s 5ms/step - loss: 0.4223 - mae: 0.4811\n",
            "Epoch 27/50\n",
            "173/173 [==============================] - 1s 6ms/step - loss: 0.4124 - mae: 0.4724\n",
            "Epoch 28/50\n",
            "173/173 [==============================] - 1s 6ms/step - loss: 0.4312 - mae: 0.4806\n",
            "Epoch 29/50\n",
            "173/173 [==============================] - 1s 5ms/step - loss: 0.4325 - mae: 0.4857\n",
            "Epoch 30/50\n",
            "173/173 [==============================] - 1s 5ms/step - loss: 0.4169 - mae: 0.4762\n",
            "Epoch 31/50\n",
            "173/173 [==============================] - 1s 5ms/step - loss: 0.4265 - mae: 0.4800\n",
            "Epoch 32/50\n",
            "173/173 [==============================] - 1s 5ms/step - loss: 0.4230 - mae: 0.4776\n",
            "Epoch 33/50\n",
            "173/173 [==============================] - 1s 5ms/step - loss: 0.4327 - mae: 0.4828\n",
            "Epoch 34/50\n",
            "173/173 [==============================] - 1s 5ms/step - loss: 0.4275 - mae: 0.4800\n",
            "Epoch 35/50\n",
            "173/173 [==============================] - 1s 5ms/step - loss: 0.4284 - mae: 0.4814\n",
            "Epoch 36/50\n",
            "173/173 [==============================] - 1s 5ms/step - loss: 0.4224 - mae: 0.4771\n",
            "Epoch 37/50\n",
            "173/173 [==============================] - 1s 5ms/step - loss: 0.4102 - mae: 0.4731\n",
            "Epoch 38/50\n",
            "173/173 [==============================] - 1s 5ms/step - loss: 0.4177 - mae: 0.4774\n",
            "Epoch 39/50\n",
            "173/173 [==============================] - 1s 5ms/step - loss: 0.4323 - mae: 0.4801\n",
            "Epoch 40/50\n",
            "173/173 [==============================] - 1s 6ms/step - loss: 0.4347 - mae: 0.4823\n",
            "Epoch 41/50\n",
            "173/173 [==============================] - 1s 5ms/step - loss: 0.4211 - mae: 0.4795\n",
            "Epoch 42/50\n",
            "173/173 [==============================] - 1s 5ms/step - loss: 0.4186 - mae: 0.4777\n",
            "Epoch 43/50\n",
            "173/173 [==============================] - 1s 5ms/step - loss: 0.4314 - mae: 0.4823\n",
            "Epoch 44/50\n",
            "173/173 [==============================] - 1s 5ms/step - loss: 0.4205 - mae: 0.4773\n",
            "Epoch 45/50\n",
            "173/173 [==============================] - 1s 5ms/step - loss: 0.4151 - mae: 0.4731\n",
            "Epoch 46/50\n",
            "173/173 [==============================] - 1s 5ms/step - loss: 0.4117 - mae: 0.4725\n",
            "Epoch 47/50\n",
            "173/173 [==============================] - 1s 5ms/step - loss: 0.4185 - mae: 0.4762\n",
            "Epoch 48/50\n",
            "173/173 [==============================] - 1s 5ms/step - loss: 0.4312 - mae: 0.4782\n",
            "Epoch 49/50\n",
            "173/173 [==============================] - 1s 5ms/step - loss: 0.4302 - mae: 0.4814\n",
            "Epoch 50/50\n",
            "173/173 [==============================] - 1s 5ms/step - loss: 0.4245 - mae: 0.4782\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fa41cf07710>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 197
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fNojV8cpF04k",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        },
        "outputId": "d43f2ff2-6193-4cc6-cad5-0ff0885e7791"
      },
      "source": [
        "df3=pd.merge(data_dem, data_full, on='ID')\n",
        "df3['ID']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0       1\n",
              "1       1\n",
              "2       1\n",
              "3       1\n",
              "4       1\n",
              "       ..\n",
              "300    33\n",
              "301    33\n",
              "302    33\n",
              "303    33\n",
              "304    33\n",
              "Name: ID, Length: 305, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 199
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hp2yhKnxeMmX"
      },
      "source": [
        "df3=df3[(df3['UPDRS 23'] < 3) ]\n",
        "df3=df3[(df3['UPDRS 22'] < 3 ) ]\n",
        "df3=df3.reset_index()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3lfFVAi7F04l",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1e6dfb4a-8806-4dfa-e06d-ea7cfa02edbf"
      },
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.models import model_from_json,load_model\n",
        "\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "from sklearn.model_selection import GroupKFold\n",
        "group_kfold = GroupKFold(n_splits=33)\n",
        "groups=df3['ID']\n",
        "GroupKFold(n_splits=33)\n",
        "scores=[]\n",
        "scores1=[]\n",
        "scores2=[]\n",
        "scores3=[]\n",
        "scores4=[]\n",
        "saved_model=load_model('model_10.h5')\n",
        "# saved_model.summary()\n",
        "for train_index, test_index in group_kfold.split(df3,df3, groups):\n",
        "\n",
        "#         model = Sequential()\n",
        "#         for layer in loaded_model.layers[:-4]: \n",
        "#             model.add(layer)\n",
        "            \n",
        "#         model.add(Flatten()) \n",
        "        # saved_model=load_model('/content/gdrive/My Drive/models/best_model_9_9.h5')\n",
        "\n",
        "\n",
        "#         model.add(Dense(1024,activation= 'relu'))\n",
        "#         model.add(Dense(512,activation='relu'))\n",
        "#         model.add(Dense(256,activation='relu')) \n",
        "\n",
        "#         model.add(Dense(128,activation='relu'))\n",
        "\n",
        "#         model.add(Dense(3, activation='linear')) \n",
        "#         model.summary()\n",
        "\n",
        "        \n",
        "        temp1 = df3['X_flight'].apply(pd.Series)\n",
        "        temp2 = df3['X_hold'].apply(pd.Series)\n",
        "\n",
        "        \n",
        "#         temp1=scaler.transform(temp1)\n",
        "        \n",
        "#         temp2=scaler2.transform(temp2)\n",
        "        npa=np.dstack((temp1,temp2))\n",
        "\n",
        "        print(npa.shape)\n",
        "\n",
        "        # npa = npa.swapaxes(1,2)\n",
        "        npa=npa.reshape(-1 ,100, 2 ,1)\n",
        "        X=npa\n",
        "        temp4= np.asarray(df3['UPDRS 22'])\n",
        "        temp5= np.asarray(df3['UPDRS 23'])\n",
        "        temp6= np.asarray(df3['UPDRS 31'])\n",
        "        temp7=np.dstack((temp4,temp5,temp6))\n",
        "        temp7=np.nan_to_num(temp7)\n",
        "        temp7=temp7.swapaxes(0,1).swapaxes(1,2)\n",
        "        temp7=temp7.reshape(-1,3)\n",
        "#         temp7=temp7.reshape(-1,3,1,1)\n",
        "        y=temp7\n",
        "        print(y.shape)\n",
        "        X_train, X_test = X[train_index], X[test_index]\n",
        "        y_train, y_test = y[train_index], y[test_index]\n",
        "#         print(X_test)\n",
        "#         print(y_test)\n",
        "        # opt3=tensorflow.keras.optimizers.Adam(learning_rate=1e-3,decay=1e-5,amsgrad='True')\n",
        "\n",
        "        # saved_model.compile(loss='mse', optimizer=opt3, metrics=['mae'])\n",
        "    \n",
        "        # saved_model.fit(X_train, y_train,\n",
        "                # epochs=50)\n",
        "#        \n",
        "        \n",
        "        \n",
        "      \n",
        "        \n",
        "        # sum1=0\n",
        "        # sum2=0\n",
        "        # sum3=0\n",
        "        \n",
        "        # for i in range (1,11):\n",
        "        #   saved_model=load_model('model_%d.h5'%(i))\n",
        "        y_hat = saved_model.predict(X_test)\n",
        "        #   y_hat1=np.median(y_hat[:,0])\n",
        "        #   y_hat2=np.median(y_hat[:,1])\n",
        "        #   y_hat3=np.median(y_hat[:,2])\n",
        "        #   sum1=sum1+y_hat1\n",
        "        #   sum2=sum2+y_hat2\n",
        "        #   sum3=sum3+y_hat3\n",
        "\n",
        "\n",
        "        # y_hat1=sum1/10.0\n",
        "        # y_hat2=sum2/10.0\n",
        "        # y_hat3=sum3/10.0\n",
        "\n",
        "        # print(y_hat)\n",
        "        # test_mse=[]\n",
        "        # test_mse1 = mean_squared_error(y_test[:,:1], y_hat[:,:1])\n",
        "        # test_mse2 = mean_squared_error(y_test[:,1:2], y_hat[:,1:2])\n",
        "        # test_mse3 = mean_squared_error(y_test[:,2:3], y_hat[:,2:3])\n",
        "        # test_mse.append(test_mse1)\n",
        "        # test_mse.append(test_mse2)\n",
        "        # test_mse.append(test_mse3)\n",
        "        \n",
        "        # scores.append(test_mse)\n",
        "        \n",
        "        y_test1=y_test[0][0]\n",
        "        y_test2=y_test[0][1]\n",
        "        y_test3=y_test[0][2]\n",
        "        y_test4=[]\n",
        "        y_test4.append(y_test1)\n",
        "        y_test4.append(y_test2)\n",
        "        y_test4.append(y_test3)\n",
        "        scores1.append(y_test4)\n",
        "        print(y_test4)\n",
        "        # print(y_test4)\n",
        "        # y_hat1=y_hat[:,0].mean()\n",
        "        # y_hat2=y_hat[:,1].mean()\n",
        "        # y_hat3=y_hat[:,2].mean()\n",
        "        # y_hatm=[]\n",
        "        # y_hatm.append(y_hat1)\n",
        "        # y_hatm.append(y_hat2)\n",
        "        # y_hatm.append(y_hat3)\n",
        "        # print(y_hatm)\n",
        "        # scores2.append(y_hatm)\n",
        "\n",
        "        y_hat1=np.median(y_hat[:,0])\n",
        "        y_hat2=np.median(y_hat[:,1])\n",
        "        y_hat3=np.median(y_hat[:,2])\n",
        "        y_hatmm=[]\n",
        "        y_hatmm.append(y_hat1)\n",
        "        y_hatmm.append(y_hat2)\n",
        "        y_hatmm.append(y_hat3)\n",
        "        scores3.append(y_hatmm)\n",
        "        print(y_hatmm)\n",
        "        \n",
        "        # y_hat1=np.percentile(y_hat[:,0],75)\n",
        "        # y_hat2=np.percentile(y_hat[:,1],75)\n",
        "        # y_hat3=np.percentile(y_hat[:,2],75)\n",
        "        # y_hatq75=[]\n",
        "        # y_hatq75.append(y_hat1)\n",
        "        # y_hatq75.append(y_hat2)\n",
        "        # y_hatq75.append(y_hat3)\n",
        "        # scores4.append(y_hatq75)\n",
        "        # print(y_hatq75)\n",
        "        "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(305, 100, 2)\n",
            "(305, 3)\n",
            "[1, 1, 0]\n",
            "[1.7033945, 1.5021191, 0.8176484]\n",
            "(305, 100, 2)\n",
            "(305, 3)\n",
            "[2, 2, 0]\n",
            "[2.3531952, 2.923308, 1.0342565]\n",
            "(305, 100, 2)\n",
            "(305, 3)\n",
            "[0, 0, 0]\n",
            "[1.9629492, 2.1442795, 0.93461597]\n",
            "(305, 100, 2)\n",
            "(305, 3)\n",
            "[0, 0, 0]\n",
            "[2.0934072, 1.5135225, 0.6799735]\n",
            "(305, 100, 2)\n",
            "(305, 3)\n",
            "[0, 0, 0]\n",
            "[1.0429616, 1.2380949, 0.6581604]\n",
            "(305, 100, 2)\n",
            "(305, 3)\n",
            "[3, 4, 2]\n",
            "[2.4540472, 2.776855, 1.051588]\n",
            "(305, 100, 2)\n",
            "(305, 3)\n",
            "[1, 1, 0]\n",
            "[0.973823, 0.9599863, 0.6312381]\n",
            "(305, 100, 2)\n",
            "(305, 3)\n",
            "[4, 4, 2]\n",
            "[2.1745048, 2.0610173, 0.88602394]\n",
            "(305, 100, 2)\n",
            "(305, 3)\n",
            "[0, 0, 0]\n",
            "[0.9222192, 1.1134965, 0.7400035]\n",
            "(305, 100, 2)\n",
            "(305, 3)\n",
            "[1, 1, 0]\n",
            "[1.5772785, 1.8982103, 0.85435504]\n",
            "(305, 100, 2)\n",
            "(305, 3)\n",
            "[0, 0, 0]\n",
            "[0.65507615, 0.5460453, 0.40207788]\n",
            "(305, 100, 2)\n",
            "(305, 3)\n",
            "[0, 0, 0]\n",
            "[0.4422498, 0.21101001, 0.14797622]\n",
            "(305, 100, 2)\n",
            "(305, 3)\n",
            "[0, 0, 0]\n",
            "[0.8702264, 0.82940984, 0.57258546]\n",
            "(305, 100, 2)\n",
            "(305, 3)\n",
            "[0, 0, 0]\n",
            "[0.34837094, 0.83920336, 0.6556202]\n",
            "(305, 100, 2)\n",
            "(305, 3)\n",
            "[3, 3, 1]\n",
            "[0.91961, 0.7707598, 0.51872027]\n",
            "(305, 100, 2)\n",
            "(305, 3)\n",
            "[0, 0, 0]\n",
            "[1.1324229, 0.9346172, 0.5896399]\n",
            "(305, 100, 2)\n",
            "(305, 3)\n",
            "[0, 0, 0]\n",
            "[0.7787701, 0.9733658, 0.70088625]\n",
            "(305, 100, 2)\n",
            "(305, 3)\n",
            "[3, 3, 1]\n",
            "[1.4499612, 1.2384343, 0.61979663]\n",
            "(305, 100, 2)\n",
            "(305, 3)\n",
            "[4, 3, 2]\n",
            "[2.614003, 3.2122865, 1.0941253]\n",
            "(305, 100, 2)\n",
            "(305, 3)\n",
            "[3, 4, 1]\n",
            "[0.8697545, 0.80923724, 0.58656305]\n",
            "(305, 100, 2)\n",
            "(305, 3)\n",
            "[0, 0, 0]\n",
            "[0.6530174, 0.61693674, 0.4290716]\n",
            "(305, 100, 2)\n",
            "(305, 3)\n",
            "[0, 0, 0]\n",
            "[0.9187202, 1.0113782, 0.66624755]\n",
            "(305, 100, 2)\n",
            "(305, 3)\n",
            "[2, 1, 1]\n",
            "[2.1815813, 2.4380114, 0.99602604]\n",
            "(305, 100, 2)\n",
            "(305, 3)\n",
            "[1, 1, 1]\n",
            "[2.112207, 2.1147692, 0.9691279]\n",
            "(305, 100, 2)\n",
            "(305, 3)\n",
            "[3, 3, 1]\n",
            "[2.8956337, 2.6632028, 1.1250532]\n",
            "(305, 100, 2)\n",
            "(305, 3)\n",
            "[0, 0, 0]\n",
            "[1.060766, 0.77611434, 0.49095753]\n",
            "(305, 100, 2)\n",
            "(305, 3)\n",
            "[0, 0, 0]\n",
            "[0.8086567, 0.7020215, 0.5684389]\n",
            "(305, 100, 2)\n",
            "(305, 3)\n",
            "[1, 2, 0]\n",
            "[1.7160361, 2.029167, 0.86817515]\n",
            "(305, 100, 2)\n",
            "(305, 3)\n",
            "[4, 4, 2]\n",
            "[3.6838255, 3.678616, 1.3217916]\n",
            "(305, 100, 2)\n",
            "(305, 3)\n",
            "[0, 0, 0]\n",
            "[0.8191755, 0.9575591, 0.6640036]\n",
            "(305, 100, 2)\n",
            "(305, 3)\n",
            "[3, 3, 1]\n",
            "[2.6124706, 3.291123, 1.0948434]\n",
            "(305, 100, 2)\n",
            "(305, 3)\n",
            "[3, 3, 1]\n",
            "[0.5638195, 0.7790685, 0.5947453]\n",
            "(305, 100, 2)\n",
            "(305, 3)\n",
            "[1, 3, 2]\n",
            "[1.0555034, 0.90990806, 0.46213058]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8uZFHwWNF04s"
      },
      "source": [
        "import csv\n",
        "temp=['UPDRS22_True','UPDRS23_True','UPDRS31_True']\n",
        "with open('/content/gdrive/My Drive/test10_1_3.csv', 'w') as csv_file:\n",
        "    csv_writer = csv.writer(csv_file, delimiter=',')\n",
        "    csv_writer.writerow(temp )\n",
        "    for i in range (0,33):\n",
        "        csv_writer.writerow(scores1[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n9muWdJiF04u"
      },
      "source": [
        "temp=['UPDRS22_Predicted','UPDRS23_Predicted','UPDRS31_Predicted']\n",
        "with open('/content/gdrive/My Drive/test10_3_3.csv', 'w') as csv_file:\n",
        "    csv_writer = csv.writer(csv_file, delimiter=',')\n",
        "    csv_writer.writerow(temp )\n",
        "    for i in range (0,33):\n",
        "        csv_writer.writerow(scores3[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vildq4R4F04w"
      },
      "source": [
        "df_test1=pd.read_csv('/content/gdrive/My Drive/test8_3_3.csv')\n",
        "df_test2=pd.read_csv('/content/gdrive/My Drive/test8_1_3.csv')\n",
        "# df_test1=df_test1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QzBABqOQF04x"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nHat5Ru0F040"
      },
      "source": [
        "\n",
        "\n",
        "df5= pd.merge(df_test1, df_test2,left_index=True, right_index=True)\n",
        "# df5.sort_values(by=['UPDRS23_True'],inplace=True)\n",
        "# df5=df5.drop([19,31])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ldWsorVkF043",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 873
        },
        "outputId": "342cbaf0-74fe-4acc-a836-025588a2f9cf"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "sns.set(rc={'figure.figsize':(13,13)})\n",
        "\n",
        "corrMatrix = df5.corr()\n",
        "ax=sns.heatmap(corrMatrix, annot=True,vmin=0, vmax=1, center=0.5,cmap= 'coolwarm')\n",
        "ax.set_title('Median')\n",
        "figure = ax.get_figure()    \n",
        "# figure.savefig('Corr_Matrix_cnn100_median_test.png', dpi=400)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 936x936 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E6k57ZqGF045",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 790
        },
        "outputId": "5ce2cccf-3d90-4f01-8966-15fcc23a2c4b"
      },
      "source": [
        "\n",
        "df5.sort_values(by=['UPDRS23_True'],inplace=True)\n",
        "x=np.arange(1,34, 1)\n",
        "ax = plt.gca()\n",
        "ax.scatter(x,np.array(df5['UPDRS23_Predicted']),color='b',marker=\"o\",label='Predicted')\n",
        "ax.scatter(x,np.array(df5['UPDRS23_True']),x,color='r',marker=\"o\",label='True')\n",
        "ax.set_xlabel('Subject')\n",
        "ax.set_title('Median')\n",
        "ax.set_ylabel('UPDRS23 score')\n",
        "plt.legend(loc='upper left');\n",
        "figure = ax.get_figure()    \n",
        "# figure.savefig('Scatter_plot_Median_UPDRS23_cnn100_test.png', dpi=400)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 936x936 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GI5DK-yIF047"
      },
      "source": [
        "df10=pd.DataFrame()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UYeZc-hqF049"
      },
      "source": [
        "df7=pd.read_csv('/content/gdrive/My Drive/test10_3_3.csv')\n",
        "df8=pd.read_csv('/content/gdrive/My Drive/test10_1_3.csv')\n",
        "df9= pd.merge(df7, df8,left_index=True, right_index=True)\n",
        "df10=df10.append([df9.corr().iloc[[3,4,5],[0,1,2]].iloc[[0],[0]],df9.corr().iloc[[3,4,5],[0,1,2]].iloc[[1],[1]],df9.corr().iloc[[3,4,5],[0,1,2]].iloc[[2],[2]]],ignore_index=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aMslIT0mF04-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "7684c2d5-608a-4f8f-c387-a7c11729454c"
      },
      "source": [
        "df10.mean()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "UPDRS22_Predicted    0.549572\n",
              "UPDRS23_Predicted    0.563727\n",
              "UPDRS31_Predicted    0.516990\n",
              "dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 352
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gqd-5WTOF05D",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "498a9de2-9751-434e-cdcd-6f898e55afee"
      },
      "source": [
        "df10.std()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "UPDRS22_Predicted    0.091279\n",
              "UPDRS23_Predicted    0.057012\n",
              "UPDRS31_Predicted    0.103596\n",
              "dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 353
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_k9ou9bdhU2Y",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "5fc9a3a5-67b4-4dd5-83e0-8c91f48fdf36"
      },
      "source": [
        "df10.mean()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "UPDRS22_Predicted    0.531326\n",
              "UPDRS23_Predicted    0.576368\n",
              "UPDRS31_Predicted    0.572276\n",
              "dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 304
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7gIaPVzVkfZy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "44cdc4f7-407a-4ba2-fbb2-fbbf3da892a2"
      },
      "source": [
        "df10.std()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "UPDRS22_Predicted    0.050340\n",
              "UPDRS23_Predicted    0.047530\n",
              "UPDRS31_Predicted    0.051432\n",
              "dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 305
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7hJt5hdBkhOv"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}